{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "***\n",
    "\n",
    "### Índice\n",
    "1. [Introdução](#Recurrent-Neural-Networks) \n",
    "2. [LSTM simples](#LSTM-simples)\n",
    "3. [LSTM empilhada (stacked)](#LSTM-empilhada-(stacked))\n",
    "4. [LSTM para previsão de palavras](#LSTMs-para-previsão-de-palavras)\n",
    "\n",
    "***\n",
    "\n",
    "As redes neurais recorrentes consideram dados anteriores ao atual, sendo muito úteis para trabalhar com dados contínuos, como análises temporais ou reconhecimento de áudio, por exemplo.\n",
    "\n",
    "As RNNs armazem estados, que funcionam como memória. O hidden output, como e conhecido, é reinserido na rede, agindo como um input secundário, que é atualizado a cada novo dado inserido. O hidden output antigo é então atualizado pela rede e funciona assim sucessivamente até o final do treinamento.\n",
    "\n",
    "![rnn](img/rnn.png)\n",
    "\n",
    "Dois tipos de RNN:\n",
    "- One-to-many: um input gera vários outputs (ex.: gerar uma frase descritiva de uma imagem)\n",
    "- Many-to-one: vários inputs geram um output (ex.: mercado financeiro ou análise de sentimentos)\n",
    "\n",
    "Porém esse modelo apresenta alguns problemas, que podem dificultar ou até inviabilizar o processo de treinamento de muitos dados. Pode-se citar o elevado custo computacional de manter os estados, o \"vanishing gradient\", quando o gradiente se estabiliza perto de 0, ou o \"exploding gradient\", quando o gradiente explode ao infinito.\n",
    "\n",
    "***\n",
    "\n",
    "# Long Short-Term Memory (LSTM)\n",
    "Para resolver problemas intrínsecos às RNNs comuns, foram desenvolvidas as LSTMs. Elas consistem de 4 elementos logísticos básicos, cada um com pesos e vieses específicos:\n",
    "- A célula de memória\n",
    "- Registro de leitura (read gate), que lê as informações da célula de memória e as envia de volta para a RNN\n",
    "- Registro de escrita (write gate), que escreve informações da célula de memória\n",
    "- Registro de esquecimento (forget/keep gate), que define quais informações antigas devem ser apagadas da célula de memória\n",
    "\n",
    "![elements](img/lstm-elements.png)\n",
    "\n",
    "O fluxo de dados inicia-se com o registro de esquecimento, que determina se a informação anterior deve ser mantida ou esquecida, recebendo tanto o input atual, quanto o input, passando pela [função de ativação sigmoidal](https://en.wikipedia.org/wiki/Sigmoid_function). Se for determinado que a informação armazenada anteriormente, deve-se multiplicar o valor armazenado pelo input, gerando um *dado candidato* a ser mantido na célula de memória.\n",
    "\n",
    "Os registros, por serem logísticos, têm uma grande facilidade de passar pelo *backpropagation*, podem ser aprendidos como devem se comportar em cada caso. O problema de armazenamento é resolvido ao selecionar qual informação deve ser armazenada. Os problemas de gradiente são resolvidos com a possibilidade de atualizar os pesos ao longo do tempo, com uma função facilmente derivável. Assim, as LSTM são uma excelente solução para os dois problemas que dificultavam o uso das RNNs\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Tensorflow conta com um modelo de RNN, podendo ser importado diretamente pela função ```tensorflow.contrib.rnn```. Precisamos passar dois parâmetros, o ```prv_output``` (também chamado de ```h```) e o ```prv_state``` (conhecido como ```c```). Também devemos incializar um vetor de estado ```state```, no caso uma tupla de dois números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'zeros:0' shape=(1, 4) dtype=float32>,\n",
       " <tf.Tensor 'zeros:0' shape=(1, 4) dtype=float32>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=True)\n",
    "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma entrada de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "print (sess.run(sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos passar essa entrada para a LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=array([[ 0.45861578, -0.30320457,  0.12372471, -0.00261922]],\n",
      "      dtype=float32), h=array([[ 0.39968607, -0.23159988,  0.03959474, -0.00215346]],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"LSTM_sample1\"):\n",
    "    output, state_new = lstm_cell(sample_input, state)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (sess.run(state_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos aqui que o estado tem duas 2 partes: o estado ```c``` e o output ```h```. Podemos ver o output a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39968607 -0.23159988  0.03959474 -0.00215346]]\n"
     ]
    }
   ],
   "source": [
    "print (sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## LSTM empilhada (stacked)\n",
    "Uma outra maneira de trabalhar com LSTMs é empilhá-las, sendo cada unidade chamada de \"célula\". Assim, o output de uma é o input de outra, funcionando como camadas de redes neurais profundas, com diferentes graus de abstração e complexidade em cada uma das camadas.\n",
    "\n",
    "Como sempre, devemos começar uma nova sessão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir o tamanho do input, de cada célula e o número de nós escondidos em cada célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 6\n",
    "\n",
    "cells = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeira célula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
    "cell1 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_1)\n",
    "cells.append(cell1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segunda célula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
    "cell2 = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE_2)\n",
    "cells.append(cell2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As células podem ser empilhadas por meio da função ```tf.contrib.rnnMultiRNNCell```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deve-se criar então uma RNN à partir da ```stacked_lstm```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size x time steps x features.\n",
    "data = tf.placeholder(tf.float32, [None, None, input_dim])\n",
    "output, state = tf.nn.dynamic_rnn(stacked_lstm, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O input da RNN será um tensor do formato [batch_size, max_time, dimension]. Se for feito um paralelo, 'dimension' são os dados obtidos de uma observação; 'max_time' é o espaço de tempo considerado para informações correlatas; 'batch_size' seriam observações tiradas em períodos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 3, 4, 3, 2], [1, 2, 1, 1, 1, 2], [1, 2, 2, 2, 2, 2]],\n",
       " [[1, 2, 3, 4, 3, 2], [3, 2, 2, 1, 1, 2], [0, 0, 0, 0, 3, 2]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(?, ?, 5) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido à quantidade de nós escondidos definidos na segunda camada (5), temos uma saída de tamanho diferente da entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.02508579, -0.0288842 , -0.032556  ,  0.01631174,\n",
       "          0.02888456],\n",
       "        [-0.05613008, -0.09874412, -0.10730629,  0.02930988,\n",
       "          0.0627039 ],\n",
       "        [-0.06494982, -0.14582326, -0.16224556,  0.03179003,\n",
       "          0.0591735 ]],\n",
       "\n",
       "       [[-0.02508579, -0.0288842 , -0.032556  ,  0.01631174,\n",
       "          0.02888456],\n",
       "        [-0.05069217, -0.09139941, -0.10482069,  0.02718367,\n",
       "          0.0638642 ],\n",
       "        [-0.04845953, -0.11996509, -0.15019654,  0.02321863,\n",
       "          0.05263352]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(output, feed_dict={data: sample_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## LSTMs para previsão de palavras\n",
    "Agora, vamos aplicar as LSTMs para um problema da vida real. Assim como o teclado do seu celular, vamos tentar prever qual será a róxima palavra que se encaixa no contexto. Tal situação se encaixa em diversos tipos de problemas, como reconhecimento de fala, tradução, legendas e correção de texto.\n",
    "\n",
    "![language-modelling](img/language-modelling.png)\n",
    "\n",
    "Para tal, vamos usar incorporadores de palavras ([word embeddings](https://www.tensorflow.org/tutorials/representation/word2vec)), vetores de n dimensões capazes de representar frases e palavras. Inicialmente, são determinados valores aleatórios para cada palavra. Ao longo do treinamento, os valores se adaptam e nos ajudam a predizer a próxima palavra. Os 'embeddings' agrupam, no espaço vetorial, palavras que são usadas em contextos semelhantes, como palavras que indicam quantidade, lugares ou sentimentos em diferentes grupos.\n",
    "\n",
    "Usaremos o dataset \"The Penn Treebank\", um grande dataset manualmente anotado pela faculdade da Pensilvânia, com alta credibilidade. Assim podemos alimentar o modelo com esse conteúdo, que varia desde textos do Departamento de Energia americano a textos da Livraria da América.\n",
    "\n",
    "O Tensorflow conta até mesmo com uma [função extra](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py), específica para ler o conteúdo do dataset, a ```tensorflow.models.rnn.ptb.reader```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q -O datasets/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "# !unzip -o datasets/ptb.zip -d datasets\n",
    "# !cp datasets/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos baixar o conteúdo do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "# !tar xzf simple-examples.tgz -C datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir os valores de hiperparâmetros e sua estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"datasets/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrutura:\n",
    "- Usaremos duas células de LSTMs. Uma com 256 hidden layers e outra com 128 hidden layers\n",
    "- 20 passos de recorrência\n",
    "\n",
    "Vamos iniciar uma nova sessão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "    session = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "except:\n",
    "    session = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nosso dataset de treino é composto de 929.589 palavras. Agora vamos ver algumas dessas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o módulo reader, vamos criar um iterator, a fim de ler as frases em batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que são nosso dataset foi divido em batches de 60 \"frases\" de 20 palavras. Vamos verificar a seguir as frases, aqui codificadas como IDs únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos dois placeholders: um para input e outro pra target (próxima palavra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos um dicionário, que vai conter o input e os targets. Podemos usá-lo para alimentar o input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}\n",
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Só então vamos empilhar as duas células da LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada LSTM tem duas matrizes: ```c_state```, o estado da célula e ```m_state```, o estado da memória. Como a primeira camada tem 256 hidden layers e input de 60 frases, temos duas matrizes [60x256]. Já na segunda camada, são duas matrizes [60x128]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver as matrizes de estado a seguir (por enquanto, zerados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "As palavras devem ser convertidas para um vetor de números. No caso, utilizar o método de One Hot Encoding não é o mais inteligente, já que são 10000 palavras únicas, ou seja, 10000 categorias. Tal solução seria extremamente custosa. Dessa forma, será criada uma camada da nossa rede, que converterá o input de IDs em uma representação densa, em forma de tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02222355, -0.02239299,  0.02285221, ..., -0.0135777 ,\n",
       "        -0.00966106, -0.01049087],\n",
       "       [-0.02212248,  0.01584486,  0.01178148, ...,  0.00932343,\n",
       "         0.0138042 , -0.0190709 ],\n",
       "       [ 0.01354723,  0.01081765,  0.01197524, ...,  0.01789277,\n",
       "         0.01237494,  0.01284679],\n",
       "       ...,\n",
       "       [-0.00505498, -0.02215985,  0.01811605, ..., -0.00453311,\n",
       "        -0.01991387, -0.00718697],\n",
       "       [-0.00291013, -0.00580085,  0.0231658 , ...,  0.01885387,\n",
       "         0.01027678, -0.01407855],\n",
       "       [-0.01132059,  0.02221517,  0.02410362, ..., -0.01758648,\n",
       "         0.00971607, -0.0231338 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função ```embedding_lookup``` converte o input do ID no vetor correspondente do embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.08730474e-02, -1.24567943e-02, -1.92565564e-02, ...,\n",
       "         1.39541328e-02,  2.42200568e-02,  1.56214535e-02],\n",
       "       [ 8.36488232e-03,  6.21220097e-04, -1.22633232e-02, ...,\n",
       "         7.27566704e-03,  1.49087235e-02, -2.36732066e-02],\n",
       "       [-1.45274960e-03, -9.85317770e-03,  6.40131906e-03, ...,\n",
       "         1.22594312e-02,  1.95758156e-02,  1.97419338e-02],\n",
       "       ...,\n",
       "       [ 9.10164788e-04,  2.15193368e-02, -1.73129663e-02, ...,\n",
       "         2.92157941e-03,  2.29546204e-02,  8.31496343e-03],\n",
       "       [-5.76356798e-03, -1.86453406e-02, -1.01010324e-02, ...,\n",
       "         5.18731214e-03, -1.49471778e-02,  1.81617029e-02],\n",
       "       [-1.33857066e-02,  5.48977219e-03,  9.89437103e-05, ...,\n",
       "         2.34806277e-02, -9.60426405e-03,  2.08830759e-02]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo a RNN\n",
    "A função ```dynamic_rnn``` criará a rede neural recorrente à partir das duas células de LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a segunda camada de LSTMs tem 128 camadas ocultas, sua saída é no formato [60x20x128], sendo 60 o tamanho do batch, 20 o tamanho do input e 128 a profundidade da saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0827976e-04, -1.2370784e-04,  4.1377652e-04, ...,\n",
       "        -3.6192688e-04, -3.1070056e-04, -5.7222578e-04],\n",
       "       [-4.6243705e-04, -2.3241562e-05, -2.0648286e-04, ...,\n",
       "        -2.8773278e-04, -4.0090283e-05, -6.5303245e-04],\n",
       "       [-3.5284500e-04,  3.5875296e-04, -4.4528861e-04, ...,\n",
       "        -1.3894135e-04,  7.7257596e-04, -5.3864077e-04],\n",
       "       ...,\n",
       "       [-3.1978011e-04,  5.9345010e-04,  7.9479569e-04, ...,\n",
       "         4.0902427e-04, -4.5945882e-04,  6.3383649e-04],\n",
       "       [ 4.6206434e-04,  3.5707920e-04,  1.2664311e-03, ...,\n",
       "         8.3880633e-04, -3.3646365e-04,  1.0801820e-03],\n",
       "       [ 7.5855060e-05,  2.3736736e-04,  7.6443620e-04, ...,\n",
       "         1.2507838e-03, -5.9488160e-04,  7.7890075e-04]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para inserir na camada softmax, devemos transformar em uma array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, devemos criar a camada logística, que nos retorna a probabilidade da próxima palavra no nosso universo de 1000 palavras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[1.00774181e-04 9.86271843e-05 9.88556567e-05 ... 9.97972238e-05\n",
      "  1.00062149e-04 9.97601164e-05]\n",
      " [1.00778823e-04 9.86246014e-05 9.88633110e-05 ... 9.97858588e-05\n",
      "  1.00062622e-04 9.97575044e-05]\n",
      " [1.00786878e-04 9.86279338e-05 9.88588872e-05 ... 9.97834140e-05\n",
      "  1.00057558e-04 9.97627285e-05]\n",
      " ...\n",
      " [1.00780358e-04 9.86357336e-05 9.88559914e-05 ... 9.97836978e-05\n",
      "  1.00047888e-04 9.97665484e-05]\n",
      " [1.00791272e-04 9.86357918e-05 9.88568645e-05 ... 9.97784518e-05\n",
      "  1.00052290e-04 9.97813331e-05]\n",
      " [1.00791935e-04 9.86377563e-05 9.88520187e-05 ... 9.97755269e-05\n",
      "  1.00050129e-04 9.97867319e-05]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter as palavras com maior probabilidade, vamos usar a função ```argmax```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1493, 9833, 9833, 7173, 7173, 8935, 8935, 9506, 8935, 9046, 9046,\n",
       "       9046, 9046, 2868, 2868, 5555, 2868, 8935, 8935, 8935])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O correto seriam as seguintes palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou, diretamente do vetor de embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo, treinado com apenas uma época de treinamento não acertou nenhuma das predições. Para melhorar, vamos definir nossa função de perda, no caso, a função ```sequence_loss_by_example```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sua saída indica a [perplexidade](https://pt.wikipedia.org/wiki/Perplexidade) de cada sequência. Quanto menor a perplexidade, melhor o modelo consegue prever o resultado desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.193686, 9.223942, 9.206563, 9.201277, 9.200038, 9.216866,\n",
       "       9.223826, 9.201516, 9.21173 , 9.219634], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir da matriz acima, obtemos a nossa função de custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.22096"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "Usando o gradiente descendente (reduzir a derivada da função), vamos reduzir o erro e obter os melhores pesos e viéses para a nossa função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver todas as variáveis declaradas ao longo do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos a função de gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f121c679f98>,\n",
       " <tf.Tensor 'gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients/add_grad/Reshape_1:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f121c693390>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[ 1.1066734e-06,  1.6076833e-06, -6.0458410e-06, ...,\n",
       "          1.8483920e-06,  9.2165701e-06,  3.6859747e-06],\n",
       "        [ 3.4974846e-06,  5.6411591e-07, -2.2276440e-06, ...,\n",
       "         -1.6159240e-06,  1.3331493e-05,  6.9155358e-06],\n",
       "        [ 4.4597073e-08,  3.4838777e-06, -2.9736254e-06, ...,\n",
       "         -3.2425794e-06,  5.3656540e-06,  8.7939925e-06],\n",
       "        ...,\n",
       "        [ 2.2553311e-06, -3.5619773e-06,  2.9856112e-06, ...,\n",
       "          8.3469286e-06, -1.5622375e-06,  3.2724295e-06],\n",
       "        [ 4.6432278e-06, -4.1974363e-06,  3.6143392e-06, ...,\n",
       "          9.4145171e-06, -1.8534065e-06,  1.7166608e-06],\n",
       "        [ 2.1976766e-06, -5.1447801e-06,  2.4829185e-06, ...,\n",
       "          3.7043799e-06, -8.2279371e-08, -2.4241028e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),\n",
       " array([[ 3.4677434e-08,  2.9318450e-08,  4.5990255e-08, ...,\n",
       "          4.3552966e-08, -1.3607192e-08, -9.6970556e-09],\n",
       "        [-2.9828399e-08,  1.3785576e-07,  3.1502081e-09, ...,\n",
       "         -2.4892127e-08,  6.5763714e-08,  3.5221777e-08],\n",
       "        [-6.1732955e-08, -6.1705975e-08, -2.0313472e-08, ...,\n",
       "         -4.1717602e-08,  8.5752756e-09,  2.3083750e-08],\n",
       "        ...,\n",
       "        [-3.6507486e-09, -5.2366684e-09,  3.0794629e-09, ...,\n",
       "          2.2348694e-08,  3.8287706e-12,  9.6716803e-09],\n",
       "        [ 4.4113286e-09, -2.6569662e-09, -4.9413568e-10, ...,\n",
       "         -3.7982519e-09, -2.2044397e-08,  7.3636497e-09],\n",
       "        [ 3.3158623e-09,  1.0459469e-09,  2.3899565e-09, ...,\n",
       "          3.7456602e-09, -1.6197559e-08,  4.7416915e-09]], dtype=float32),\n",
       " array([ 4.4317067e-06,  6.2707045e-07, -6.1013031e-07, ...,\n",
       "         1.7732010e-06, -5.1477946e-06,  2.0554228e-06], dtype=float32),\n",
       " array([[-4.07521128e-09, -2.86397239e-09, -2.75682965e-09, ...,\n",
       "          1.36677079e-08,  5.13038101e-09,  7.15674320e-09],\n",
       "        [-1.84764790e-08, -1.07720739e-08, -6.20901330e-09, ...,\n",
       "         -1.11823351e-09,  8.53735638e-10,  2.59297495e-09],\n",
       "        [-6.07782091e-09, -2.38841280e-09, -6.29935837e-09, ...,\n",
       "          9.97899807e-09,  5.63998404e-09, -3.92994792e-09],\n",
       "        ...,\n",
       "        [ 2.22203900e-09,  2.16306595e-09,  2.83388535e-09, ...,\n",
       "         -1.14867795e-08, -1.99089656e-09,  1.56183191e-10],\n",
       "        [-1.93908867e-09,  1.18074883e-09,  8.27606650e-10, ...,\n",
       "         -2.82622303e-09, -7.13424519e-09, -9.38380595e-10],\n",
       "        [-1.66823499e-09,  1.79803050e-09,  2.60200350e-09, ...,\n",
       "         -4.72880402e-09, -3.99880085e-09,  5.59853441e-10]], dtype=float32),\n",
       " array([-3.48770004e-06, -4.56429916e-06,  1.16804461e-06,  3.73866919e-06,\n",
       "         4.25715939e-07,  1.87848582e-06,  1.99309028e-07,  6.99821044e-07,\n",
       "         4.29905913e-06, -4.35722495e-06,  5.66049175e-06,  1.81361725e-06,\n",
       "         1.83198995e-06, -5.98012321e-07, -2.48466267e-06,  1.11172881e-06,\n",
       "         2.26142902e-06,  3.83292161e-07, -1.18132516e-06,  8.73296358e-06,\n",
       "        -1.94821314e-06, -4.39474161e-06, -3.77237234e-06, -3.02019589e-06,\n",
       "        -6.16538910e-06, -2.38039956e-06, -3.19011542e-06,  2.70832766e-06,\n",
       "         2.10729422e-06, -3.13561600e-06,  4.88790056e-06,  8.82204404e-06,\n",
       "         8.72693636e-06,  2.43247109e-06, -7.85074235e-07,  1.96647420e-06,\n",
       "        -3.93037089e-06, -1.42953354e-06,  7.39881557e-07, -9.22751838e-07,\n",
       "         4.43128238e-06, -1.00819739e-06, -8.41854046e-07,  5.88306875e-06,\n",
       "         1.79930441e-06,  3.57894714e-06,  7.16482816e-07,  3.39952919e-07,\n",
       "        -1.07217729e-05,  5.26034682e-09,  4.03611875e-06,  3.08094923e-06,\n",
       "         5.10187374e-06, -8.49385231e-07, -3.14426211e-06,  2.46571813e-06,\n",
       "         2.09509781e-05,  1.97080726e-06,  7.39861343e-06,  1.07305380e-06,\n",
       "         1.69592147e-06, -1.75075490e-06,  9.49662990e-06, -1.01961086e-06,\n",
       "         5.41943518e-06,  9.14094869e-07,  1.15955322e-06, -1.21304345e-06,\n",
       "         7.07714025e-06, -3.05378734e-07, -2.33515124e-07, -4.01512807e-06,\n",
       "         2.13261956e-06, -7.81912058e-06,  2.46058357e-06, -6.83144776e-07,\n",
       "        -2.88328442e-06,  5.24698225e-06, -2.07192920e-06, -2.96373059e-06,\n",
       "         2.81835878e-06,  4.63505830e-06,  9.59354293e-06,  1.10231917e-06,\n",
       "        -1.65490269e-06,  3.83985662e-06, -1.81472660e-06, -1.83595580e-06,\n",
       "         1.77100992e-06,  4.85333567e-06,  5.67395819e-06, -1.71119052e-06,\n",
       "         1.44172793e-06,  2.06479240e-06, -4.27747864e-07,  4.17830051e-06,\n",
       "         7.14547298e-07, -1.25529164e-06, -4.05643448e-07, -4.33334890e-06,\n",
       "        -2.02052524e-06,  1.85464637e-06, -2.82327414e-06, -4.64272119e-07,\n",
       "         1.48496815e-06, -3.79903372e-06, -1.48552067e-06,  3.53308792e-07,\n",
       "        -3.53496580e-06,  2.39573365e-06,  8.41064946e-07,  1.60957870e-06,\n",
       "        -1.77927996e-06,  7.15814906e-07,  3.75688910e-06,  2.31301851e-06,\n",
       "         3.83759243e-06, -1.05352660e-06, -2.36887422e-06,  2.69737666e-06,\n",
       "        -3.29282898e-06,  4.67813607e-06,  4.13403268e-06, -5.95171628e-07,\n",
       "         1.01522869e-06,  4.62466824e-06,  2.72445254e-06,  1.85731471e-06,\n",
       "         1.32118929e-02,  6.40854798e-03,  1.85919739e-02, -2.30162628e-02,\n",
       "        -1.54467775e-02, -6.23120600e-03,  9.73748136e-03, -1.90023351e-02,\n",
       "        -1.97694055e-03, -1.79148689e-02,  1.90387145e-02, -1.68035179e-02,\n",
       "         2.75339782e-02, -1.30151613e-02, -1.72336977e-02, -9.10937227e-03,\n",
       "        -7.43712764e-03, -1.38517946e-03, -7.09384494e-03,  9.59272217e-03,\n",
       "         2.49642357e-02,  9.41648148e-03,  2.25527212e-02, -9.80979204e-03,\n",
       "        -2.49508712e-02, -2.66126962e-03, -1.17506394e-02,  1.51001653e-02,\n",
       "        -1.99628412e-05, -8.25771596e-03, -2.37327665e-02,  2.51968298e-02,\n",
       "        -4.70645614e-02, -1.38750989e-02,  7.44028436e-03, -1.82433277e-02,\n",
       "         2.37853979e-04,  5.85484505e-03, -1.11216691e-03, -1.66496411e-02,\n",
       "        -1.32896565e-02,  7.95664173e-03, -6.26766495e-03,  7.03604566e-03,\n",
       "        -4.02910262e-03,  1.90743227e-02,  2.16297228e-02,  1.39820494e-03,\n",
       "         3.19414996e-02,  7.31028244e-03,  3.05591803e-03, -4.52631107e-03,\n",
       "        -1.07574947e-02, -2.14944296e-02, -1.86383594e-02, -1.48431677e-03,\n",
       "        -4.01608907e-02,  1.85858812e-02, -5.37751615e-03, -2.41448614e-03,\n",
       "        -2.47975718e-03,  2.13001333e-02, -3.28509063e-02,  1.73593070e-02,\n",
       "         9.15621594e-03, -3.13602109e-03,  1.96025372e-02,  5.34765422e-03,\n",
       "        -2.33433824e-02, -6.67998195e-03,  4.47370391e-03,  5.22656692e-03,\n",
       "         2.65716296e-03,  6.27778703e-03, -1.90515704e-02, -2.78559211e-03,\n",
       "        -6.53144845e-04, -1.00944061e-02, -6.97089452e-03,  7.47745158e-04,\n",
       "         6.13295822e-04,  2.53459476e-02, -2.99687367e-02,  3.24484371e-02,\n",
       "         6.90421462e-03, -8.02250486e-03,  1.78714748e-03, -8.73328466e-03,\n",
       "         2.58797372e-04,  5.33040147e-03, -1.96245443e-02,  8.39706883e-03,\n",
       "         7.97824748e-03, -7.57649075e-03,  2.67063966e-03, -5.99758560e-03,\n",
       "         4.33389051e-03,  1.32018812e-02, -1.37240142e-02,  7.68954866e-03,\n",
       "         4.66242572e-03, -1.83609184e-02, -2.04289965e-02, -1.28275659e-02,\n",
       "        -6.41634455e-03, -2.43663825e-02, -8.39769095e-03,  8.65294319e-03,\n",
       "        -2.20380276e-02,  3.95575911e-03, -8.63438821e-04,  2.17055269e-02,\n",
       "        -1.26105808e-02, -3.47282109e-03, -2.76742000e-02, -2.07118541e-02,\n",
       "         5.93028637e-03, -2.49808142e-03, -6.65948540e-03, -5.61605114e-03,\n",
       "        -1.67397223e-02,  1.59948561e-02,  6.50889240e-03,  4.03535226e-03,\n",
       "         3.54298158e-03, -2.34743785e-02, -1.88093279e-02,  8.92015174e-03,\n",
       "        -7.53062125e-07, -2.61896912e-06,  2.45112233e-07,  3.22344158e-06,\n",
       "        -5.23530048e-07,  1.76115441e-06, -2.87036846e-06,  6.95300741e-07,\n",
       "         5.74437280e-08, -3.62096216e-06,  1.83589498e-06,  3.70818611e-07,\n",
       "         4.29063903e-06,  8.61951037e-07,  2.03743753e-06,  1.19476067e-06,\n",
       "         1.38862003e-06,  1.35260188e-06, -2.47222215e-06,  6.95116614e-06,\n",
       "         1.73614194e-06, -1.97910549e-06, -8.46725470e-07, -2.09765176e-06,\n",
       "        -1.43861939e-06, -2.12391319e-06, -4.42888540e-06,  2.13246017e-06,\n",
       "         3.73308308e-06,  2.40411708e-07,  4.44108673e-06,  9.77120726e-06,\n",
       "         6.59584748e-06,  1.06107836e-06,  1.72064460e-08,  4.93579637e-06,\n",
       "        -2.25233293e-06, -1.82514509e-06,  2.08647293e-06,  3.69520706e-07,\n",
       "         1.35925313e-06, -3.09398530e-07,  6.19646585e-07,  5.73955822e-06,\n",
       "         8.57339273e-07,  4.82331552e-06, -1.32171203e-06, -1.54626366e-06,\n",
       "        -6.98172789e-06, -2.24234540e-07,  9.05255888e-07,  1.90580465e-06,\n",
       "         3.72061299e-06, -1.39661984e-06, -2.91328774e-06,  1.96331030e-06,\n",
       "         1.47874080e-05,  8.48511888e-07,  7.84886083e-07, -2.03558216e-06,\n",
       "        -7.42954398e-07, -4.20924380e-06,  4.56935413e-06, -1.62253923e-06,\n",
       "         6.83043436e-06,  5.71722353e-07,  1.67592034e-07, -4.63721835e-06,\n",
       "         5.80223332e-06,  2.35549777e-07,  1.91682761e-06, -1.88957836e-06,\n",
       "         8.87285267e-08, -6.27379723e-06,  3.03800311e-06,  1.63812729e-07,\n",
       "        -2.71249974e-06,  2.44238208e-06, -1.05735239e-06, -2.76887192e-07,\n",
       "         4.59881505e-07,  4.38790903e-06,  5.69291205e-06,  1.22082531e-06,\n",
       "        -4.48191804e-06, -1.19960850e-06, -1.89506068e-07, -1.66893574e-06,\n",
       "         1.57734439e-06,  3.40865859e-06,  3.28627561e-06, -1.74726694e-07,\n",
       "        -1.08263009e-06, -4.11451140e-07, -2.84722290e-07,  2.62743333e-06,\n",
       "         1.09662528e-06,  8.90255365e-07, -3.29036226e-07, -1.68120675e-06,\n",
       "        -1.34188792e-06, -5.85073963e-07, -1.42253066e-06, -3.70184978e-07,\n",
       "        -1.81724715e-06, -4.19379194e-06, -3.63517444e-07,  1.51080962e-06,\n",
       "        -4.37605877e-06,  1.47719527e-06,  2.05896811e-08,  4.38445295e-06,\n",
       "        -2.51987422e-06,  2.88626461e-06,  9.80535674e-07,  1.72373586e-06,\n",
       "         3.28896249e-06,  1.65024016e-06, -3.02071044e-06,  1.82955580e-06,\n",
       "        -1.12055886e-06,  2.35984589e-06, -1.44681394e-06, -2.26943911e-07,\n",
       "        -1.54817815e-06,  3.98891643e-06,  4.25263443e-06,  3.78822028e-06,\n",
       "        -3.48686171e-06, -4.56121552e-06,  1.16472984e-06,  3.73906778e-06,\n",
       "         4.22933908e-07,  1.87908893e-06,  1.99946669e-07,  6.94899200e-07,\n",
       "         4.29371948e-06, -4.35710217e-06,  5.66464223e-06,  1.81485427e-06,\n",
       "         1.83693328e-06, -5.96315544e-07, -2.48314700e-06,  1.11008399e-06,\n",
       "         2.26394695e-06,  3.82048114e-07, -1.18319485e-06,  8.73200406e-06,\n",
       "        -1.94645531e-06, -4.39438418e-06, -3.76156731e-06, -3.02406488e-06,\n",
       "        -6.17312071e-06, -2.38516509e-06, -3.19077890e-06,  2.71049589e-06,\n",
       "         2.10409144e-06, -3.13802275e-06,  4.88649448e-06,  8.81359392e-06,\n",
       "         8.72102828e-06,  2.44624744e-06, -7.92399987e-07,  1.96432097e-06,\n",
       "        -3.93194023e-06, -1.42645126e-06,  7.39933967e-07, -9.21841888e-07,\n",
       "         4.43238378e-06, -1.00584180e-06, -8.39426889e-07,  5.88363355e-06,\n",
       "         1.79447147e-06,  3.58204852e-06,  7.15959459e-07,  3.40192827e-07,\n",
       "        -1.07228298e-05,  6.86912927e-09,  4.03107379e-06,  3.07905520e-06,\n",
       "         5.09773827e-06, -8.43699240e-07, -3.14644331e-06,  2.46760374e-06,\n",
       "         2.09536502e-05,  1.97531381e-06,  7.39793268e-06,  1.07022038e-06,\n",
       "         1.70199542e-06, -1.75545301e-06,  9.49740661e-06, -1.02154650e-06,\n",
       "         5.42389034e-06,  9.11835343e-07,  1.15720673e-06, -1.21192784e-06,\n",
       "         7.07392110e-06, -3.07102709e-07, -2.33439067e-07, -4.01165607e-06,\n",
       "         2.13506291e-06, -7.81967810e-06,  2.46191871e-06, -6.82441964e-07,\n",
       "        -2.88548904e-06,  5.25334235e-06, -2.07089192e-06, -2.96326334e-06,\n",
       "         2.81809866e-06,  4.63276228e-06,  9.58478631e-06,  1.10364772e-06,\n",
       "        -1.66408483e-06,  3.84235409e-06, -1.81284906e-06, -1.83107807e-06,\n",
       "         1.77027755e-06,  4.85301325e-06,  5.67182360e-06, -1.71455167e-06,\n",
       "         1.43868272e-06,  2.06457730e-06, -4.24722373e-07,  4.18186528e-06,\n",
       "         7.16485829e-07, -1.25882627e-06, -4.04967381e-07, -4.33687819e-06,\n",
       "        -2.01555213e-06,  1.85806857e-06, -2.82127849e-06, -4.62568607e-07,\n",
       "         1.48699792e-06, -3.79887388e-06, -1.48710183e-06,  3.49374602e-07,\n",
       "        -3.54490066e-06,  2.39358747e-06,  8.41883036e-07,  1.61587332e-06,\n",
       "        -1.77376660e-06,  7.09509663e-07,  3.75621539e-06,  2.31437321e-06,\n",
       "         3.83809356e-06, -1.05243271e-06, -2.36656069e-06,  2.69870657e-06,\n",
       "        -3.28795227e-06,  4.67764812e-06,  4.13539510e-06, -5.94499909e-07,\n",
       "         1.01197611e-06,  4.62725984e-06,  2.71792192e-06,  1.85780561e-06],\n",
       "       dtype=float32),\n",
       " array([[-1.36933377e-05, -1.27977628e-05,  9.66352163e-05, ...,\n",
       "         -1.04789727e-07, -1.07156289e-07, -1.07225404e-07],\n",
       "        [ 2.08307993e-05, -1.95522098e-05, -2.39199202e-04, ...,\n",
       "          1.09565484e-07,  1.12018341e-07,  1.12112104e-07],\n",
       "        [-1.66790069e-05, -3.40451807e-04,  3.04649548e-05, ...,\n",
       "          1.21633917e-07,  1.24328636e-07,  1.24495330e-07],\n",
       "        ...,\n",
       "        [ 1.04273451e-04, -8.72103992e-05,  4.00753015e-05, ...,\n",
       "         -1.58891581e-07, -1.62493919e-07, -1.62642721e-07],\n",
       "        [-3.82605940e-05, -7.89334663e-05,  7.37209921e-05, ...,\n",
       "          3.05911598e-08,  3.12648609e-08,  3.12943520e-08],\n",
       "        [-2.96987913e-04, -3.86143191e-04, -2.12351333e-05, ...,\n",
       "          4.63409066e-07,  4.73735497e-07,  4.74363105e-07]], dtype=float32),\n",
       " array([-0.7813506 , -1.0979968 , -0.9813153 , ...,  0.00197197,\n",
       "         0.00201595,  0.00201843], dtype=float32)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos treinar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe LSTM\n",
    "Vamos usar programação orientada a objetos para criar uma classe que implementa as funções acima definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "    \n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar funções para nos avisar do progresso do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 3935.783 speed: 15986 wps\n",
      "Itr 87 of 774, perplexity: 1262.493 speed: 18209 wps\n",
      "Itr 164 of 774, perplexity: 970.080 speed: 18048 wps\n",
      "Itr 241 of 774, perplexity: 806.905 speed: 16529 wps\n",
      "Itr 318 of 774, perplexity: 713.001 speed: 16258 wps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5ef6a77fa31d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Run the loop for this epoch in the training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d : Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-0f72e283cf78>\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(session, m, data, eval_op, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                      {m.input_data: x,\n\u001b[1;32m     20\u001b[0m                                       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                       m.initial_state: state})\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#Add returned cost to costs (which keeps track of the total costs for this epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
