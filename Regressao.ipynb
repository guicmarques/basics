{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão\n",
    "Um dos métodos mais usados em estatística para estimar valores ou classes.\n",
    "\n",
    "1. [Regressão Linear](#Regress%C3%A3o-Linear)\n",
    "2. [Regressão Linear com Tensorflow](#Regressão-linear-no-Tensorflow)\n",
    "3. [Regressão Logística](#Regressão-Logística)\n",
    "4. [Regressão Logística com Tensorflow](#Regressão-Logística-com-Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "A regressão linear consiste na aproximação de um modelo linear, utilizando os dados para criar uma reta, relacionando o valor das variáveis com um valor objetivo (variável Y, ou variável dependente). A regressão pode contar com um ou mais varáveis independentes, conhecido como regressão linear múltipla.\n",
    "\n",
    "A equação do modelo linear simples é:\n",
    "\n",
    "$$Y = a X + b $$\n",
    "\n",
    "Sendo Y a variável de interesse e X a variável independente. ```a``` é o coeficiente angular ('slope' ou 'gradient'), indicando a tangente entre a reta e o eixo x. ```b``` é o coeficiente linear ('intercept') e indica o ponto que intercepta o eixo y quando x = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos variáveis independentes (valores entre 0 e 5 com passo de 0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n",
       "       1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n",
       "       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n",
       "       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0.0, 5.0, 0.1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 5\n",
    "\n",
    "Y= a * X + b \n",
    "\n",
    "plt.plot(X, Y) \n",
    "plt.ylabel('Dependent Variable')\n",
    "plt.xlabel('Independent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear no Tensorflow\n",
    "Para observar a aplicação de regressão linear com Tensorflow, uso um [dataset](http://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64) que correlaciona consumo de combustível com emissão de dióxido de carbono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-22 23:45:00--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.193\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72629 (71K) [text/csv]\n",
      "Saving to: ‘FuelConsumption.csv’\n",
      "\n",
      "FuelConsumption.csv 100%[===================>]  70,93K   140KB/s    in 0,5s    \n",
      "\n",
      "2018-12-22 23:45:02 (140 KB/s) - ‘FuelConsumption.csv’ saved [72629/72629]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODELYEAR</th>\n",
       "      <th>MAKE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>VEHICLECLASS</th>\n",
       "      <th>ENGINESIZE</th>\n",
       "      <th>CYLINDERS</th>\n",
       "      <th>TRANSMISSION</th>\n",
       "      <th>FUELTYPE</th>\n",
       "      <th>FUELCONSUMPTION_CITY</th>\n",
       "      <th>FUELCONSUMPTION_HWY</th>\n",
       "      <th>FUELCONSUMPTION_COMB</th>\n",
       "      <th>FUELCONSUMPTION_COMB_MPG</th>\n",
       "      <th>CO2EMISSIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>ILX</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AS5</td>\n",
       "      <td>Z</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>33</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>ILX</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>29</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>ILX HYBRID</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4</td>\n",
       "      <td>AV7</td>\n",
       "      <td>Z</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>48</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>MDX 4WD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.7</td>\n",
       "      <td>9.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>25</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>RDX AWD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.1</td>\n",
       "      <td>8.7</td>\n",
       "      <td>10.6</td>\n",
       "      <td>27</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>RLX</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TL</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.8</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>28</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TL AWD</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>25</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TL AWD</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>24</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TSX</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4</td>\n",
       "      <td>AS5</td>\n",
       "      <td>Z</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.2</td>\n",
       "      <td>31</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TSX</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>29</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014</td>\n",
       "      <td>ACURA</td>\n",
       "      <td>TSX</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6</td>\n",
       "      <td>AS5</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>27</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>DB9</td>\n",
       "      <td>MINICOMPACT</td>\n",
       "      <td>5.9</td>\n",
       "      <td>12</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>RAPIDE</td>\n",
       "      <td>SUBCOMPACT</td>\n",
       "      <td>5.9</td>\n",
       "      <td>12</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>V8 VANTAGE</td>\n",
       "      <td>TWO-SEATER</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>AM7</td>\n",
       "      <td>Z</td>\n",
       "      <td>17.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>14.7</td>\n",
       "      <td>19</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>V8 VANTAGE</td>\n",
       "      <td>TWO-SEATER</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>V8 VANTAGE S</td>\n",
       "      <td>TWO-SEATER</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>AM7</td>\n",
       "      <td>Z</td>\n",
       "      <td>17.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>14.7</td>\n",
       "      <td>19</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>V8 VANTAGE S</td>\n",
       "      <td>TWO-SEATER</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014</td>\n",
       "      <td>ASTON MARTIN</td>\n",
       "      <td>VANQUISH</td>\n",
       "      <td>MINICOMPACT</td>\n",
       "      <td>5.9</td>\n",
       "      <td>12</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A4</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AV8</td>\n",
       "      <td>Z</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.8</td>\n",
       "      <td>32</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A4 QUATTRO</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A4 QUATTRO</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.3</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A5 CABRIOLET QUATTRO</td>\n",
       "      <td>SUBCOMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A5 QUATTRO</td>\n",
       "      <td>SUBCOMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A5 QUATTRO</td>\n",
       "      <td>SUBCOMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.3</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A6 QUATTRO</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>28</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A6 QUATTRO</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>10.9</td>\n",
       "      <td>26</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A6 QUATTRO TDI CLEAN DIESEL</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>D</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>34</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A7 QUATTRO</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>13.1</td>\n",
       "      <td>8.8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>25</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>A7 QUATTRO TDI CLEAN DIESEL</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>D</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>34</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M5</td>\n",
       "      <td>X</td>\n",
       "      <td>10.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA GLI</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>10.2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA GLI</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>31</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA TDI CLEAN DIESEL</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>D</td>\n",
       "      <td>7.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.9</td>\n",
       "      <td>41</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA TDI CLEAN DIESEL</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>D</td>\n",
       "      <td>7.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>41</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>JETTA TURBO HYBRID</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4</td>\n",
       "      <td>AM7</td>\n",
       "      <td>Z</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>52</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>X</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>33</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4</td>\n",
       "      <td>M5</td>\n",
       "      <td>X</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>33</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>A6</td>\n",
       "      <td>X</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>29</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>M5</td>\n",
       "      <td>X</td>\n",
       "      <td>11.4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>9.8</td>\n",
       "      <td>29</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>12.4</td>\n",
       "      <td>8.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>26</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT TDI CLEAN DIESEL</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>D</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>40</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>PASSAT TDI CLEAN DIESEL</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>D</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>6.8</td>\n",
       "      <td>42</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>ROUTAN</td>\n",
       "      <td>MINIVAN</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6</td>\n",
       "      <td>A6</td>\n",
       "      <td>X</td>\n",
       "      <td>14.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>23</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>TIGUAN</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.7</td>\n",
       "      <td>26</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>TIGUAN</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>M6</td>\n",
       "      <td>Z</td>\n",
       "      <td>13.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>24</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>TIGUAN 4MOTION</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>A6</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>26</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>TOUAREG</td>\n",
       "      <td>SUV - STANDARD</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>Z</td>\n",
       "      <td>13.8</td>\n",
       "      <td>10.3</td>\n",
       "      <td>12.2</td>\n",
       "      <td>23</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>TOUAREG TDI CLEAN DIESEL</td>\n",
       "      <td>SUV - STANDARD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS8</td>\n",
       "      <td>D</td>\n",
       "      <td>12.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>27</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S60</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>29</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S60 AWD</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>28</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S60 AWD</td>\n",
       "      <td>COMPACT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S80</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>11.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>28</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S80 AWD</td>\n",
       "      <td>MID-SIZE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC60</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>25</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC60 AWD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>24</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC60 AWD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>25</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC70 AWD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>24</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC70 AWD</td>\n",
       "      <td>SUV - SMALL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>12.9</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>2014</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>XC90 AWD</td>\n",
       "      <td>SUV - STANDARD</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>AS6</td>\n",
       "      <td>X</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.2</td>\n",
       "      <td>12.8</td>\n",
       "      <td>22</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1067 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MODELYEAR          MAKE                        MODEL    VEHICLECLASS  \\\n",
       "0          2014         ACURA                          ILX         COMPACT   \n",
       "1          2014         ACURA                          ILX         COMPACT   \n",
       "2          2014         ACURA                   ILX HYBRID         COMPACT   \n",
       "3          2014         ACURA                      MDX 4WD     SUV - SMALL   \n",
       "4          2014         ACURA                      RDX AWD     SUV - SMALL   \n",
       "5          2014         ACURA                          RLX        MID-SIZE   \n",
       "6          2014         ACURA                           TL        MID-SIZE   \n",
       "7          2014         ACURA                       TL AWD        MID-SIZE   \n",
       "8          2014         ACURA                       TL AWD        MID-SIZE   \n",
       "9          2014         ACURA                          TSX         COMPACT   \n",
       "10         2014         ACURA                          TSX         COMPACT   \n",
       "11         2014         ACURA                          TSX         COMPACT   \n",
       "12         2014  ASTON MARTIN                          DB9     MINICOMPACT   \n",
       "13         2014  ASTON MARTIN                       RAPIDE      SUBCOMPACT   \n",
       "14         2014  ASTON MARTIN                   V8 VANTAGE      TWO-SEATER   \n",
       "15         2014  ASTON MARTIN                   V8 VANTAGE      TWO-SEATER   \n",
       "16         2014  ASTON MARTIN                 V8 VANTAGE S      TWO-SEATER   \n",
       "17         2014  ASTON MARTIN                 V8 VANTAGE S      TWO-SEATER   \n",
       "18         2014  ASTON MARTIN                     VANQUISH     MINICOMPACT   \n",
       "19         2014          AUDI                           A4         COMPACT   \n",
       "20         2014          AUDI                   A4 QUATTRO         COMPACT   \n",
       "21         2014          AUDI                   A4 QUATTRO         COMPACT   \n",
       "22         2014          AUDI         A5 CABRIOLET QUATTRO      SUBCOMPACT   \n",
       "23         2014          AUDI                   A5 QUATTRO      SUBCOMPACT   \n",
       "24         2014          AUDI                   A5 QUATTRO      SUBCOMPACT   \n",
       "25         2014          AUDI                   A6 QUATTRO        MID-SIZE   \n",
       "26         2014          AUDI                   A6 QUATTRO        MID-SIZE   \n",
       "27         2014          AUDI  A6 QUATTRO TDI CLEAN DIESEL        MID-SIZE   \n",
       "28         2014          AUDI                   A7 QUATTRO        MID-SIZE   \n",
       "29         2014          AUDI  A7 QUATTRO TDI CLEAN DIESEL        MID-SIZE   \n",
       "...         ...           ...                          ...             ...   \n",
       "1037       2014    VOLKSWAGEN                        JETTA         COMPACT   \n",
       "1038       2014    VOLKSWAGEN                    JETTA GLI         COMPACT   \n",
       "1039       2014    VOLKSWAGEN                    JETTA GLI         COMPACT   \n",
       "1040       2014    VOLKSWAGEN       JETTA TDI CLEAN DIESEL         COMPACT   \n",
       "1041       2014    VOLKSWAGEN       JETTA TDI CLEAN DIESEL         COMPACT   \n",
       "1042       2014    VOLKSWAGEN           JETTA TURBO HYBRID         COMPACT   \n",
       "1043       2014    VOLKSWAGEN                       PASSAT        MID-SIZE   \n",
       "1044       2014    VOLKSWAGEN                       PASSAT        MID-SIZE   \n",
       "1045       2014    VOLKSWAGEN                       PASSAT        MID-SIZE   \n",
       "1046       2014    VOLKSWAGEN                       PASSAT        MID-SIZE   \n",
       "1047       2014    VOLKSWAGEN                       PASSAT        MID-SIZE   \n",
       "1048       2014    VOLKSWAGEN      PASSAT TDI CLEAN DIESEL        MID-SIZE   \n",
       "1049       2014    VOLKSWAGEN      PASSAT TDI CLEAN DIESEL        MID-SIZE   \n",
       "1050       2014    VOLKSWAGEN                       ROUTAN         MINIVAN   \n",
       "1051       2014    VOLKSWAGEN                       TIGUAN     SUV - SMALL   \n",
       "1052       2014    VOLKSWAGEN                       TIGUAN     SUV - SMALL   \n",
       "1053       2014    VOLKSWAGEN               TIGUAN 4MOTION     SUV - SMALL   \n",
       "1054       2014    VOLKSWAGEN                      TOUAREG  SUV - STANDARD   \n",
       "1055       2014    VOLKSWAGEN     TOUAREG TDI CLEAN DIESEL  SUV - STANDARD   \n",
       "1056       2014         VOLVO                          S60         COMPACT   \n",
       "1057       2014         VOLVO                      S60 AWD         COMPACT   \n",
       "1058       2014         VOLVO                      S60 AWD         COMPACT   \n",
       "1059       2014         VOLVO                          S80        MID-SIZE   \n",
       "1060       2014         VOLVO                      S80 AWD        MID-SIZE   \n",
       "1061       2014         VOLVO                         XC60     SUV - SMALL   \n",
       "1062       2014         VOLVO                     XC60 AWD     SUV - SMALL   \n",
       "1063       2014         VOLVO                     XC60 AWD     SUV - SMALL   \n",
       "1064       2014         VOLVO                     XC70 AWD     SUV - SMALL   \n",
       "1065       2014         VOLVO                     XC70 AWD     SUV - SMALL   \n",
       "1066       2014         VOLVO                     XC90 AWD  SUV - STANDARD   \n",
       "\n",
       "      ENGINESIZE  CYLINDERS TRANSMISSION FUELTYPE  FUELCONSUMPTION_CITY  \\\n",
       "0            2.0          4          AS5        Z                   9.9   \n",
       "1            2.4          4           M6        Z                  11.2   \n",
       "2            1.5          4          AV7        Z                   6.0   \n",
       "3            3.5          6          AS6        Z                  12.7   \n",
       "4            3.5          6          AS6        Z                  12.1   \n",
       "5            3.5          6          AS6        Z                  11.9   \n",
       "6            3.5          6          AS6        Z                  11.8   \n",
       "7            3.7          6          AS6        Z                  12.8   \n",
       "8            3.7          6           M6        Z                  13.4   \n",
       "9            2.4          4          AS5        Z                  10.6   \n",
       "10           2.4          4           M6        Z                  11.2   \n",
       "11           3.5          6          AS5        Z                  12.1   \n",
       "12           5.9         12           A6        Z                  18.0   \n",
       "13           5.9         12           A6        Z                  18.0   \n",
       "14           4.7          8          AM7        Z                  17.4   \n",
       "15           4.7          8           M6        Z                  18.1   \n",
       "16           4.7          8          AM7        Z                  17.4   \n",
       "17           4.7          8           M6        Z                  18.1   \n",
       "18           5.9         12           A6        Z                  18.0   \n",
       "19           2.0          4          AV8        Z                   9.9   \n",
       "20           2.0          4          AS8        Z                  11.5   \n",
       "21           2.0          4           M6        Z                  10.8   \n",
       "22           2.0          4          AS8        Z                  11.5   \n",
       "23           2.0          4          AS8        Z                  11.5   \n",
       "24           2.0          4           M6        Z                  10.8   \n",
       "25           2.0          4          AS8        Z                  12.0   \n",
       "26           3.0          6          AS8        Z                  12.8   \n",
       "27           3.0          6          AS8        D                   9.8   \n",
       "28           3.0          6          AS8        Z                  13.1   \n",
       "29           3.0          6          AS8        D                   9.8   \n",
       "...          ...        ...          ...      ...                   ...   \n",
       "1037         2.0          4           M5        X                  10.4   \n",
       "1038         2.0          4           A6        Z                  10.2   \n",
       "1039         2.0          4           M6        Z                  10.6   \n",
       "1040         2.0          4           A6        D                   7.9   \n",
       "1041         2.0          4           M6        D                   7.9   \n",
       "1042         1.4          4          AM7        Z                   5.6   \n",
       "1043         1.8          4           A6        X                   9.9   \n",
       "1044         1.8          4           M5        X                  10.0   \n",
       "1045         2.5          5           A6        X                  11.0   \n",
       "1046         2.5          5           M5        X                  11.4   \n",
       "1047         3.6          6           A6        Z                  12.4   \n",
       "1048         2.0          4           A6        D                   8.1   \n",
       "1049         2.0          4           M6        D                   8.0   \n",
       "1050         3.6          6           A6        X                  14.2   \n",
       "1051         2.0          4           A6        Z                  11.7   \n",
       "1052         2.0          4           M6        Z                  13.6   \n",
       "1053         2.0          4           A6        Z                  11.7   \n",
       "1054         3.6          6          AS8        Z                  13.8   \n",
       "1055         3.0          6          AS8        D                  12.3   \n",
       "1056         2.5          5          AS6        X                  11.3   \n",
       "1057         2.5          5          AS6        X                  11.6   \n",
       "1058         3.0          6          AS6        X                  13.2   \n",
       "1059         3.2          6          AS6        X                  11.9   \n",
       "1060         3.0          6          AS6        X                  13.2   \n",
       "1061         3.2          6          AS6        X                  13.0   \n",
       "1062         3.0          6          AS6        X                  13.4   \n",
       "1063         3.2          6          AS6        X                  13.2   \n",
       "1064         3.0          6          AS6        X                  13.4   \n",
       "1065         3.2          6          AS6        X                  12.9   \n",
       "1066         3.2          6          AS6        X                  14.9   \n",
       "\n",
       "      FUELCONSUMPTION_HWY  FUELCONSUMPTION_COMB  FUELCONSUMPTION_COMB_MPG  \\\n",
       "0                     6.7                   8.5                        33   \n",
       "1                     7.7                   9.6                        29   \n",
       "2                     5.8                   5.9                        48   \n",
       "3                     9.1                  11.1                        25   \n",
       "4                     8.7                  10.6                        27   \n",
       "5                     7.7                  10.0                        28   \n",
       "6                     8.1                  10.1                        28   \n",
       "7                     9.0                  11.1                        25   \n",
       "8                     9.5                  11.6                        24   \n",
       "9                     7.5                   9.2                        31   \n",
       "10                    8.1                   9.8                        29   \n",
       "11                    8.3                  10.4                        27   \n",
       "12                   12.6                  15.6                        18   \n",
       "13                   12.6                  15.6                        18   \n",
       "14                   11.3                  14.7                        19   \n",
       "15                   12.2                  15.4                        18   \n",
       "16                   11.3                  14.7                        19   \n",
       "17                   12.2                  15.4                        18   \n",
       "18                   12.6                  15.6                        18   \n",
       "19                    7.4                   8.8                        32   \n",
       "20                    8.1                  10.0                        28   \n",
       "21                    7.5                   9.3                        30   \n",
       "22                    8.1                  10.0                        28   \n",
       "23                    8.1                  10.0                        28   \n",
       "24                    7.5                   9.3                        30   \n",
       "25                    8.1                  10.2                        28   \n",
       "26                    8.6                  10.9                        26   \n",
       "27                    6.4                   8.3                        34   \n",
       "28                    8.8                  11.2                        25   \n",
       "29                    6.4                   8.3                        34   \n",
       "...                   ...                   ...                       ...   \n",
       "1037                  7.2                   9.0                        31   \n",
       "1038                  7.5                   9.0                        31   \n",
       "1039                  7.4                   9.2                        31   \n",
       "1040                  5.7                   6.9                        41   \n",
       "1041                  5.6                   6.9                        41   \n",
       "1042                  5.2                   5.4                        52   \n",
       "1043                  6.9                   8.6                        33   \n",
       "1044                  6.9                   8.6                        33   \n",
       "1045                  8.0                   9.7                        29   \n",
       "1046                  7.8                   9.8                        29   \n",
       "1047                  8.8                  10.8                        26   \n",
       "1048                  5.9                   7.1                        40   \n",
       "1049                  5.4                   6.8                        42   \n",
       "1050                  9.5                  12.1                        23   \n",
       "1051                  9.5                  10.7                        26   \n",
       "1052                  9.2                  11.6                        24   \n",
       "1053                  9.4                  10.7                        26   \n",
       "1054                 10.3                  12.2                        23   \n",
       "1055                  8.0                  10.4                        27   \n",
       "1056                  7.8                   9.7                        29   \n",
       "1057                  8.3                  10.1                        28   \n",
       "1058                  9.5                  11.5                        25   \n",
       "1059                  8.1                  10.2                        28   \n",
       "1060                  9.5                  11.5                        25   \n",
       "1061                  8.9                  11.2                        25   \n",
       "1062                  9.8                  11.8                        24   \n",
       "1063                  9.5                  11.5                        25   \n",
       "1064                  9.8                  11.8                        24   \n",
       "1065                  9.3                  11.3                        25   \n",
       "1066                 10.2                  12.8                        22   \n",
       "\n",
       "      CO2EMISSIONS  \n",
       "0              196  \n",
       "1              221  \n",
       "2              136  \n",
       "3              255  \n",
       "4              244  \n",
       "5              230  \n",
       "6              232  \n",
       "7              255  \n",
       "8              267  \n",
       "9              212  \n",
       "10             225  \n",
       "11             239  \n",
       "12             359  \n",
       "13             359  \n",
       "14             338  \n",
       "15             354  \n",
       "16             338  \n",
       "17             354  \n",
       "18             359  \n",
       "19             202  \n",
       "20             230  \n",
       "21             214  \n",
       "22             230  \n",
       "23             230  \n",
       "24             214  \n",
       "25             235  \n",
       "26             251  \n",
       "27             224  \n",
       "28             258  \n",
       "29             224  \n",
       "...            ...  \n",
       "1037           207  \n",
       "1038           207  \n",
       "1039           212  \n",
       "1040           186  \n",
       "1041           186  \n",
       "1042           124  \n",
       "1043           198  \n",
       "1044           198  \n",
       "1045           223  \n",
       "1046           225  \n",
       "1047           248  \n",
       "1048           192  \n",
       "1049           184  \n",
       "1050           278  \n",
       "1051           246  \n",
       "1052           267  \n",
       "1053           246  \n",
       "1054           281  \n",
       "1055           281  \n",
       "1056           223  \n",
       "1057           232  \n",
       "1058           264  \n",
       "1059           235  \n",
       "1060           264  \n",
       "1061           258  \n",
       "1062           271  \n",
       "1063           264  \n",
       "1064           271  \n",
       "1065           260  \n",
       "1066           294  \n",
       "\n",
       "[1067 rows x 13 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/FuelConsumption.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dataset vemos que existem os seguintes dados:\n",
    "- **MODELYEAR** e.g. 2014\n",
    "- **MAKE** e.g. Acura\n",
    "- **MODEL** e.g. ILX\n",
    "- **VEHICLE CLASS** e.g. SUV\n",
    "- **ENGINE SIZE** e.g. 4.7\n",
    "- **CYLINDERS** e.g 6\n",
    "- **TRANSMISSION** e.g. A6\n",
    "- **FUEL CONSUMPTION in CITY(L/100 km)** e.g. 9.9\n",
    "- **FUEL CONSUMPTION in HWY (L/100 km)** e.g. 8.9\n",
    "- **FUEL CONSUMPTION COMB (L/100 km)** e.g. 9.2\n",
    "- **CO2 EMISSIONS (g/km)** e.g. 182   --> low --> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para predizer o consumo à partir do tamanho do carro, utilizamos a seguinte sintaxe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.asanyarray(df[['ENGINESIZE']])\n",
    "train_y = np.asanyarray(df[['CO2EMISSIONS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável independente, no caso, é ```CO2EMISSIONS``` e a variável independente é ```ENGINESIZE```. Com essas duas colunas do dataframe, podemos criar um modelo de regressão linear e obter algumas estimativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, definimos ```a``` e ```b``` (da fórmula da regressão linear) como valores arbitrários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(20.0)\n",
    "b = tf.Variable(30.2)\n",
    "y = a * train_x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Para conseguir fazer estimativas, devemos minimizar o erro quadrado (pra mais e pra menos), assim os dados estimados se aproximam mais dos dados reais que temos à disposição. Definimos essa como a função a ser minimizada. Para isso, usa-se a função ```tf.reduce_mean()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização (gradient descent)\n",
    "Para otimizar esse problema, vamos usar o método do [gradiente descendente](https://en.wikipedia.org/wiki/Gradient_descent). Ele executa derivadas da função buscada para obter mínimos. Por vezes, podemos encontrar mínimos locais, o que não é o valor ótimo, se esse mínimo local não for um mínimo global. O parâmetro passado indica a taxa de aprendizagem e indica a velocidade com que cada ponto da derivada é testado. Quanto menor o valor, mais demorado será o processo, porém a chance de encontrar um mínimo é maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para minimizar a função 'loss', utilizamos a função ```minimize()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devemos então definir as variáveis antes de executar o grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio a sessão e a cada 5 passos, printo o valor da função 'loss', de ```a``` e de ```b```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26992.594 31.414211 33.382053\n",
      "5 2922.954 54.816692 40.65341\n",
      "10 2007.6387 59.065193 42.998283\n",
      "15 1946.111 59.64082 44.38414\n",
      "20 1916.8186 59.51489 45.572674\n",
      "25 1889.4736 59.257656 46.710266\n",
      "30 1862.939 58.978527 47.825195\n",
      "35 1837.153 58.698444 48.92307\n",
      "40 1812.0923 58.42139 50.005154\n",
      "45 1787.7363 58.148083 51.07186\n",
      "50 1764.0656 57.87861 52.12345\n",
      "55 1741.061 57.612946 53.16014\n",
      "60 1718.7034 57.351044 54.182144\n",
      "65 1696.975 57.092854 55.189667\n",
      "70 1675.8575 56.838318 56.18292\n",
      "75 1655.3341 56.587395 57.162106\n",
      "80 1635.3883 56.340023 58.127415\n",
      "85 1616.0035 56.096153 59.079052\n",
      "90 1597.164 55.85574 60.017212\n",
      "95 1578.8541 55.618732 60.942078\n",
      "100 1561.0602 55.385082 61.85384\n",
      "105 1543.7661 55.15474 62.75269\n",
      "110 1526.959 54.927666 63.6388\n",
      "115 1510.6245 54.703804 64.51236\n",
      "120 1494.7495 54.483116 65.37355\n",
      "125 1479.321 54.265553 66.222534\n",
      "130 1464.3267 54.05107 67.059494\n",
      "135 1449.7544 53.83963 67.88459\n",
      "140 1435.5918 53.631184 68.698006\n",
      "145 1421.8275 53.42569 69.49989\n",
      "150 1408.4509 53.223106 70.29042\n",
      "155 1395.4497 53.02339 71.069756\n",
      "160 1382.8152 52.826508 71.83805\n",
      "165 1370.5356 52.632416 72.59547\n",
      "170 1358.6011 52.441067 73.34215\n",
      "175 1347.0029 52.252434 74.07825\n",
      "180 1335.7308 52.06647 74.803925\n",
      "185 1324.7761 51.88314 75.51932\n",
      "190 1314.1292 51.702408 76.22459\n",
      "195 1303.7817 51.524235 76.91985\n",
      "200 1293.7253 51.348587 77.60528\n",
      "205 1283.9524 51.175426 78.28099\n",
      "210 1274.454 51.004723 78.94713\n",
      "215 1265.2228 50.836437 79.60383\n",
      "220 1256.2515 50.670532 80.25123\n",
      "225 1247.5327 50.50698 80.88945\n",
      "230 1239.0586 50.345745 81.51865\n",
      "235 1230.823 50.18679 82.138916\n",
      "240 1222.8195 50.03009 82.7504\n",
      "245 1215.0409 49.875614 83.353226\n",
      "250 1207.4812 49.723316 83.94751\n",
      "255 1200.134 49.57318 84.53338\n",
      "260 1192.9937 49.42517 85.11094\n",
      "265 1186.0541 49.279263 85.68032\n",
      "270 1179.3098 49.135418 86.241646\n",
      "275 1172.7551 48.993614 86.795006\n",
      "280 1166.3851 48.853813 87.34053\n",
      "285 1160.1943 48.715996 87.878334\n",
      "290 1154.177 48.58013 88.408516\n",
      "295 1148.33 48.446194 88.931175\n",
      "300 1142.6469 48.31415 89.44644\n",
      "305 1137.1235 48.18398 89.95441\n",
      "310 1131.756 48.05565 90.45518\n",
      "315 1126.5392 47.92914 90.948845\n",
      "320 1121.469 47.804424 91.435524\n",
      "325 1116.542 47.681473 91.915306\n",
      "330 1111.7528 47.56026 92.388306\n",
      "335 1107.0992 47.44077 92.85459\n",
      "340 1102.5759 47.32297 93.31428\n",
      "345 1098.1798 47.20684 93.76746\n",
      "350 1093.9075 47.092354 94.21422\n",
      "355 1089.7556 46.97949 94.65463\n",
      "360 1085.7203 46.868225 95.08881\n",
      "365 1081.7985 46.758533 95.51686\n",
      "370 1077.9869 46.6504 95.938835\n",
      "375 1074.283 46.543797 96.35483\n",
      "380 1070.6827 46.4387 96.76492\n",
      "385 1067.1841 46.3351 97.16921\n",
      "390 1063.7839 46.232964 97.56778\n",
      "395 1060.4794 46.132275 97.96069\n",
      "400 1057.2676 46.03301 98.34804\n",
      "405 1054.1464 45.935154 98.729904\n",
      "410 1051.1129 45.838684 99.10635\n",
      "415 1048.1649 45.74358 99.47747\n",
      "420 1045.2997 45.649822 99.84334\n",
      "425 1042.515 45.557392 100.20402\n",
      "430 1039.8087 45.466274 100.559586\n",
      "435 1037.1786 45.376446 100.910126\n",
      "440 1034.622 45.287888 101.2557\n",
      "445 1032.1382 45.20059 101.596375\n",
      "450 1029.7238 45.11452 101.93222\n",
      "455 1027.3771 45.029675 102.26331\n",
      "460 1025.0967 44.946033 102.589714\n",
      "465 1022.8805 44.86357 102.91148\n",
      "470 1020.72644 44.78228 103.2287\n",
      "475 1018.6332 44.702145 103.54142\n",
      "480 1016.5989 44.62314 103.849724\n",
      "485 1014.62134 44.545254 104.15365\n",
      "490 1012.69995 44.46847 104.45327\n",
      "495 1010.83234 44.39278 104.74864\n",
      "500 1009.0176 44.31816 105.03983\n",
      "505 1007.2533 44.244595 105.3269\n",
      "510 1005.5391 44.172073 105.60989\n",
      "515 1003.873 44.10058 105.888885\n",
      "520 1002.25397 44.030098 106.163925\n",
      "525 1000.68005 43.960613 106.43507\n",
      "530 999.15063 43.892113 106.70236\n",
      "535 997.66437 43.82459 106.96588\n",
      "540 996.2199 43.758015 107.22566\n",
      "545 994.8156 43.692383 107.481766\n",
      "550 993.4515 43.627686 107.73424\n",
      "555 992.12537 43.563904 107.98314\n",
      "560 990.83655 43.501022 108.228516\n",
      "565 989.584 43.439034 108.470406\n",
      "570 988.367 43.377922 108.70887\n",
      "575 987.18384 43.31768 108.94396\n",
      "580 986.0342 43.258286 109.17572\n",
      "585 984.9167 43.199738 109.40419\n",
      "590 983.83093 43.142017 109.62943\n",
      "595 982.7755 43.085117 109.85148\n",
      "600 981.74976 43.029022 110.07038\n",
      "605 980.75287 42.97372 110.28618\n",
      "610 979.7841 42.919205 110.498924\n",
      "615 978.8425 42.865456 110.70866\n",
      "620 977.92725 42.81247 110.91542\n",
      "625 977.0382 42.760235 111.119255\n",
      "630 976.1738 42.70874 111.32021\n",
      "635 975.33356 42.657974 111.51831\n",
      "640 974.5172 42.60793 111.7136\n",
      "645 973.724 42.55859 111.90613\n",
      "650 972.95276 42.509953 112.095924\n",
      "655 972.20337 42.462 112.283035\n",
      "660 971.4751 42.414738 112.46749\n",
      "665 970.7674 42.368137 112.64934\n",
      "670 970.07935 42.322193 112.828606\n",
      "675 969.41077 42.276905 113.00533\n",
      "680 968.7611 42.232258 113.17955\n",
      "685 968.12946 42.188244 113.35131\n",
      "690 967.5159 42.14485 113.52063\n",
      "695 966.9194 42.102077 113.68755\n",
      "700 966.3399 42.059906 113.85212\n",
      "705 965.7763 42.018333 114.01436\n",
      "710 965.22906 41.97735 114.174286\n",
      "715 964.69696 41.936947 114.331955\n",
      "720 964.1797 41.897114 114.48739\n",
      "725 963.67725 41.85784 114.64062\n",
      "730 963.1884 41.81913 114.79168\n",
      "735 962.7139 41.78097 114.940605\n",
      "740 962.25275 41.743347 115.087425\n",
      "745 961.80414 41.706257 115.232155\n",
      "750 961.36835 41.669693 115.37484\n",
      "755 960.9449 41.633648 115.515495\n",
      "760 960.5333 41.598114 115.65416\n",
      "765 960.13336 41.56308 115.79086\n",
      "770 959.7446 41.528545 115.92564\n",
      "775 959.36676 41.4945 116.0585\n",
      "780 958.9993 41.46093 116.189476\n",
      "785 958.6427 41.42784 116.318596\n",
      "790 958.2958 41.39522 116.445885\n",
      "795 957.9587 41.363064 116.57137\n",
      "800 957.63104 41.33136 116.69508\n",
      "805 957.3128 41.30011 116.81704\n",
      "810 957.0033 41.2693 116.93728\n",
      "815 956.70276 41.238922 117.0558\n",
      "820 956.4104 41.20898 117.17265\n",
      "825 956.12634 41.17946 117.28784\n",
      "830 955.85034 41.15036 117.4014\n",
      "835 955.5821 41.12167 117.51335\n",
      "840 955.3214 41.09339 117.62372\n",
      "845 955.0678 41.06551 117.73251\n",
      "850 954.8217 41.038025 117.83978\n",
      "855 954.58234 41.01092 117.945526\n",
      "860 954.34973 40.984207 118.04977\n",
      "865 954.12366 40.95787 118.152534\n",
      "870 953.90405 40.93191 118.253845\n",
      "875 953.6906 40.906315 118.35372\n",
      "880 953.483 40.881084 118.45218\n",
      "885 953.28125 40.85621 118.54925\n",
      "890 953.0853 40.831688 118.644936\n",
      "895 952.8947 40.807514 118.73927\n",
      "900 952.70996 40.783684 118.832275\n",
      "905 952.5298 40.76019 118.92395\n",
      "910 952.35486 40.73703 119.014336\n",
      "915 952.185 40.714195 119.10343\n",
      "920 952.01984 40.691685 119.19128\n",
      "925 951.8591 40.66949 119.27788\n",
      "930 951.7033 40.647614 119.36324\n",
      "935 951.5518 40.62605 119.4474\n",
      "940 951.4044 40.60479 119.53037\n",
      "945 951.261 40.583828 119.61216\n",
      "950 951.122 40.563164 119.6928\n",
      "955 950.9866 40.542793 119.7723\n",
      "960 950.8551 40.52271 119.85066\n",
      "965 950.72736 40.50291 119.92792\n",
      "970 950.6032 40.483395 120.004074\n",
      "975 950.4826 40.464153 120.079155\n",
      "980 950.36554 40.445187 120.15317\n",
      "985 950.2516 40.426487 120.226135\n",
      "990 950.14075 40.40805 120.29807\n",
      "995 950.033 40.38988 120.36899\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "train_data = []\n",
    "for step in range(1000):\n",
    "    _, loss_val, a_val, b_val = sess.run([train, loss, a, b])\n",
    "    loss_values.append(loss_val)\n",
    "    if step % 5 == 0:\n",
    "        print(step, loss_val, a_val, b_val)\n",
    "        train_data.append([a_val, b_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3c485d64a8>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos aqui que o valor da função 'loss' ainda decrescia bastante, até estabilizar em torno da iteração de número 500, chegando próximo de um possível mínimo global (ou local, não dá pra saber)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cr, cg, cb = (1.0, 1.0, 0.0)\n",
    "for f in train_data:\n",
    "    cb += 1.0 / len(train_data)\n",
    "    cg -= 1.0 / len(train_data)\n",
    "    if cb > 1.0: cb = 1.0\n",
    "    if cg < 0.0: cg = 0.0\n",
    "    [a, b] = f\n",
    "    f_y = np.vectorize(lambda x: a*x + b)(train_x)\n",
    "    line = plt.plot(train_x, f_y)\n",
    "    plt.setp(line, color=(cr,cg,cb))\n",
    "\n",
    "plt.plot(train_x, train_y, 'ro')\n",
    "\n",
    "\n",
    "green_line = mpatches.Patch(color='red', label='Data Points')\n",
    "\n",
    "plt.legend(handles=[green_line])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística\n",
    "Enquanto usamos a regressão linear para estimar valores contínuos, a regressão logística tem um uso mais direcionado para classificar pontos em um dataset, obtendo a classe à qual esse ponto tem mais chance de pertencer.\n",
    "\n",
    "Da regressão linear:\n",
    "$$\n",
    "y = w0 + w1 \\times x1 + w2 \\times x2 + \\cdots\n",
    "$$\n",
    "e:\n",
    "$$\n",
    "Y = W X + b\n",
    "$$\n",
    "\n",
    "Chegamos a:\n",
    "$$\n",
    "P(ŷ = Y) = \\theta(y) = \\frac{e^y}{1 + e^y} = \\frac{1}{1+e^{-y}} = p \n",
    "$$\n",
    "\n",
    "Assim obtendo um p-valor entre 0 e 1 para cada ponto.\n",
    "\n",
    "A equação acima produz uma [curva em forma de \"s\"](http://en.wikipedia.org/wiki/Logistic_function), que divide o espaço em duas classes.\n",
    "![title](img/logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística com Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse caso, vamos usar o clássico Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_X, iris_y = iris.data[:-1,:], iris.target[:-1]\n",
    "iris_y= pd.get_dummies(iris_y).values\n",
    "trainX, testX, trainY, testY = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset foi criado pelo biólogo e estatístico Ronald Fisher e consiste de 50 exemplos de cada espécie de Iris (Iris setosa, Iris virginica e Iris versicolor). Cada flor contém 5 atributos: comprimento e largura da pétala, comprimento e largura da sépala e sua espécie(variável de interesse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "Para que o nosso algoritmo aceite dados sem saber o tamanho do vetor ou a quantidade de dados que alimentamos o nosso grafo. Para isso, informamos o número de features e de labels de interesse que temos. Assim, o Tensor pode receber dados em grupos de variados tamanhos, já que os placeholders consistem de vetores vazios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numFeatures is the number of features in our input data.\n",
    "# In the iris dataset, this number is '4'.\n",
    "numFeatures = trainX.shape[1]\n",
    "\n",
    "# numLabels is the number of classes our data points can be in.\n",
    "# In the iris dataset, this number is '3'.\n",
    "numLabels = trainY.shape[1]\n",
    "\n",
    "\n",
    "# Placeholders\n",
    "# 'None' means TensorFlow shouldn't expect a fixed number in that dimension\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures]) # Iris has 4 features, so X is a tensor to hold our data.\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels]) # This will be our correct answers matrix for 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesos e viés\n",
    "Assim como na regressão linear, devemos definir alguns valores iniciais, para poder então encontrar os valores ótimos para chegar na melhor estimativa.\n",
    "\n",
    "Vamos inicializar ```W``` e ```b``` como tensores nulos, para então aprendê-los com os dados presentes no dataset. Ao final do treinamento podemos salvar esses valores e utilizá-los depois para fazer novas estimativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([4, 3]))  # 4-dimensional input and  3 classes\n",
    "b = tf.Variable(tf.zeros([3])) # 3-dimensional output [0,0,1],[0,1,0],[1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos o Tensor ```W``` como um vetor de 3 dimensões, que conta com um input de 4 dimensões e 3 classes possíveis. Já ```b``` conta com 2 dimensões, indicando a saída de uma das 3 classes. Os pesos devem ter um valor inicial, no caso, escolhemos Tensores com seus valores zerados. Na próxima célula, valores aleatórios da distribuição normal são definidos para cada Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly sample from a normal distribution with standard deviation .01\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=0.01,\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=0.01,\n",
    "                                    name=\"bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "A regressão logística é definida pela seguinte equação:\n",
    "\n",
    "$$\n",
    "ŷ =sigmoid(WX+b)\n",
    "$$\n",
    "\n",
    "No Tensorflow, ela pode ser definida como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-component breakdown of the Logistic Regression equation.\n",
    "# Note that these feed into each other.\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função aqui usada será a logística $(\\frac{1}{1+e^{-Wx}})$, que recebe os dados do dataset, após a aplicação dos pesos e do viés. Essa função é aplicada por meio do método ```nn.sigmoid```. Essa função insere os pesos junto ao viés à entrada e aplica a uma curva de 0 a 100 por cento, chegando à função de probabilidade de nosso interesse.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "Queremos encontrar o modelo que apresente os melhores pesos (**w**).\n",
    "\n",
    "Para isso, devemos minimizar o erro/custo da função. No caso, será utilizado a função de erro '[Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error)' (erro quadrático médio).\n",
    "\n",
    "Para minimizar a função de custo, vamos aplicar novamente o [gradiente descendente](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "Para esse treinamento, define-se a taxa de aprendizagem e o número de iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs in our training\n",
    "numEpochs = 100000\n",
    "\n",
    "# Defining our learning rate iterations (decay)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our cost function - Squared Mean Error\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "#Defining our Gradient Descent\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar o processamento, devemos iniciar uma Sessão e executar as operações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize our weights and biases variables.\n",
    "init_OP = tf.global_variables_initializer()\n",
    "\n",
    "# Initialize all tensorflow variables\n",
    "sess.run(init_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os seguintes comandos têm como função analisar a eficiencia do modelo ao longo do treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax(activation_OP, 1) returns the label with the most probability\n",
    "# argmax(yGold, 1) is the correct label\n",
    "correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(yGold,1))\n",
    "\n",
    "# If every false prediction is 0 and every true prediction is 1, the average returns us the accuracy\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "\n",
    "# Summary op for regression output\n",
    "activation_summary_OP = tf.summary.histogram(\"output\", activation_OP)\n",
    "\n",
    "# Summary op for accuracy\n",
    "accuracy_summary_OP = tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "\n",
    "# Summary op for cost\n",
    "cost_summary_OP = tf.summary.scalar(\"cost\", cost_OP)\n",
    "\n",
    "# Summary ops to check how variables (W, b) are updating after each iteration\n",
    "weightSummary = tf.summary.histogram(\"weights\", weights.eval(session=sess))\n",
    "biasSummary = tf.summary.histogram(\"biases\", bias.eval(session=sess))\n",
    "\n",
    "# Merge all summaries\n",
    "merged = tf.summary.merge([activation_summary_OP, accuracy_summary_OP, cost_summary_OP, weightSummary, biasSummary])\n",
    "\n",
    "# Summary writer\n",
    "writer = tf.summary.FileWriter(\"summary_logs\", sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente posso iniciar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.353535, cost 34.6055, change in cost 34.6055\n",
      "step 10, training accuracy 0.606061, cost 30.0186, change in cost 4.58697\n",
      "step 20, training accuracy 0.646465, cost 28.0836, change in cost 1.93498\n",
      "step 30, training accuracy 0.646465, cost 26.4563, change in cost 1.62728\n",
      "step 40, training accuracy 0.646465, cost 25.1063, change in cost 1.35002\n",
      "step 50, training accuracy 0.646465, cost 23.9912, change in cost 1.11505\n",
      "step 60, training accuracy 0.646465, cost 23.0678, change in cost 0.923435\n",
      "step 70, training accuracy 0.646465, cost 22.2978, change in cost 0.769979\n",
      "step 80, training accuracy 0.646465, cost 21.65, change in cost 0.647812\n",
      "step 90, training accuracy 0.646465, cost 21.0996, change in cost 0.55044\n",
      "step 100, training accuracy 0.656566, cost 20.6271, change in cost 0.472435\n",
      "step 110, training accuracy 0.666667, cost 20.2176, change in cost 0.409479\n",
      "step 120, training accuracy 0.666667, cost 19.8594, change in cost 0.35825\n",
      "step 130, training accuracy 0.666667, cost 19.5432, change in cost 0.316198\n",
      "step 140, training accuracy 0.666667, cost 19.2618, change in cost 0.281382\n",
      "step 150, training accuracy 0.666667, cost 19.0095, change in cost 0.252298\n",
      "step 160, training accuracy 0.676768, cost 18.7817, change in cost 0.227818\n",
      "step 170, training accuracy 0.686869, cost 18.5747, change in cost 0.207045\n",
      "step 180, training accuracy 0.69697, cost 18.3854, change in cost 0.189289\n",
      "step 190, training accuracy 0.707071, cost 18.2114, change in cost 0.174002\n",
      "step 200, training accuracy 0.717172, cost 18.0506, change in cost 0.160759\n",
      "step 210, training accuracy 0.737374, cost 17.9014, change in cost 0.149216\n",
      "step 220, training accuracy 0.737374, cost 17.7623, change in cost 0.13909\n",
      "step 230, training accuracy 0.747475, cost 17.6321, change in cost 0.130163\n",
      "step 240, training accuracy 0.757576, cost 17.5099, change in cost 0.12225\n",
      "step 250, training accuracy 0.777778, cost 17.3947, change in cost 0.115202\n",
      "step 260, training accuracy 0.787879, cost 17.2858, change in cost 0.108894\n",
      "step 270, training accuracy 0.787879, cost 17.1826, change in cost 0.103226\n",
      "step 280, training accuracy 0.787879, cost 17.0845, change in cost 0.0981121\n",
      "step 290, training accuracy 0.787879, cost 16.991, change in cost 0.0934715\n",
      "step 300, training accuracy 0.787879, cost 16.9017, change in cost 0.0892582\n",
      "step 310, training accuracy 0.79798, cost 16.8163, change in cost 0.0854092\n",
      "step 320, training accuracy 0.79798, cost 16.7344, change in cost 0.0818806\n",
      "step 330, training accuracy 0.79798, cost 16.6558, change in cost 0.0786457\n",
      "step 340, training accuracy 0.808081, cost 16.5801, change in cost 0.0756607\n",
      "step 350, training accuracy 0.818182, cost 16.5072, change in cost 0.0728989\n",
      "step 360, training accuracy 0.838384, cost 16.4369, change in cost 0.070343\n",
      "step 370, training accuracy 0.838384, cost 16.3689, change in cost 0.0679684\n",
      "step 380, training accuracy 0.838384, cost 16.3032, change in cost 0.0657539\n",
      "step 390, training accuracy 0.838384, cost 16.2395, change in cost 0.0636845\n",
      "step 400, training accuracy 0.848485, cost 16.1777, change in cost 0.0617485\n",
      "step 410, training accuracy 0.848485, cost 16.1178, change in cost 0.0599346\n",
      "step 420, training accuracy 0.848485, cost 16.0596, change in cost 0.0582314\n",
      "step 430, training accuracy 0.858586, cost 16.0029, change in cost 0.0566216\n",
      "step 440, training accuracy 0.858586, cost 15.9478, change in cost 0.0551043\n",
      "step 450, training accuracy 0.868687, cost 15.8942, change in cost 0.0536737\n",
      "step 460, training accuracy 0.868687, cost 15.8419, change in cost 0.0523157\n",
      "step 470, training accuracy 0.878788, cost 15.7908, change in cost 0.0510273\n",
      "step 480, training accuracy 0.878788, cost 15.741, change in cost 0.0498047\n",
      "step 490, training accuracy 0.878788, cost 15.6924, change in cost 0.0486422\n",
      "step 500, training accuracy 0.878788, cost 15.6448, change in cost 0.047534\n",
      "step 510, training accuracy 0.878788, cost 15.5984, change in cost 0.0464754\n",
      "step 520, training accuracy 0.878788, cost 15.5529, change in cost 0.0454664\n",
      "step 530, training accuracy 0.888889, cost 15.5084, change in cost 0.0445004\n",
      "step 540, training accuracy 0.888889, cost 15.4648, change in cost 0.0435743\n",
      "step 550, training accuracy 0.89899, cost 15.4221, change in cost 0.0426893\n",
      "step 560, training accuracy 0.89899, cost 15.3803, change in cost 0.0418386\n",
      "step 570, training accuracy 0.89899, cost 15.3393, change in cost 0.0410185\n",
      "step 580, training accuracy 0.89899, cost 15.299, change in cost 0.0402327\n",
      "step 590, training accuracy 0.89899, cost 15.2596, change in cost 0.0394773\n",
      "step 600, training accuracy 0.909091, cost 15.2208, change in cost 0.0387478\n",
      "step 610, training accuracy 0.909091, cost 15.1828, change in cost 0.0380459\n",
      "step 620, training accuracy 0.909091, cost 15.1454, change in cost 0.0373659\n",
      "step 630, training accuracy 0.909091, cost 15.1087, change in cost 0.0367098\n",
      "step 640, training accuracy 0.909091, cost 15.0726, change in cost 0.0360794\n",
      "step 650, training accuracy 0.909091, cost 15.0372, change in cost 0.0354643\n",
      "step 660, training accuracy 0.909091, cost 15.0023, change in cost 0.034873\n",
      "step 670, training accuracy 0.909091, cost 14.968, change in cost 0.034297\n",
      "step 680, training accuracy 0.909091, cost 14.9342, change in cost 0.033742\n",
      "step 690, training accuracy 0.909091, cost 14.901, change in cost 0.0332031\n",
      "step 700, training accuracy 0.909091, cost 14.8684, change in cost 0.0326796\n",
      "step 710, training accuracy 0.909091, cost 14.8362, change in cost 0.0321674\n",
      "step 720, training accuracy 0.909091, cost 14.8045, change in cost 0.0316763\n",
      "step 730, training accuracy 0.909091, cost 14.7733, change in cost 0.0311947\n",
      "step 740, training accuracy 0.909091, cost 14.7426, change in cost 0.0307302\n",
      "step 750, training accuracy 0.909091, cost 14.7123, change in cost 0.0302753\n",
      "step 760, training accuracy 0.909091, cost 14.6825, change in cost 0.0298319\n",
      "step 770, training accuracy 0.909091, cost 14.6531, change in cost 0.0294027\n",
      "step 780, training accuracy 0.909091, cost 14.6241, change in cost 0.0289831\n",
      "step 790, training accuracy 0.909091, cost 14.5955, change in cost 0.028574\n",
      "step 800, training accuracy 0.909091, cost 14.5674, change in cost 0.0281763\n",
      "step 810, training accuracy 0.909091, cost 14.5396, change in cost 0.0277863\n",
      "step 820, training accuracy 0.909091, cost 14.5122, change in cost 0.0274086\n",
      "step 830, training accuracy 0.909091, cost 14.4851, change in cost 0.0270376\n",
      "step 840, training accuracy 0.909091, cost 14.4584, change in cost 0.0266771\n",
      "step 850, training accuracy 0.919192, cost 14.4321, change in cost 0.0263214\n",
      "step 860, training accuracy 0.919192, cost 14.4061, change in cost 0.02598\n",
      "step 870, training accuracy 0.919192, cost 14.3805, change in cost 0.0256414\n",
      "step 880, training accuracy 0.919192, cost 14.3552, change in cost 0.0253134\n",
      "step 890, training accuracy 0.919192, cost 14.3302, change in cost 0.0249882\n",
      "step 900, training accuracy 0.919192, cost 14.3055, change in cost 0.0246735\n",
      "step 910, training accuracy 0.919192, cost 14.2812, change in cost 0.0243664\n",
      "step 920, training accuracy 0.919192, cost 14.2571, change in cost 0.0240641\n",
      "step 930, training accuracy 0.929293, cost 14.2333, change in cost 0.0237684\n",
      "step 940, training accuracy 0.929293, cost 14.2098, change in cost 0.0234785\n",
      "step 950, training accuracy 0.929293, cost 14.1867, change in cost 0.0231953\n",
      "step 960, training accuracy 0.929293, cost 14.1637, change in cost 0.0229177\n",
      "step 970, training accuracy 0.929293, cost 14.1411, change in cost 0.022645\n",
      "step 980, training accuracy 0.939394, cost 14.1187, change in cost 0.0223808\n",
      "step 990, training accuracy 0.939394, cost 14.0966, change in cost 0.0221157\n",
      "step 1000, training accuracy 0.939394, cost 14.0747, change in cost 0.021862\n",
      "step 1010, training accuracy 0.939394, cost 14.0531, change in cost 0.0216103\n",
      "step 1020, training accuracy 0.939394, cost 14.0318, change in cost 0.0213614\n",
      "step 1030, training accuracy 0.939394, cost 14.0106, change in cost 0.021122\n",
      "step 1040, training accuracy 0.939394, cost 13.9898, change in cost 0.0208836\n",
      "step 1050, training accuracy 0.939394, cost 13.9691, change in cost 0.0206499\n",
      "step 1060, training accuracy 0.939394, cost 13.9487, change in cost 0.020422\n",
      "step 1070, training accuracy 0.939394, cost 13.9285, change in cost 0.0201969\n",
      "step 1080, training accuracy 0.939394, cost 13.9085, change in cost 0.0199757\n",
      "step 1090, training accuracy 0.939394, cost 13.8887, change in cost 0.0197601\n",
      "step 1100, training accuracy 0.939394, cost 13.8692, change in cost 0.0195475\n",
      "step 1110, training accuracy 0.939394, cost 13.8499, change in cost 0.0193386\n",
      "step 1120, training accuracy 0.939394, cost 13.8307, change in cost 0.0191307\n",
      "step 1130, training accuracy 0.939394, cost 13.8118, change in cost 0.0189314\n",
      "step 1140, training accuracy 0.939394, cost 13.7931, change in cost 0.018733\n",
      "step 1150, training accuracy 0.939394, cost 13.7745, change in cost 0.0185366\n",
      "step 1160, training accuracy 0.939394, cost 13.7562, change in cost 0.0183458\n",
      "step 1170, training accuracy 0.939394, cost 13.738, change in cost 0.0181561\n",
      "step 1180, training accuracy 0.939394, cost 13.7201, change in cost 0.017971\n",
      "step 1190, training accuracy 0.939394, cost 13.7023, change in cost 0.0177889\n",
      "step 1200, training accuracy 0.939394, cost 13.6847, change in cost 0.0176096\n",
      "step 1210, training accuracy 0.939394, cost 13.6672, change in cost 0.0174332\n",
      "step 1220, training accuracy 0.939394, cost 13.65, change in cost 0.0172586\n",
      "step 1230, training accuracy 0.939394, cost 13.6329, change in cost 0.0170889\n",
      "step 1240, training accuracy 0.939394, cost 13.616, change in cost 0.0169191\n",
      "step 1250, training accuracy 0.939394, cost 13.5992, change in cost 0.0167561\n",
      "step 1260, training accuracy 0.939394, cost 13.5826, change in cost 0.016592\n",
      "step 1270, training accuracy 0.939394, cost 13.5662, change in cost 0.0164318\n",
      "step 1280, training accuracy 0.939394, cost 13.5499, change in cost 0.0162725\n",
      "step 1290, training accuracy 0.939394, cost 13.5338, change in cost 0.016118\n",
      "step 1300, training accuracy 0.939394, cost 13.5178, change in cost 0.0159645\n",
      "step 1310, training accuracy 0.939394, cost 13.502, change in cost 0.0158167\n",
      "step 1320, training accuracy 0.939394, cost 13.4863, change in cost 0.0156631\n",
      "step 1330, training accuracy 0.939394, cost 13.4708, change in cost 0.0155182\n",
      "step 1340, training accuracy 0.939394, cost 13.4554, change in cost 0.015377\n",
      "step 1350, training accuracy 0.939394, cost 13.4402, change in cost 0.0152321\n",
      "step 1360, training accuracy 0.939394, cost 13.4251, change in cost 0.0150919\n",
      "step 1370, training accuracy 0.939394, cost 13.4102, change in cost 0.0149546\n",
      "step 1380, training accuracy 0.939394, cost 13.3953, change in cost 0.0148201\n",
      "step 1390, training accuracy 0.939394, cost 13.3807, change in cost 0.0146837\n",
      "step 1400, training accuracy 0.939394, cost 13.3661, change in cost 0.0145531\n",
      "step 1410, training accuracy 0.939394, cost 13.3517, change in cost 0.0144224\n",
      "step 1420, training accuracy 0.939394, cost 13.3374, change in cost 0.0142946\n",
      "step 1430, training accuracy 0.939394, cost 13.3232, change in cost 0.0141668\n",
      "step 1440, training accuracy 0.939394, cost 13.3092, change in cost 0.0140429\n",
      "step 1450, training accuracy 0.939394, cost 13.2953, change in cost 0.0139198\n",
      "step 1460, training accuracy 0.939394, cost 13.2815, change in cost 0.0137987\n",
      "step 1470, training accuracy 0.939394, cost 13.2678, change in cost 0.0136776\n",
      "step 1480, training accuracy 0.939394, cost 13.2542, change in cost 0.0135612\n",
      "step 1490, training accuracy 0.939394, cost 13.2408, change in cost 0.0134439\n",
      "step 1500, training accuracy 0.939394, cost 13.2275, change in cost 0.0133286\n",
      "step 1510, training accuracy 0.939394, cost 13.2142, change in cost 0.013216\n",
      "step 1520, training accuracy 0.939394, cost 13.2011, change in cost 0.0131044\n",
      "step 1530, training accuracy 0.939394, cost 13.1881, change in cost 0.012991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1540, training accuracy 0.939394, cost 13.1753, change in cost 0.012886\n",
      "step 1550, training accuracy 0.939394, cost 13.1625, change in cost 0.0127764\n",
      "step 1560, training accuracy 0.939394, cost 13.1498, change in cost 0.0126705\n",
      "step 1570, training accuracy 0.939394, cost 13.1372, change in cost 0.0125656\n",
      "step 1580, training accuracy 0.939394, cost 13.1248, change in cost 0.0124636\n",
      "step 1590, training accuracy 0.939394, cost 13.1124, change in cost 0.0123606\n",
      "step 1600, training accuracy 0.939394, cost 13.1002, change in cost 0.0122595\n",
      "step 1610, training accuracy 0.949495, cost 13.088, change in cost 0.0121603\n",
      "step 1620, training accuracy 0.949495, cost 13.0759, change in cost 0.012063\n",
      "step 1630, training accuracy 0.949495, cost 13.064, change in cost 0.0119658\n",
      "step 1640, training accuracy 0.949495, cost 13.0521, change in cost 0.0118694\n",
      "step 1650, training accuracy 0.949495, cost 13.0403, change in cost 0.0117731\n",
      "step 1660, training accuracy 0.949495, cost 13.0286, change in cost 0.0116825\n",
      "step 1670, training accuracy 0.949495, cost 13.0171, change in cost 0.011591\n",
      "step 1680, training accuracy 0.949495, cost 13.0056, change in cost 0.0114985\n",
      "step 1690, training accuracy 0.949495, cost 12.9942, change in cost 0.0114069\n",
      "step 1700, training accuracy 0.949495, cost 12.9828, change in cost 0.0113201\n",
      "step 1710, training accuracy 0.949495, cost 12.9716, change in cost 0.0112324\n",
      "step 1720, training accuracy 0.949495, cost 12.9605, change in cost 0.0111446\n",
      "step 1730, training accuracy 0.949495, cost 12.9494, change in cost 0.0110588\n",
      "step 1740, training accuracy 0.949495, cost 12.9384, change in cost 0.0109739\n",
      "step 1750, training accuracy 0.949495, cost 12.9275, change in cost 0.010891\n",
      "step 1760, training accuracy 0.949495, cost 12.9167, change in cost 0.0108089\n",
      "step 1770, training accuracy 0.949495, cost 12.906, change in cost 0.0107279\n",
      "step 1780, training accuracy 0.949495, cost 12.8953, change in cost 0.0106459\n",
      "step 1790, training accuracy 0.949495, cost 12.8848, change in cost 0.0105648\n",
      "step 1800, training accuracy 0.949495, cost 12.8743, change in cost 0.0104866\n",
      "step 1810, training accuracy 0.949495, cost 12.8639, change in cost 0.0104094\n",
      "step 1820, training accuracy 0.949495, cost 12.8536, change in cost 0.0103331\n",
      "step 1830, training accuracy 0.949495, cost 12.8433, change in cost 0.0102549\n",
      "step 1840, training accuracy 0.949495, cost 12.8331, change in cost 0.0101824\n",
      "step 1850, training accuracy 0.949495, cost 12.823, change in cost 0.0101061\n",
      "step 1860, training accuracy 0.949495, cost 12.813, change in cost 0.0100336\n",
      "step 1870, training accuracy 0.949495, cost 12.803, change in cost 0.00996017\n",
      "step 1880, training accuracy 0.949495, cost 12.7931, change in cost 0.00988865\n",
      "step 1890, training accuracy 0.949495, cost 12.7833, change in cost 0.00981808\n",
      "step 1900, training accuracy 0.949495, cost 12.7736, change in cost 0.0097456\n",
      "step 1910, training accuracy 0.949495, cost 12.7639, change in cost 0.00967789\n",
      "step 1920, training accuracy 0.949495, cost 12.7543, change in cost 0.00960827\n",
      "step 1930, training accuracy 0.949495, cost 12.7447, change in cost 0.0095396\n",
      "step 1940, training accuracy 0.949495, cost 12.7353, change in cost 0.00947571\n",
      "step 1950, training accuracy 0.949495, cost 12.7259, change in cost 0.00940609\n",
      "step 1960, training accuracy 0.949495, cost 12.7165, change in cost 0.00934219\n",
      "step 1970, training accuracy 0.949495, cost 12.7072, change in cost 0.00927448\n",
      "step 1980, training accuracy 0.949495, cost 12.698, change in cost 0.00921249\n",
      "step 1990, training accuracy 0.949495, cost 12.6889, change in cost 0.00914669\n",
      "step 2000, training accuracy 0.949495, cost 12.6798, change in cost 0.00908566\n",
      "step 2010, training accuracy 0.949495, cost 12.6708, change in cost 0.00902271\n",
      "step 2020, training accuracy 0.949495, cost 12.6618, change in cost 0.00896072\n",
      "step 2030, training accuracy 0.949495, cost 12.6529, change in cost 0.00889969\n",
      "step 2040, training accuracy 0.949495, cost 12.6441, change in cost 0.00883961\n",
      "step 2050, training accuracy 0.949495, cost 12.6353, change in cost 0.00878048\n",
      "step 2060, training accuracy 0.949495, cost 12.6266, change in cost 0.0087204\n",
      "step 2070, training accuracy 0.949495, cost 12.6179, change in cost 0.00866222\n",
      "step 2080, training accuracy 0.949495, cost 12.6093, change in cost 0.00860405\n",
      "step 2090, training accuracy 0.949495, cost 12.6008, change in cost 0.00854683\n",
      "step 2100, training accuracy 0.949495, cost 12.5923, change in cost 0.00849247\n",
      "step 2110, training accuracy 0.949495, cost 12.5838, change in cost 0.00843239\n",
      "step 2120, training accuracy 0.949495, cost 12.5755, change in cost 0.00837803\n",
      "step 2130, training accuracy 0.949495, cost 12.5671, change in cost 0.00832462\n",
      "step 2140, training accuracy 0.949495, cost 12.5589, change in cost 0.00827026\n",
      "step 2150, training accuracy 0.949495, cost 12.5506, change in cost 0.00821495\n",
      "step 2160, training accuracy 0.949495, cost 12.5425, change in cost 0.0081625\n",
      "step 2170, training accuracy 0.949495, cost 12.5344, change in cost 0.00811005\n",
      "step 2180, training accuracy 0.949495, cost 12.5263, change in cost 0.00805664\n",
      "step 2190, training accuracy 0.949495, cost 12.5183, change in cost 0.0080061\n",
      "step 2200, training accuracy 0.949495, cost 12.5104, change in cost 0.00795555\n",
      "step 2210, training accuracy 0.949495, cost 12.5025, change in cost 0.00790215\n",
      "step 2220, training accuracy 0.949495, cost 12.4946, change in cost 0.00785351\n",
      "step 2230, training accuracy 0.949495, cost 12.4868, change in cost 0.00780392\n",
      "step 2240, training accuracy 0.949495, cost 12.479, change in cost 0.00775623\n",
      "step 2250, training accuracy 0.949495, cost 12.4713, change in cost 0.00770473\n",
      "step 2260, training accuracy 0.949495, cost 12.4637, change in cost 0.007658\n",
      "step 2270, training accuracy 0.949495, cost 12.4561, change in cost 0.00761127\n",
      "step 2280, training accuracy 0.949495, cost 12.4485, change in cost 0.00756168\n",
      "step 2290, training accuracy 0.949495, cost 12.441, change in cost 0.00751591\n",
      "step 2300, training accuracy 0.949495, cost 12.4335, change in cost 0.00747013\n",
      "step 2310, training accuracy 0.949495, cost 12.4261, change in cost 0.00742435\n",
      "step 2320, training accuracy 0.949495, cost 12.4187, change in cost 0.00737858\n",
      "step 2330, training accuracy 0.949495, cost 12.4114, change in cost 0.0073328\n",
      "step 2340, training accuracy 0.949495, cost 12.4041, change in cost 0.00728893\n",
      "step 2350, training accuracy 0.949495, cost 12.3968, change in cost 0.00724506\n",
      "step 2360, training accuracy 0.949495, cost 12.3896, change in cost 0.00720024\n",
      "step 2370, training accuracy 0.959596, cost 12.3825, change in cost 0.00715733\n",
      "step 2380, training accuracy 0.959596, cost 12.3754, change in cost 0.00711441\n",
      "step 2390, training accuracy 0.959596, cost 12.3683, change in cost 0.00707054\n",
      "step 2400, training accuracy 0.959596, cost 12.3613, change in cost 0.00703049\n",
      "step 2410, training accuracy 0.959596, cost 12.3543, change in cost 0.00698853\n",
      "step 2420, training accuracy 0.959596, cost 12.3473, change in cost 0.00694466\n",
      "step 2430, training accuracy 0.959596, cost 12.3404, change in cost 0.00690746\n",
      "step 2440, training accuracy 0.959596, cost 12.3336, change in cost 0.00686264\n",
      "step 2450, training accuracy 0.959596, cost 12.3267, change in cost 0.00682354\n",
      "step 2460, training accuracy 0.959596, cost 12.32, change in cost 0.00678444\n",
      "step 2470, training accuracy 0.959596, cost 12.3132, change in cost 0.00674629\n",
      "step 2480, training accuracy 0.959596, cost 12.3065, change in cost 0.00670433\n",
      "step 2490, training accuracy 0.959596, cost 12.2998, change in cost 0.00666904\n",
      "step 2500, training accuracy 0.959596, cost 12.2932, change in cost 0.00662613\n",
      "step 2510, training accuracy 0.959596, cost 12.2866, change in cost 0.00659084\n",
      "step 2520, training accuracy 0.959596, cost 12.2801, change in cost 0.0065527\n",
      "step 2530, training accuracy 0.959596, cost 12.2736, change in cost 0.00651646\n",
      "step 2540, training accuracy 0.959596, cost 12.2671, change in cost 0.00647831\n",
      "step 2550, training accuracy 0.959596, cost 12.2606, change in cost 0.00644112\n",
      "step 2560, training accuracy 0.959596, cost 12.2542, change in cost 0.00640488\n",
      "step 2570, training accuracy 0.959596, cost 12.2479, change in cost 0.00636768\n",
      "step 2580, training accuracy 0.959596, cost 12.2415, change in cost 0.0063343\n",
      "step 2590, training accuracy 0.959596, cost 12.2352, change in cost 0.00629711\n",
      "step 2600, training accuracy 0.959596, cost 12.229, change in cost 0.00626183\n",
      "step 2610, training accuracy 0.959596, cost 12.2227, change in cost 0.00622845\n",
      "step 2620, training accuracy 0.959596, cost 12.2166, change in cost 0.00619125\n",
      "step 2630, training accuracy 0.959596, cost 12.2104, change in cost 0.00615978\n",
      "step 2640, training accuracy 0.959596, cost 12.2043, change in cost 0.0061264\n",
      "step 2650, training accuracy 0.959596, cost 12.1982, change in cost 0.00608826\n",
      "step 2660, training accuracy 0.959596, cost 12.1921, change in cost 0.00605869\n",
      "step 2670, training accuracy 0.959596, cost 12.1861, change in cost 0.00602436\n",
      "step 2680, training accuracy 0.959596, cost 12.1801, change in cost 0.00599194\n",
      "step 2690, training accuracy 0.959596, cost 12.1741, change in cost 0.00596046\n",
      "step 2700, training accuracy 0.959596, cost 12.1682, change in cost 0.00592804\n",
      "step 2710, training accuracy 0.959596, cost 12.1623, change in cost 0.00589371\n",
      "step 2720, training accuracy 0.959596, cost 12.1565, change in cost 0.00586319\n",
      "step 2730, training accuracy 0.959596, cost 12.1506, change in cost 0.00582981\n",
      "step 2740, training accuracy 0.959596, cost 12.1448, change in cost 0.00580215\n",
      "step 2750, training accuracy 0.959596, cost 12.1391, change in cost 0.00576973\n",
      "step 2760, training accuracy 0.959596, cost 12.1333, change in cost 0.0057373\n",
      "step 2770, training accuracy 0.959596, cost 12.1276, change in cost 0.00570965\n",
      "step 2780, training accuracy 0.959596, cost 12.1219, change in cost 0.00567913\n",
      "step 2790, training accuracy 0.959596, cost 12.1163, change in cost 0.00564861\n",
      "step 2800, training accuracy 0.959596, cost 12.1107, change in cost 0.00562\n",
      "step 2810, training accuracy 0.959596, cost 12.1051, change in cost 0.00558949\n",
      "step 2820, training accuracy 0.959596, cost 12.0995, change in cost 0.00555897\n",
      "step 2830, training accuracy 0.959596, cost 12.094, change in cost 0.00553322\n",
      "step 2840, training accuracy 0.959596, cost 12.0885, change in cost 0.0055027\n",
      "step 2850, training accuracy 0.959596, cost 12.083, change in cost 0.00547504\n",
      "step 2860, training accuracy 0.959596, cost 12.0776, change in cost 0.00544643\n",
      "step 2870, training accuracy 0.959596, cost 12.0721, change in cost 0.00541878\n",
      "step 2880, training accuracy 0.959596, cost 12.0667, change in cost 0.00539112\n",
      "step 2890, training accuracy 0.959596, cost 12.0614, change in cost 0.00536156\n",
      "step 2900, training accuracy 0.959596, cost 12.056, change in cost 0.00533581\n",
      "step 2910, training accuracy 0.959596, cost 12.0507, change in cost 0.0053091\n",
      "step 2920, training accuracy 0.959596, cost 12.0455, change in cost 0.00528049\n",
      "step 2930, training accuracy 0.959596, cost 12.0402, change in cost 0.00525665\n",
      "step 2940, training accuracy 0.959596, cost 12.035, change in cost 0.00522804\n",
      "step 2950, training accuracy 0.959596, cost 12.0298, change in cost 0.00520515\n",
      "step 2960, training accuracy 0.959596, cost 12.0246, change in cost 0.00517464\n",
      "step 2970, training accuracy 0.959596, cost 12.0194, change in cost 0.00515079\n",
      "step 2980, training accuracy 0.959596, cost 12.0143, change in cost 0.00512505\n",
      "step 2990, training accuracy 0.959596, cost 12.0092, change in cost 0.00509834\n",
      "step 3000, training accuracy 0.959596, cost 12.0041, change in cost 0.00507545\n",
      "step 3010, training accuracy 0.959596, cost 11.9991, change in cost 0.00504971\n",
      "step 3020, training accuracy 0.959596, cost 11.9941, change in cost 0.00502491\n",
      "step 3030, training accuracy 0.959596, cost 11.9891, change in cost 0.00500011\n",
      "step 3040, training accuracy 0.959596, cost 11.9841, change in cost 0.00497532\n",
      "step 3050, training accuracy 0.959596, cost 11.9791, change in cost 0.00495243\n",
      "step 3060, training accuracy 0.959596, cost 11.9742, change in cost 0.00492477\n",
      "step 3070, training accuracy 0.959596, cost 11.9693, change in cost 0.00490475\n",
      "step 3080, training accuracy 0.959596, cost 11.9644, change in cost 0.00487995\n",
      "step 3090, training accuracy 0.959596, cost 11.9596, change in cost 0.00485706\n",
      "step 3100, training accuracy 0.959596, cost 11.9547, change in cost 0.00483322\n",
      "step 3110, training accuracy 0.959596, cost 11.9499, change in cost 0.00480938\n",
      "step 3120, training accuracy 0.959596, cost 11.9451, change in cost 0.00478745\n",
      "step 3130, training accuracy 0.959596, cost 11.9404, change in cost 0.0047636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3140, training accuracy 0.959596, cost 11.9356, change in cost 0.00474167\n",
      "step 3150, training accuracy 0.959596, cost 11.9309, change in cost 0.00471973\n",
      "step 3160, training accuracy 0.959596, cost 11.9262, change in cost 0.00469685\n",
      "step 3170, training accuracy 0.959596, cost 11.9216, change in cost 0.004673\n",
      "step 3180, training accuracy 0.959596, cost 11.9169, change in cost 0.00465393\n",
      "step 3190, training accuracy 0.959596, cost 11.9123, change in cost 0.00463009\n",
      "step 3200, training accuracy 0.959596, cost 11.9077, change in cost 0.00461006\n",
      "step 3210, training accuracy 0.959596, cost 11.9031, change in cost 0.00458717\n",
      "step 3220, training accuracy 0.959596, cost 11.8985, change in cost 0.00456619\n",
      "step 3230, training accuracy 0.959596, cost 11.894, change in cost 0.00454426\n",
      "step 3240, training accuracy 0.959596, cost 11.8894, change in cost 0.00452709\n",
      "step 3250, training accuracy 0.959596, cost 11.8849, change in cost 0.00450325\n",
      "step 3260, training accuracy 0.959596, cost 11.8804, change in cost 0.00448227\n",
      "step 3270, training accuracy 0.959596, cost 11.876, change in cost 0.0044632\n",
      "step 3280, training accuracy 0.959596, cost 11.8715, change in cost 0.00444126\n",
      "step 3290, training accuracy 0.959596, cost 11.8671, change in cost 0.00442219\n",
      "step 3300, training accuracy 0.959596, cost 11.8627, change in cost 0.00440216\n",
      "step 3310, training accuracy 0.959596, cost 11.8583, change in cost 0.00438118\n",
      "step 3320, training accuracy 0.959596, cost 11.854, change in cost 0.00436211\n",
      "step 3330, training accuracy 0.959596, cost 11.8496, change in cost 0.00434303\n",
      "step 3340, training accuracy 0.959596, cost 11.8453, change in cost 0.00432205\n",
      "step 3350, training accuracy 0.959596, cost 11.841, change in cost 0.00430202\n",
      "step 3360, training accuracy 0.959596, cost 11.8367, change in cost 0.00428486\n",
      "step 3370, training accuracy 0.959596, cost 11.8325, change in cost 0.00426579\n",
      "step 3380, training accuracy 0.959596, cost 11.8282, change in cost 0.00424767\n",
      "step 3390, training accuracy 0.959596, cost 11.824, change in cost 0.00422668\n",
      "step 3400, training accuracy 0.959596, cost 11.8198, change in cost 0.00420952\n",
      "step 3410, training accuracy 0.959596, cost 11.8156, change in cost 0.00419044\n",
      "step 3420, training accuracy 0.959596, cost 11.8114, change in cost 0.00417137\n",
      "step 3430, training accuracy 0.959596, cost 11.8073, change in cost 0.00415516\n",
      "step 3440, training accuracy 0.959596, cost 11.8031, change in cost 0.00413418\n",
      "step 3450, training accuracy 0.959596, cost 11.799, change in cost 0.00411797\n",
      "step 3460, training accuracy 0.959596, cost 11.7949, change in cost 0.00409794\n",
      "step 3470, training accuracy 0.959596, cost 11.7908, change in cost 0.00408459\n",
      "step 3480, training accuracy 0.959596, cost 11.7868, change in cost 0.00406265\n",
      "step 3490, training accuracy 0.959596, cost 11.7827, change in cost 0.00404644\n",
      "step 3500, training accuracy 0.959596, cost 11.7787, change in cost 0.00402927\n",
      "step 3510, training accuracy 0.959596, cost 11.7747, change in cost 0.00401115\n",
      "step 3520, training accuracy 0.959596, cost 11.7707, change in cost 0.00399494\n",
      "step 3530, training accuracy 0.959596, cost 11.7667, change in cost 0.00397778\n",
      "step 3540, training accuracy 0.959596, cost 11.7627, change in cost 0.00396061\n",
      "step 3550, training accuracy 0.959596, cost 11.7588, change in cost 0.00394535\n",
      "step 3560, training accuracy 0.959596, cost 11.7549, change in cost 0.00392628\n",
      "step 3570, training accuracy 0.959596, cost 11.751, change in cost 0.00391197\n",
      "step 3580, training accuracy 0.959596, cost 11.7471, change in cost 0.0038929\n",
      "step 3590, training accuracy 0.959596, cost 11.7432, change in cost 0.00387859\n",
      "step 3600, training accuracy 0.959596, cost 11.7393, change in cost 0.00386047\n",
      "step 3610, training accuracy 0.959596, cost 11.7355, change in cost 0.00384808\n",
      "step 3620, training accuracy 0.959596, cost 11.7316, change in cost 0.003829\n",
      "step 3630, training accuracy 0.959596, cost 11.7278, change in cost 0.00381374\n",
      "step 3640, training accuracy 0.959596, cost 11.724, change in cost 0.00379848\n",
      "step 3650, training accuracy 0.959596, cost 11.7203, change in cost 0.00378036\n",
      "step 3660, training accuracy 0.959596, cost 11.7165, change in cost 0.00376701\n",
      "step 3670, training accuracy 0.959596, cost 11.7127, change in cost 0.00375271\n",
      "step 3680, training accuracy 0.959596, cost 11.709, change in cost 0.00373554\n",
      "step 3690, training accuracy 0.959596, cost 11.7053, change in cost 0.00372028\n",
      "step 3700, training accuracy 0.959596, cost 11.7016, change in cost 0.00370502\n",
      "step 3710, training accuracy 0.959596, cost 11.6979, change in cost 0.00368881\n",
      "step 3720, training accuracy 0.959596, cost 11.6942, change in cost 0.00367641\n",
      "step 3730, training accuracy 0.959596, cost 11.6905, change in cost 0.0036602\n",
      "step 3740, training accuracy 0.959596, cost 11.6869, change in cost 0.0036478\n",
      "step 3750, training accuracy 0.959596, cost 11.6833, change in cost 0.00362778\n",
      "step 3760, training accuracy 0.959596, cost 11.6797, change in cost 0.00361729\n",
      "step 3770, training accuracy 0.959596, cost 11.6761, change in cost 0.00360203\n",
      "step 3780, training accuracy 0.959596, cost 11.6725, change in cost 0.00358582\n",
      "step 3790, training accuracy 0.959596, cost 11.6689, change in cost 0.00357437\n",
      "step 3800, training accuracy 0.959596, cost 11.6653, change in cost 0.00355721\n",
      "step 3810, training accuracy 0.959596, cost 11.6618, change in cost 0.00354671\n",
      "step 3820, training accuracy 0.959596, cost 11.6583, change in cost 0.00352955\n",
      "step 3830, training accuracy 0.959596, cost 11.6547, change in cost 0.00351715\n",
      "step 3840, training accuracy 0.959596, cost 11.6512, change in cost 0.00350285\n",
      "step 3850, training accuracy 0.959596, cost 11.6477, change in cost 0.00348949\n",
      "step 3860, training accuracy 0.959596, cost 11.6443, change in cost 0.00347519\n",
      "step 3870, training accuracy 0.959596, cost 11.6408, change in cost 0.00346279\n",
      "step 3880, training accuracy 0.959596, cost 11.6374, change in cost 0.00344753\n",
      "step 3890, training accuracy 0.959596, cost 11.6339, change in cost 0.00343418\n",
      "step 3900, training accuracy 0.959596, cost 11.6305, change in cost 0.00342369\n",
      "step 3910, training accuracy 0.959596, cost 11.6271, change in cost 0.00340748\n",
      "step 3920, training accuracy 0.959596, cost 11.6237, change in cost 0.00339603\n",
      "step 3930, training accuracy 0.959596, cost 11.6203, change in cost 0.00338268\n",
      "step 3940, training accuracy 0.959596, cost 11.617, change in cost 0.00336742\n",
      "step 3950, training accuracy 0.959596, cost 11.6136, change in cost 0.00335789\n",
      "step 3960, training accuracy 0.959596, cost 11.6103, change in cost 0.00334358\n",
      "step 3970, training accuracy 0.959596, cost 11.6069, change in cost 0.00333118\n",
      "step 3980, training accuracy 0.959596, cost 11.6036, change in cost 0.00331688\n",
      "step 3990, training accuracy 0.959596, cost 11.6003, change in cost 0.00330448\n",
      "step 4000, training accuracy 0.959596, cost 11.597, change in cost 0.00329494\n",
      "step 4010, training accuracy 0.959596, cost 11.5937, change in cost 0.00327873\n",
      "step 4020, training accuracy 0.959596, cost 11.5905, change in cost 0.00326824\n",
      "step 4030, training accuracy 0.959596, cost 11.5872, change in cost 0.0032568\n",
      "step 4040, training accuracy 0.959596, cost 11.584, change in cost 0.0032444\n",
      "step 4050, training accuracy 0.959596, cost 11.5807, change in cost 0.00323105\n",
      "step 4060, training accuracy 0.959596, cost 11.5775, change in cost 0.00322151\n",
      "step 4070, training accuracy 0.959596, cost 11.5743, change in cost 0.00320625\n",
      "step 4080, training accuracy 0.959596, cost 11.5711, change in cost 0.00319576\n",
      "step 4090, training accuracy 0.959596, cost 11.5679, change in cost 0.00318432\n",
      "step 4100, training accuracy 0.959596, cost 11.5647, change in cost 0.00317192\n",
      "step 4110, training accuracy 0.959596, cost 11.5616, change in cost 0.00316048\n",
      "step 4120, training accuracy 0.959596, cost 11.5584, change in cost 0.00314903\n",
      "step 4130, training accuracy 0.959596, cost 11.5553, change in cost 0.00313854\n",
      "step 4140, training accuracy 0.959596, cost 11.5522, change in cost 0.00312614\n",
      "step 4150, training accuracy 0.959596, cost 11.5491, change in cost 0.00311375\n",
      "step 4160, training accuracy 0.959596, cost 11.546, change in cost 0.0031023\n",
      "step 4170, training accuracy 0.959596, cost 11.5429, change in cost 0.00309277\n",
      "step 4180, training accuracy 0.959596, cost 11.5398, change in cost 0.00308228\n",
      "step 4190, training accuracy 0.959596, cost 11.5367, change in cost 0.00306892\n",
      "step 4200, training accuracy 0.959596, cost 11.5337, change in cost 0.00305843\n",
      "step 4210, training accuracy 0.959596, cost 11.5306, change in cost 0.00304699\n",
      "step 4220, training accuracy 0.959596, cost 11.5276, change in cost 0.00303745\n",
      "step 4230, training accuracy 0.959596, cost 11.5245, change in cost 0.00302505\n",
      "step 4240, training accuracy 0.959596, cost 11.5215, change in cost 0.00301552\n",
      "step 4250, training accuracy 0.959596, cost 11.5185, change in cost 0.00300503\n",
      "step 4260, training accuracy 0.959596, cost 11.5155, change in cost 0.00299358\n",
      "step 4270, training accuracy 0.959596, cost 11.5125, change in cost 0.00298214\n",
      "step 4280, training accuracy 0.959596, cost 11.5096, change in cost 0.00297356\n",
      "step 4290, training accuracy 0.959596, cost 11.5066, change in cost 0.00296021\n",
      "step 4300, training accuracy 0.959596, cost 11.5037, change in cost 0.00295162\n",
      "step 4310, training accuracy 0.959596, cost 11.5007, change in cost 0.00294209\n",
      "step 4320, training accuracy 0.959596, cost 11.4978, change in cost 0.00292969\n",
      "step 4330, training accuracy 0.959596, cost 11.4949, change in cost 0.00292206\n",
      "step 4340, training accuracy 0.959596, cost 11.492, change in cost 0.00290966\n",
      "step 4350, training accuracy 0.959596, cost 11.4891, change in cost 0.00290108\n",
      "step 4360, training accuracy 0.959596, cost 11.4862, change in cost 0.00289059\n",
      "step 4370, training accuracy 0.959596, cost 11.4833, change in cost 0.0028801\n",
      "step 4380, training accuracy 0.959596, cost 11.4804, change in cost 0.00287056\n",
      "step 4390, training accuracy 0.959596, cost 11.4776, change in cost 0.00285912\n",
      "step 4400, training accuracy 0.959596, cost 11.4747, change in cost 0.00285053\n",
      "step 4410, training accuracy 0.959596, cost 11.4719, change in cost 0.002841\n",
      "step 4420, training accuracy 0.959596, cost 11.469, change in cost 0.00283146\n",
      "step 4430, training accuracy 0.959596, cost 11.4662, change in cost 0.00282192\n",
      "step 4440, training accuracy 0.949495, cost 11.4634, change in cost 0.00281239\n",
      "step 4450, training accuracy 0.949495, cost 11.4606, change in cost 0.0028019\n",
      "step 4460, training accuracy 0.949495, cost 11.4578, change in cost 0.00279331\n",
      "step 4470, training accuracy 0.949495, cost 11.455, change in cost 0.00278282\n",
      "step 4480, training accuracy 0.949495, cost 11.4522, change in cost 0.00277328\n",
      "step 4490, training accuracy 0.949495, cost 11.4495, change in cost 0.0027647\n",
      "step 4500, training accuracy 0.949495, cost 11.4467, change in cost 0.00275517\n",
      "step 4510, training accuracy 0.949495, cost 11.444, change in cost 0.00274658\n",
      "step 4520, training accuracy 0.949495, cost 11.4412, change in cost 0.00273895\n",
      "step 4530, training accuracy 0.949495, cost 11.4385, change in cost 0.00272751\n",
      "step 4540, training accuracy 0.949495, cost 11.4358, change in cost 0.00271702\n",
      "step 4550, training accuracy 0.949495, cost 11.4331, change in cost 0.00271034\n",
      "step 4560, training accuracy 0.949495, cost 11.4304, change in cost 0.00269985\n",
      "step 4570, training accuracy 0.949495, cost 11.4277, change in cost 0.00269318\n",
      "step 4580, training accuracy 0.949495, cost 11.425, change in cost 0.00268364\n",
      "step 4590, training accuracy 0.949495, cost 11.4223, change in cost 0.00267315\n",
      "step 4600, training accuracy 0.949495, cost 11.4197, change in cost 0.00266552\n",
      "step 4610, training accuracy 0.949495, cost 11.417, change in cost 0.00265598\n",
      "step 4620, training accuracy 0.949495, cost 11.4144, change in cost 0.00264835\n",
      "step 4630, training accuracy 0.949495, cost 11.4117, change in cost 0.00263977\n",
      "step 4640, training accuracy 0.949495, cost 11.4091, change in cost 0.00263119\n",
      "step 4650, training accuracy 0.949495, cost 11.4065, change in cost 0.0026226\n",
      "step 4660, training accuracy 0.949495, cost 11.4039, change in cost 0.00261402\n",
      "step 4670, training accuracy 0.949495, cost 11.4013, change in cost 0.00260448\n",
      "step 4680, training accuracy 0.949495, cost 11.3987, change in cost 0.00259686\n",
      "step 4690, training accuracy 0.949495, cost 11.3961, change in cost 0.00259018\n",
      "step 4700, training accuracy 0.949495, cost 11.3935, change in cost 0.00257778\n",
      "step 4710, training accuracy 0.949495, cost 11.3909, change in cost 0.00257301\n",
      "step 4720, training accuracy 0.949495, cost 11.3884, change in cost 0.00256443\n",
      "step 4730, training accuracy 0.949495, cost 11.3858, change in cost 0.00255489\n",
      "step 4740, training accuracy 0.949495, cost 11.3833, change in cost 0.00254726\n",
      "step 4750, training accuracy 0.949495, cost 11.3807, change in cost 0.00253868\n",
      "step 4760, training accuracy 0.949495, cost 11.3782, change in cost 0.00253296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4770, training accuracy 0.949495, cost 11.3757, change in cost 0.00252342\n",
      "step 4780, training accuracy 0.949495, cost 11.3731, change in cost 0.00251484\n",
      "step 4790, training accuracy 0.949495, cost 11.3706, change in cost 0.00250721\n",
      "step 4800, training accuracy 0.949495, cost 11.3681, change in cost 0.00250053\n",
      "step 4810, training accuracy 0.949495, cost 11.3656, change in cost 0.002491\n",
      "step 4820, training accuracy 0.949495, cost 11.3632, change in cost 0.00248528\n",
      "step 4830, training accuracy 0.949495, cost 11.3607, change in cost 0.00247478\n",
      "step 4840, training accuracy 0.949495, cost 11.3582, change in cost 0.00246906\n",
      "step 4850, training accuracy 0.949495, cost 11.3558, change in cost 0.00246143\n",
      "step 4860, training accuracy 0.949495, cost 11.3533, change in cost 0.0024519\n",
      "step 4870, training accuracy 0.949495, cost 11.3509, change in cost 0.00244808\n",
      "step 4880, training accuracy 0.949495, cost 11.3484, change in cost 0.00243759\n",
      "step 4890, training accuracy 0.949495, cost 11.346, change in cost 0.00243092\n",
      "step 4900, training accuracy 0.949495, cost 11.3436, change in cost 0.00242329\n",
      "step 4910, training accuracy 0.949495, cost 11.3411, change in cost 0.0024147\n",
      "step 4920, training accuracy 0.949495, cost 11.3387, change in cost 0.00240898\n",
      "step 4930, training accuracy 0.949495, cost 11.3363, change in cost 0.0024004\n",
      "step 4940, training accuracy 0.949495, cost 11.3339, change in cost 0.00239468\n",
      "step 4950, training accuracy 0.949495, cost 11.3316, change in cost 0.00238705\n",
      "step 4960, training accuracy 0.949495, cost 11.3292, change in cost 0.00237751\n",
      "step 4970, training accuracy 0.949495, cost 11.3268, change in cost 0.0023737\n",
      "step 4980, training accuracy 0.949495, cost 11.3244, change in cost 0.00236416\n",
      "step 4990, training accuracy 0.949495, cost 11.3221, change in cost 0.00235748\n",
      "step 5000, training accuracy 0.949495, cost 11.3197, change in cost 0.00235176\n",
      "step 5010, training accuracy 0.949495, cost 11.3174, change in cost 0.00234222\n",
      "step 5020, training accuracy 0.949495, cost 11.3151, change in cost 0.00233746\n",
      "step 5030, training accuracy 0.949495, cost 11.3127, change in cost 0.00233078\n",
      "step 5040, training accuracy 0.949495, cost 11.3104, change in cost 0.00232315\n",
      "step 5050, training accuracy 0.949495, cost 11.3081, change in cost 0.00231552\n",
      "step 5060, training accuracy 0.949495, cost 11.3058, change in cost 0.00230885\n",
      "step 5070, training accuracy 0.949495, cost 11.3035, change in cost 0.00230312\n",
      "step 5080, training accuracy 0.949495, cost 11.3012, change in cost 0.00229454\n",
      "step 5090, training accuracy 0.949495, cost 11.2989, change in cost 0.00228786\n",
      "step 5100, training accuracy 0.949495, cost 11.2966, change in cost 0.00228214\n",
      "step 5110, training accuracy 0.949495, cost 11.2943, change in cost 0.00227451\n",
      "step 5120, training accuracy 0.949495, cost 11.2921, change in cost 0.0022707\n",
      "step 5130, training accuracy 0.949495, cost 11.2898, change in cost 0.00226116\n",
      "step 5140, training accuracy 0.949495, cost 11.2875, change in cost 0.00225544\n",
      "step 5150, training accuracy 0.949495, cost 11.2853, change in cost 0.00224972\n",
      "step 5160, training accuracy 0.949495, cost 11.2831, change in cost 0.00224018\n",
      "step 5170, training accuracy 0.949495, cost 11.2808, change in cost 0.00223637\n",
      "step 5180, training accuracy 0.949495, cost 11.2786, change in cost 0.00222969\n",
      "step 5190, training accuracy 0.949495, cost 11.2764, change in cost 0.00222206\n",
      "step 5200, training accuracy 0.949495, cost 11.2741, change in cost 0.00221729\n",
      "step 5210, training accuracy 0.949495, cost 11.2719, change in cost 0.00221062\n",
      "step 5220, training accuracy 0.949495, cost 11.2697, change in cost 0.00220299\n",
      "step 5230, training accuracy 0.949495, cost 11.2675, change in cost 0.00219631\n",
      "step 5240, training accuracy 0.949495, cost 11.2653, change in cost 0.0021925\n",
      "step 5250, training accuracy 0.949495, cost 11.2632, change in cost 0.00218487\n",
      "step 5260, training accuracy 0.949495, cost 11.261, change in cost 0.00217915\n",
      "step 5270, training accuracy 0.949495, cost 11.2588, change in cost 0.00217152\n",
      "step 5280, training accuracy 0.949495, cost 11.2566, change in cost 0.00216675\n",
      "step 5290, training accuracy 0.949495, cost 11.2545, change in cost 0.00216103\n",
      "step 5300, training accuracy 0.949495, cost 11.2523, change in cost 0.00215435\n",
      "step 5310, training accuracy 0.949495, cost 11.2502, change in cost 0.00214672\n",
      "step 5320, training accuracy 0.949495, cost 11.248, change in cost 0.00214386\n",
      "step 5330, training accuracy 0.949495, cost 11.2459, change in cost 0.00213623\n",
      "step 5340, training accuracy 0.949495, cost 11.2438, change in cost 0.00212765\n",
      "step 5350, training accuracy 0.949495, cost 11.2416, change in cost 0.00212574\n",
      "step 5360, training accuracy 0.949495, cost 11.2395, change in cost 0.00211906\n",
      "step 5370, training accuracy 0.949495, cost 11.2374, change in cost 0.00211239\n",
      "step 5380, training accuracy 0.949495, cost 11.2353, change in cost 0.00210571\n",
      "step 5390, training accuracy 0.949495, cost 11.2332, change in cost 0.0021019\n",
      "step 5400, training accuracy 0.949495, cost 11.2311, change in cost 0.00209427\n",
      "step 5410, training accuracy 0.949495, cost 11.229, change in cost 0.00208855\n",
      "step 5420, training accuracy 0.949495, cost 11.2269, change in cost 0.00208378\n",
      "step 5430, training accuracy 0.949495, cost 11.2249, change in cost 0.0020771\n",
      "step 5440, training accuracy 0.949495, cost 11.2228, change in cost 0.00207233\n",
      "step 5450, training accuracy 0.949495, cost 11.2207, change in cost 0.00206757\n",
      "step 5460, training accuracy 0.949495, cost 11.2187, change in cost 0.00206089\n",
      "step 5470, training accuracy 0.949495, cost 11.2166, change in cost 0.00205421\n",
      "step 5480, training accuracy 0.949495, cost 11.2146, change in cost 0.00204945\n",
      "step 5490, training accuracy 0.949495, cost 11.2125, change in cost 0.00204563\n",
      "step 5500, training accuracy 0.949495, cost 11.2105, change in cost 0.002038\n",
      "step 5510, training accuracy 0.949495, cost 11.2084, change in cost 0.00203133\n",
      "step 5520, training accuracy 0.949495, cost 11.2064, change in cost 0.00202847\n",
      "step 5530, training accuracy 0.949495, cost 11.2044, change in cost 0.00202274\n",
      "step 5540, training accuracy 0.949495, cost 11.2024, change in cost 0.00201511\n",
      "step 5550, training accuracy 0.949495, cost 11.2004, change in cost 0.00201321\n",
      "step 5560, training accuracy 0.949495, cost 11.1984, change in cost 0.00200462\n",
      "step 5570, training accuracy 0.949495, cost 11.1964, change in cost 0.00199986\n",
      "step 5580, training accuracy 0.949495, cost 11.1944, change in cost 0.00199699\n",
      "step 5590, training accuracy 0.949495, cost 11.1924, change in cost 0.00198746\n",
      "step 5600, training accuracy 0.949495, cost 11.1904, change in cost 0.00198555\n",
      "step 5610, training accuracy 0.949495, cost 11.1884, change in cost 0.00197887\n",
      "step 5620, training accuracy 0.949495, cost 11.1864, change in cost 0.00197506\n",
      "step 5630, training accuracy 0.949495, cost 11.1845, change in cost 0.00196743\n",
      "step 5640, training accuracy 0.949495, cost 11.1825, change in cost 0.00196457\n",
      "step 5650, training accuracy 0.949495, cost 11.1805, change in cost 0.0019598\n",
      "step 5660, training accuracy 0.949495, cost 11.1786, change in cost 0.00195217\n",
      "step 5670, training accuracy 0.949495, cost 11.1766, change in cost 0.00194931\n",
      "step 5680, training accuracy 0.949495, cost 11.1747, change in cost 0.00194168\n",
      "step 5690, training accuracy 0.949495, cost 11.1728, change in cost 0.00193787\n",
      "step 5700, training accuracy 0.949495, cost 11.1708, change in cost 0.00193501\n",
      "step 5710, training accuracy 0.949495, cost 11.1689, change in cost 0.00192738\n",
      "step 5720, training accuracy 0.949495, cost 11.167, change in cost 0.00192356\n",
      "step 5730, training accuracy 0.949495, cost 11.1651, change in cost 0.00191689\n",
      "step 5740, training accuracy 0.949495, cost 11.1631, change in cost 0.00191307\n",
      "step 5750, training accuracy 0.949495, cost 11.1612, change in cost 0.00190926\n",
      "step 5760, training accuracy 0.949495, cost 11.1593, change in cost 0.00190353\n",
      "step 5770, training accuracy 0.949495, cost 11.1574, change in cost 0.00189781\n",
      "step 5780, training accuracy 0.949495, cost 11.1555, change in cost 0.00189495\n",
      "step 5790, training accuracy 0.949495, cost 11.1537, change in cost 0.00188732\n",
      "step 5800, training accuracy 0.949495, cost 11.1518, change in cost 0.00188541\n",
      "step 5810, training accuracy 0.949495, cost 11.1499, change in cost 0.00187874\n",
      "step 5820, training accuracy 0.949495, cost 11.148, change in cost 0.00187302\n",
      "step 5830, training accuracy 0.949495, cost 11.1461, change in cost 0.00187016\n",
      "step 5840, training accuracy 0.949495, cost 11.1443, change in cost 0.00186729\n",
      "step 5850, training accuracy 0.949495, cost 11.1424, change in cost 0.00185871\n",
      "step 5860, training accuracy 0.949495, cost 11.1406, change in cost 0.00185585\n",
      "step 5870, training accuracy 0.949495, cost 11.1387, change in cost 0.00185108\n",
      "step 5880, training accuracy 0.949495, cost 11.1369, change in cost 0.00184631\n",
      "step 5890, training accuracy 0.949495, cost 11.135, change in cost 0.0018425\n",
      "step 5900, training accuracy 0.949495, cost 11.1332, change in cost 0.00183582\n",
      "step 5910, training accuracy 0.949495, cost 11.1314, change in cost 0.00183201\n",
      "step 5920, training accuracy 0.949495, cost 11.1295, change in cost 0.0018301\n",
      "step 5930, training accuracy 0.949495, cost 11.1277, change in cost 0.00182056\n",
      "step 5940, training accuracy 0.949495, cost 11.1259, change in cost 0.00182056\n",
      "step 5950, training accuracy 0.949495, cost 11.1241, change in cost 0.00181389\n",
      "step 5960, training accuracy 0.949495, cost 11.1223, change in cost 0.00180912\n",
      "step 5970, training accuracy 0.949495, cost 11.1205, change in cost 0.00180626\n",
      "step 5980, training accuracy 0.949495, cost 11.1187, change in cost 0.0018034\n",
      "step 5990, training accuracy 0.949495, cost 11.1169, change in cost 0.00179482\n",
      "step 6000, training accuracy 0.949495, cost 11.1151, change in cost 0.00179195\n",
      "step 6010, training accuracy 0.949495, cost 11.1133, change in cost 0.00178719\n",
      "step 6020, training accuracy 0.949495, cost 11.1115, change in cost 0.00178432\n",
      "step 6030, training accuracy 0.949495, cost 11.1097, change in cost 0.0017786\n",
      "step 6040, training accuracy 0.949495, cost 11.1079, change in cost 0.00177574\n",
      "step 6050, training accuracy 0.949495, cost 11.1062, change in cost 0.00176907\n",
      "step 6060, training accuracy 0.949495, cost 11.1044, change in cost 0.0017662\n",
      "step 6070, training accuracy 0.949495, cost 11.1026, change in cost 0.00176239\n",
      "step 6080, training accuracy 0.949495, cost 11.1009, change in cost 0.00175667\n",
      "step 6090, training accuracy 0.949495, cost 11.0991, change in cost 0.00175381\n",
      "step 6100, training accuracy 0.949495, cost 11.0974, change in cost 0.00174904\n",
      "step 6110, training accuracy 0.949495, cost 11.0956, change in cost 0.00174618\n",
      "step 6120, training accuracy 0.949495, cost 11.0939, change in cost 0.00174046\n",
      "step 6130, training accuracy 0.949495, cost 11.0922, change in cost 0.00173569\n",
      "step 6140, training accuracy 0.949495, cost 11.0904, change in cost 0.00173378\n",
      "step 6150, training accuracy 0.949495, cost 11.0887, change in cost 0.00172806\n",
      "step 6160, training accuracy 0.949495, cost 11.087, change in cost 0.00172424\n",
      "step 6170, training accuracy 0.949495, cost 11.0853, change in cost 0.00172043\n",
      "step 6180, training accuracy 0.949495, cost 11.0835, change in cost 0.00171566\n",
      "step 6190, training accuracy 0.949495, cost 11.0818, change in cost 0.00171185\n",
      "step 6200, training accuracy 0.949495, cost 11.0801, change in cost 0.00170898\n",
      "step 6210, training accuracy 0.949495, cost 11.0784, change in cost 0.00170422\n",
      "step 6220, training accuracy 0.949495, cost 11.0767, change in cost 0.00169849\n",
      "step 6230, training accuracy 0.949495, cost 11.075, change in cost 0.00169563\n",
      "step 6240, training accuracy 0.949495, cost 11.0733, change in cost 0.00169277\n",
      "step 6250, training accuracy 0.949495, cost 11.0716, change in cost 0.001688\n",
      "step 6260, training accuracy 0.949495, cost 11.07, change in cost 0.00168419\n",
      "step 6270, training accuracy 0.949495, cost 11.0683, change in cost 0.00167942\n",
      "step 6280, training accuracy 0.949495, cost 11.0666, change in cost 0.00167656\n",
      "step 6290, training accuracy 0.949495, cost 11.0649, change in cost 0.00167274\n",
      "step 6300, training accuracy 0.949495, cost 11.0633, change in cost 0.00166702\n",
      "step 6310, training accuracy 0.949495, cost 11.0616, change in cost 0.00166607\n",
      "step 6320, training accuracy 0.949495, cost 11.0599, change in cost 0.00166035\n",
      "step 6330, training accuracy 0.949495, cost 11.0583, change in cost 0.00165653\n",
      "step 6340, training accuracy 0.949495, cost 11.0566, change in cost 0.00165367\n",
      "step 6350, training accuracy 0.949495, cost 11.055, change in cost 0.00164795\n",
      "step 6360, training accuracy 0.949495, cost 11.0533, change in cost 0.00164604\n",
      "step 6370, training accuracy 0.949495, cost 11.0517, change in cost 0.00164223\n",
      "step 6380, training accuracy 0.949495, cost 11.05, change in cost 0.00164032\n",
      "step 6390, training accuracy 0.939394, cost 11.0484, change in cost 0.00163269\n",
      "step 6400, training accuracy 0.939394, cost 11.0468, change in cost 0.00163078\n",
      "step 6410, training accuracy 0.939394, cost 11.0452, change in cost 0.00162601\n",
      "step 6420, training accuracy 0.939394, cost 11.0435, change in cost 0.00162411\n",
      "step 6430, training accuracy 0.939394, cost 11.0419, change in cost 0.00161934\n",
      "step 6440, training accuracy 0.939394, cost 11.0403, change in cost 0.00161552\n",
      "step 6450, training accuracy 0.939394, cost 11.0387, change in cost 0.00161171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6460, training accuracy 0.939394, cost 11.0371, change in cost 0.0016098\n",
      "step 6470, training accuracy 0.939394, cost 11.0355, change in cost 0.00160503\n",
      "step 6480, training accuracy 0.939394, cost 11.0339, change in cost 0.00160122\n",
      "step 6490, training accuracy 0.939394, cost 11.0323, change in cost 0.0015974\n",
      "step 6500, training accuracy 0.939394, cost 11.0307, change in cost 0.00159264\n",
      "step 6510, training accuracy 0.939394, cost 11.0291, change in cost 0.00159264\n",
      "step 6520, training accuracy 0.939394, cost 11.0275, change in cost 0.00158596\n",
      "step 6530, training accuracy 0.939394, cost 11.0259, change in cost 0.00158405\n",
      "step 6540, training accuracy 0.939394, cost 11.0243, change in cost 0.00158024\n",
      "step 6550, training accuracy 0.939394, cost 11.0228, change in cost 0.00157642\n",
      "step 6560, training accuracy 0.939394, cost 11.0212, change in cost 0.00157261\n",
      "step 6570, training accuracy 0.939394, cost 11.0196, change in cost 0.00156975\n",
      "step 6580, training accuracy 0.939394, cost 11.0181, change in cost 0.00156593\n",
      "step 6590, training accuracy 0.939394, cost 11.0165, change in cost 0.00156212\n",
      "step 6600, training accuracy 0.939394, cost 11.0149, change in cost 0.00156021\n",
      "step 6610, training accuracy 0.939394, cost 11.0134, change in cost 0.0015564\n",
      "step 6620, training accuracy 0.939394, cost 11.0118, change in cost 0.00155067\n",
      "step 6630, training accuracy 0.939394, cost 11.0103, change in cost 0.00155067\n",
      "step 6640, training accuracy 0.939394, cost 11.0087, change in cost 0.00154495\n",
      "step 6650, training accuracy 0.939394, cost 11.0072, change in cost 0.00154305\n",
      "step 6660, training accuracy 0.939394, cost 11.0056, change in cost 0.00153923\n",
      "step 6670, training accuracy 0.939394, cost 11.0041, change in cost 0.00153542\n",
      "step 6680, training accuracy 0.939394, cost 11.0026, change in cost 0.00153255\n",
      "step 6690, training accuracy 0.939394, cost 11.001, change in cost 0.00152779\n",
      "step 6700, training accuracy 0.939394, cost 10.9995, change in cost 0.00152588\n",
      "step 6710, training accuracy 0.939394, cost 10.998, change in cost 0.00152302\n",
      "step 6720, training accuracy 0.939394, cost 10.9965, change in cost 0.00152016\n",
      "step 6730, training accuracy 0.939394, cost 10.995, change in cost 0.00151348\n",
      "step 6740, training accuracy 0.939394, cost 10.9935, change in cost 0.00151348\n",
      "step 6750, training accuracy 0.939394, cost 10.9919, change in cost 0.00150871\n",
      "step 6760, training accuracy 0.939394, cost 10.9904, change in cost 0.00150681\n",
      "step 6770, training accuracy 0.939394, cost 10.9889, change in cost 0.00150394\n",
      "step 6780, training accuracy 0.939394, cost 10.9874, change in cost 0.00149918\n",
      "step 6790, training accuracy 0.939394, cost 10.9859, change in cost 0.00149632\n",
      "step 6800, training accuracy 0.939394, cost 10.9844, change in cost 0.0014925\n",
      "step 6810, training accuracy 0.939394, cost 10.983, change in cost 0.0014925\n",
      "step 6820, training accuracy 0.939394, cost 10.9815, change in cost 0.00148582\n",
      "step 6830, training accuracy 0.939394, cost 10.98, change in cost 0.00148487\n",
      "step 6840, training accuracy 0.939394, cost 10.9785, change in cost 0.0014801\n",
      "step 6850, training accuracy 0.939394, cost 10.977, change in cost 0.0014782\n",
      "step 6860, training accuracy 0.939394, cost 10.9756, change in cost 0.00147343\n",
      "step 6870, training accuracy 0.939394, cost 10.9741, change in cost 0.00147247\n",
      "step 6880, training accuracy 0.939394, cost 10.9726, change in cost 0.00146866\n",
      "step 6890, training accuracy 0.939394, cost 10.9711, change in cost 0.0014658\n",
      "step 6900, training accuracy 0.939394, cost 10.9697, change in cost 0.00146103\n",
      "step 6910, training accuracy 0.939394, cost 10.9682, change in cost 0.00145912\n",
      "step 6920, training accuracy 0.939394, cost 10.9668, change in cost 0.00145817\n",
      "step 6930, training accuracy 0.939394, cost 10.9653, change in cost 0.00145245\n",
      "step 6940, training accuracy 0.939394, cost 10.9639, change in cost 0.00144958\n",
      "step 6950, training accuracy 0.939394, cost 10.9624, change in cost 0.00144768\n",
      "step 6960, training accuracy 0.939394, cost 10.961, change in cost 0.00144291\n",
      "step 6970, training accuracy 0.939394, cost 10.9595, change in cost 0.00144196\n",
      "step 6980, training accuracy 0.939394, cost 10.9581, change in cost 0.00144005\n",
      "step 6990, training accuracy 0.939394, cost 10.9567, change in cost 0.00143433\n",
      "step 7000, training accuracy 0.939394, cost 10.9552, change in cost 0.00143242\n",
      "step 7010, training accuracy 0.939394, cost 10.9538, change in cost 0.00142956\n",
      "step 7020, training accuracy 0.939394, cost 10.9524, change in cost 0.0014267\n",
      "step 7030, training accuracy 0.939394, cost 10.9509, change in cost 0.00142288\n",
      "step 7040, training accuracy 0.939394, cost 10.9495, change in cost 0.00142002\n",
      "step 7050, training accuracy 0.939394, cost 10.9481, change in cost 0.00142002\n",
      "step 7060, training accuracy 0.939394, cost 10.9467, change in cost 0.00141335\n",
      "step 7070, training accuracy 0.939394, cost 10.9453, change in cost 0.00141525\n",
      "step 7080, training accuracy 0.939394, cost 10.9439, change in cost 0.00140762\n",
      "step 7090, training accuracy 0.939394, cost 10.9425, change in cost 0.00140572\n",
      "step 7100, training accuracy 0.939394, cost 10.9411, change in cost 0.00140572\n",
      "step 7110, training accuracy 0.939394, cost 10.9397, change in cost 0.00139999\n",
      "step 7120, training accuracy 0.939394, cost 10.9383, change in cost 0.00139809\n",
      "step 7130, training accuracy 0.939394, cost 10.9369, change in cost 0.00139427\n",
      "step 7140, training accuracy 0.939394, cost 10.9355, change in cost 0.00139332\n",
      "step 7150, training accuracy 0.939394, cost 10.9341, change in cost 0.00138855\n",
      "step 7160, training accuracy 0.939394, cost 10.9327, change in cost 0.00138664\n",
      "step 7170, training accuracy 0.939394, cost 10.9313, change in cost 0.00138474\n",
      "step 7180, training accuracy 0.939394, cost 10.9299, change in cost 0.00138092\n",
      "step 7190, training accuracy 0.939394, cost 10.9286, change in cost 0.00137901\n",
      "step 7200, training accuracy 0.939394, cost 10.9272, change in cost 0.0013752\n",
      "step 7210, training accuracy 0.939394, cost 10.9258, change in cost 0.00137329\n",
      "step 7220, training accuracy 0.939394, cost 10.9244, change in cost 0.00137043\n",
      "step 7230, training accuracy 0.939394, cost 10.9231, change in cost 0.00136852\n",
      "step 7240, training accuracy 0.939394, cost 10.9217, change in cost 0.00136375\n",
      "step 7250, training accuracy 0.939394, cost 10.9203, change in cost 0.00136375\n",
      "step 7260, training accuracy 0.939394, cost 10.919, change in cost 0.00135899\n",
      "step 7270, training accuracy 0.939394, cost 10.9176, change in cost 0.00135612\n",
      "step 7280, training accuracy 0.939394, cost 10.9163, change in cost 0.00135612\n",
      "step 7290, training accuracy 0.939394, cost 10.9149, change in cost 0.00134945\n",
      "step 7300, training accuracy 0.939394, cost 10.9136, change in cost 0.0013504\n",
      "step 7310, training accuracy 0.939394, cost 10.9122, change in cost 0.00134563\n",
      "step 7320, training accuracy 0.939394, cost 10.9109, change in cost 0.00134373\n",
      "step 7330, training accuracy 0.939394, cost 10.9095, change in cost 0.00134087\n",
      "step 7340, training accuracy 0.939394, cost 10.9082, change in cost 0.00133801\n",
      "step 7350, training accuracy 0.939394, cost 10.9069, change in cost 0.00133705\n",
      "step 7360, training accuracy 0.939394, cost 10.9055, change in cost 0.00133419\n",
      "step 7370, training accuracy 0.939394, cost 10.9042, change in cost 0.00133133\n",
      "step 7380, training accuracy 0.939394, cost 10.9029, change in cost 0.00132751\n",
      "step 7390, training accuracy 0.939394, cost 10.9015, change in cost 0.00132465\n",
      "step 7400, training accuracy 0.939394, cost 10.9002, change in cost 0.00132465\n",
      "step 7410, training accuracy 0.939394, cost 10.8989, change in cost 0.00131989\n",
      "step 7420, training accuracy 0.939394, cost 10.8976, change in cost 0.00131893\n",
      "step 7430, training accuracy 0.939394, cost 10.8963, change in cost 0.00131607\n",
      "step 7440, training accuracy 0.939394, cost 10.895, change in cost 0.00131321\n",
      "step 7450, training accuracy 0.939394, cost 10.8936, change in cost 0.00130939\n",
      "step 7460, training accuracy 0.939394, cost 10.8923, change in cost 0.00131035\n",
      "step 7470, training accuracy 0.939394, cost 10.891, change in cost 0.00130463\n",
      "step 7480, training accuracy 0.939394, cost 10.8897, change in cost 0.00130272\n",
      "step 7490, training accuracy 0.939394, cost 10.8884, change in cost 0.00130177\n",
      "step 7500, training accuracy 0.949495, cost 10.8871, change in cost 0.001297\n",
      "step 7510, training accuracy 0.949495, cost 10.8858, change in cost 0.001297\n",
      "step 7520, training accuracy 0.949495, cost 10.8845, change in cost 0.00129318\n",
      "step 7530, training accuracy 0.949495, cost 10.8832, change in cost 0.00129032\n",
      "step 7540, training accuracy 0.949495, cost 10.882, change in cost 0.00128746\n",
      "step 7550, training accuracy 0.949495, cost 10.8807, change in cost 0.00128746\n",
      "step 7560, training accuracy 0.949495, cost 10.8794, change in cost 0.0012846\n",
      "step 7570, training accuracy 0.949495, cost 10.8781, change in cost 0.00127983\n",
      "step 7580, training accuracy 0.949495, cost 10.8768, change in cost 0.00127983\n",
      "step 7590, training accuracy 0.949495, cost 10.8755, change in cost 0.00127602\n",
      "step 7600, training accuracy 0.949495, cost 10.8743, change in cost 0.00127411\n",
      "step 7610, training accuracy 0.949495, cost 10.873, change in cost 0.0012722\n",
      "step 7620, training accuracy 0.949495, cost 10.8717, change in cost 0.00126839\n",
      "step 7630, training accuracy 0.949495, cost 10.8705, change in cost 0.00126743\n",
      "step 7640, training accuracy 0.949495, cost 10.8692, change in cost 0.00126457\n",
      "step 7650, training accuracy 0.949495, cost 10.8679, change in cost 0.00126171\n",
      "step 7660, training accuracy 0.949495, cost 10.8667, change in cost 0.00126171\n",
      "step 7670, training accuracy 0.949495, cost 10.8654, change in cost 0.0012579\n",
      "step 7680, training accuracy 0.949495, cost 10.8642, change in cost 0.00125504\n",
      "step 7690, training accuracy 0.949495, cost 10.8629, change in cost 0.00125408\n",
      "step 7700, training accuracy 0.949495, cost 10.8617, change in cost 0.00125027\n",
      "step 7710, training accuracy 0.949495, cost 10.8604, change in cost 0.00124741\n",
      "step 7720, training accuracy 0.949495, cost 10.8592, change in cost 0.00124645\n",
      "step 7730, training accuracy 0.949495, cost 10.8579, change in cost 0.00124645\n",
      "step 7740, training accuracy 0.949495, cost 10.8567, change in cost 0.00124168\n",
      "step 7750, training accuracy 0.949495, cost 10.8554, change in cost 0.00123882\n",
      "step 7760, training accuracy 0.949495, cost 10.8542, change in cost 0.00123787\n",
      "step 7770, training accuracy 0.949495, cost 10.853, change in cost 0.0012331\n",
      "step 7780, training accuracy 0.949495, cost 10.8517, change in cost 0.00123405\n",
      "step 7790, training accuracy 0.949495, cost 10.8505, change in cost 0.00123024\n",
      "step 7800, training accuracy 0.949495, cost 10.8493, change in cost 0.00122738\n",
      "step 7810, training accuracy 0.949495, cost 10.8481, change in cost 0.00122643\n",
      "step 7820, training accuracy 0.949495, cost 10.8468, change in cost 0.00122547\n",
      "step 7830, training accuracy 0.949495, cost 10.8456, change in cost 0.0012207\n",
      "step 7840, training accuracy 0.949495, cost 10.8444, change in cost 0.00121975\n",
      "step 7850, training accuracy 0.949495, cost 10.8432, change in cost 0.00121784\n",
      "step 7860, training accuracy 0.949495, cost 10.842, change in cost 0.00121403\n",
      "step 7870, training accuracy 0.949495, cost 10.8407, change in cost 0.00121307\n",
      "step 7880, training accuracy 0.949495, cost 10.8395, change in cost 0.00121021\n",
      "step 7890, training accuracy 0.949495, cost 10.8383, change in cost 0.00121117\n",
      "step 7900, training accuracy 0.949495, cost 10.8371, change in cost 0.00120544\n",
      "step 7910, training accuracy 0.949495, cost 10.8359, change in cost 0.00120544\n",
      "step 7920, training accuracy 0.949495, cost 10.8347, change in cost 0.00120258\n",
      "step 7930, training accuracy 0.949495, cost 10.8335, change in cost 0.00119972\n",
      "step 7940, training accuracy 0.949495, cost 10.8323, change in cost 0.00119781\n",
      "step 7950, training accuracy 0.949495, cost 10.8311, change in cost 0.00119591\n",
      "step 7960, training accuracy 0.949495, cost 10.8299, change in cost 0.00119305\n",
      "step 7970, training accuracy 0.949495, cost 10.8287, change in cost 0.00119114\n",
      "step 7980, training accuracy 0.949495, cost 10.8275, change in cost 0.00119019\n",
      "step 7990, training accuracy 0.949495, cost 10.8264, change in cost 0.00118828\n",
      "step 8000, training accuracy 0.949495, cost 10.8252, change in cost 0.00118446\n",
      "step 8010, training accuracy 0.949495, cost 10.824, change in cost 0.00118351\n",
      "step 8020, training accuracy 0.949495, cost 10.8228, change in cost 0.0011816\n",
      "step 8030, training accuracy 0.949495, cost 10.8216, change in cost 0.00117874\n",
      "step 8040, training accuracy 0.949495, cost 10.8204, change in cost 0.00117874\n",
      "step 8050, training accuracy 0.949495, cost 10.8193, change in cost 0.00117493\n",
      "step 8060, training accuracy 0.949495, cost 10.8181, change in cost 0.00117302\n",
      "step 8070, training accuracy 0.949495, cost 10.8169, change in cost 0.00117111\n",
      "step 8080, training accuracy 0.949495, cost 10.8158, change in cost 0.0011692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8090, training accuracy 0.949495, cost 10.8146, change in cost 0.00116634\n",
      "step 8100, training accuracy 0.949495, cost 10.8134, change in cost 0.0011673\n",
      "step 8110, training accuracy 0.949495, cost 10.8123, change in cost 0.00116253\n",
      "step 8120, training accuracy 0.949495, cost 10.8111, change in cost 0.00115967\n",
      "step 8130, training accuracy 0.949495, cost 10.8099, change in cost 0.00115967\n",
      "step 8140, training accuracy 0.949495, cost 10.8088, change in cost 0.00115776\n",
      "step 8150, training accuracy 0.949495, cost 10.8076, change in cost 0.0011549\n",
      "step 8160, training accuracy 0.949495, cost 10.8065, change in cost 0.00115299\n",
      "step 8170, training accuracy 0.949495, cost 10.8053, change in cost 0.00115204\n",
      "step 8180, training accuracy 0.949495, cost 10.8042, change in cost 0.00114822\n",
      "step 8190, training accuracy 0.949495, cost 10.803, change in cost 0.00114727\n",
      "step 8200, training accuracy 0.949495, cost 10.8019, change in cost 0.00114536\n",
      "step 8210, training accuracy 0.949495, cost 10.8007, change in cost 0.00114441\n",
      "step 8220, training accuracy 0.949495, cost 10.7996, change in cost 0.00114059\n",
      "step 8230, training accuracy 0.949495, cost 10.7985, change in cost 0.00113869\n",
      "step 8240, training accuracy 0.949495, cost 10.7973, change in cost 0.00113869\n",
      "step 8250, training accuracy 0.949495, cost 10.7962, change in cost 0.00113487\n",
      "step 8260, training accuracy 0.949495, cost 10.7951, change in cost 0.00113297\n",
      "step 8270, training accuracy 0.949495, cost 10.7939, change in cost 0.00113297\n",
      "step 8280, training accuracy 0.949495, cost 10.7928, change in cost 0.00113106\n",
      "step 8290, training accuracy 0.949495, cost 10.7917, change in cost 0.0011282\n",
      "step 8300, training accuracy 0.949495, cost 10.7905, change in cost 0.00112438\n",
      "step 8310, training accuracy 0.949495, cost 10.7894, change in cost 0.00112534\n",
      "step 8320, training accuracy 0.949495, cost 10.7883, change in cost 0.00112247\n",
      "step 8330, training accuracy 0.949495, cost 10.7872, change in cost 0.00111866\n",
      "step 8340, training accuracy 0.949495, cost 10.786, change in cost 0.00112057\n",
      "step 8350, training accuracy 0.949495, cost 10.7849, change in cost 0.00111866\n",
      "step 8360, training accuracy 0.949495, cost 10.7838, change in cost 0.00111389\n",
      "step 8370, training accuracy 0.949495, cost 10.7827, change in cost 0.00111198\n",
      "step 8380, training accuracy 0.949495, cost 10.7816, change in cost 0.00111103\n",
      "step 8390, training accuracy 0.949495, cost 10.7805, change in cost 0.00111008\n",
      "step 8400, training accuracy 0.949495, cost 10.7794, change in cost 0.00110817\n",
      "step 8410, training accuracy 0.949495, cost 10.7783, change in cost 0.00110531\n",
      "step 8420, training accuracy 0.949495, cost 10.7772, change in cost 0.00110435\n",
      "step 8430, training accuracy 0.949495, cost 10.7761, change in cost 0.00110054\n",
      "step 8440, training accuracy 0.949495, cost 10.775, change in cost 0.00110054\n",
      "step 8450, training accuracy 0.949495, cost 10.7739, change in cost 0.00109959\n",
      "step 8460, training accuracy 0.949495, cost 10.7728, change in cost 0.00109673\n",
      "step 8470, training accuracy 0.949495, cost 10.7717, change in cost 0.00109577\n",
      "step 8480, training accuracy 0.949495, cost 10.7706, change in cost 0.00109291\n",
      "step 8490, training accuracy 0.949495, cost 10.7695, change in cost 0.00109005\n",
      "step 8500, training accuracy 0.949495, cost 10.7684, change in cost 0.00109005\n",
      "step 8510, training accuracy 0.949495, cost 10.7673, change in cost 0.00108624\n",
      "step 8520, training accuracy 0.949495, cost 10.7662, change in cost 0.00108719\n",
      "step 8530, training accuracy 0.949495, cost 10.7651, change in cost 0.00108433\n",
      "step 8540, training accuracy 0.949495, cost 10.7641, change in cost 0.00108147\n",
      "step 8550, training accuracy 0.949495, cost 10.763, change in cost 0.00108147\n",
      "step 8560, training accuracy 0.949495, cost 10.7619, change in cost 0.00107956\n",
      "step 8570, training accuracy 0.949495, cost 10.7608, change in cost 0.00107765\n",
      "step 8580, training accuracy 0.949495, cost 10.7597, change in cost 0.00107574\n",
      "step 8590, training accuracy 0.949495, cost 10.7587, change in cost 0.00107384\n",
      "step 8600, training accuracy 0.949495, cost 10.7576, change in cost 0.00107288\n",
      "step 8610, training accuracy 0.949495, cost 10.7565, change in cost 0.00106812\n",
      "step 8620, training accuracy 0.949495, cost 10.7555, change in cost 0.00107002\n",
      "step 8630, training accuracy 0.949495, cost 10.7544, change in cost 0.00106716\n",
      "step 8640, training accuracy 0.949495, cost 10.7533, change in cost 0.00106525\n",
      "step 8650, training accuracy 0.949495, cost 10.7523, change in cost 0.00106335\n",
      "step 8660, training accuracy 0.949495, cost 10.7512, change in cost 0.00106144\n",
      "step 8670, training accuracy 0.949495, cost 10.7501, change in cost 0.00105953\n",
      "step 8680, training accuracy 0.949495, cost 10.7491, change in cost 0.00106049\n",
      "step 8690, training accuracy 0.949495, cost 10.748, change in cost 0.00105572\n",
      "step 8700, training accuracy 0.949495, cost 10.747, change in cost 0.00105667\n",
      "step 8710, training accuracy 0.949495, cost 10.7459, change in cost 0.00105286\n",
      "step 8720, training accuracy 0.949495, cost 10.7449, change in cost 0.0010519\n",
      "step 8730, training accuracy 0.949495, cost 10.7438, change in cost 0.00105\n",
      "step 8740, training accuracy 0.949495, cost 10.7428, change in cost 0.00104904\n",
      "step 8750, training accuracy 0.949495, cost 10.7417, change in cost 0.00104713\n",
      "step 8760, training accuracy 0.949495, cost 10.7407, change in cost 0.00104427\n",
      "step 8770, training accuracy 0.949495, cost 10.7396, change in cost 0.00104332\n",
      "step 8780, training accuracy 0.949495, cost 10.7386, change in cost 0.00104332\n",
      "step 8790, training accuracy 0.949495, cost 10.7375, change in cost 0.00104046\n",
      "step 8800, training accuracy 0.949495, cost 10.7365, change in cost 0.00103855\n",
      "step 8810, training accuracy 0.949495, cost 10.7355, change in cost 0.00103569\n",
      "step 8820, training accuracy 0.949495, cost 10.7344, change in cost 0.00103569\n",
      "step 8830, training accuracy 0.949495, cost 10.7334, change in cost 0.00103474\n",
      "step 8840, training accuracy 0.949495, cost 10.7324, change in cost 0.00103283\n",
      "step 8850, training accuracy 0.949495, cost 10.7313, change in cost 0.00102997\n",
      "step 8860, training accuracy 0.949495, cost 10.7303, change in cost 0.00102806\n",
      "step 8870, training accuracy 0.949495, cost 10.7293, change in cost 0.00102806\n",
      "step 8880, training accuracy 0.949495, cost 10.7283, change in cost 0.00102711\n",
      "step 8890, training accuracy 0.949495, cost 10.7272, change in cost 0.00102234\n",
      "step 8900, training accuracy 0.949495, cost 10.7262, change in cost 0.00102329\n",
      "step 8910, training accuracy 0.949495, cost 10.7252, change in cost 0.00102139\n",
      "step 8920, training accuracy 0.949495, cost 10.7242, change in cost 0.00102043\n",
      "step 8930, training accuracy 0.949495, cost 10.7232, change in cost 0.00101662\n",
      "step 8940, training accuracy 0.949495, cost 10.7221, change in cost 0.00101757\n",
      "step 8950, training accuracy 0.949495, cost 10.7211, change in cost 0.0010128\n",
      "step 8960, training accuracy 0.949495, cost 10.7201, change in cost 0.00101566\n",
      "step 8970, training accuracy 0.949495, cost 10.7191, change in cost 0.00101089\n",
      "step 8980, training accuracy 0.949495, cost 10.7181, change in cost 0.00101089\n",
      "step 8990, training accuracy 0.949495, cost 10.7171, change in cost 0.00100803\n",
      "step 9000, training accuracy 0.949495, cost 10.7161, change in cost 0.00100803\n",
      "step 9010, training accuracy 0.949495, cost 10.7151, change in cost 0.00100517\n",
      "step 9020, training accuracy 0.949495, cost 10.7141, change in cost 0.00100422\n",
      "step 9030, training accuracy 0.949495, cost 10.7131, change in cost 0.00100136\n",
      "step 9040, training accuracy 0.949495, cost 10.7121, change in cost 0.00100231\n",
      "step 9050, training accuracy 0.949495, cost 10.7111, change in cost 0.000999451\n",
      "step 9060, training accuracy 0.949495, cost 10.7101, change in cost 0.000997543\n",
      "step 9070, training accuracy 0.949495, cost 10.7091, change in cost 0.000997543\n",
      "step 9080, training accuracy 0.949495, cost 10.7081, change in cost 0.000995636\n",
      "step 9090, training accuracy 0.949495, cost 10.7071, change in cost 0.000992775\n",
      "step 9100, training accuracy 0.949495, cost 10.7061, change in cost 0.000992775\n",
      "step 9110, training accuracy 0.949495, cost 10.7051, change in cost 0.000989914\n",
      "step 9120, training accuracy 0.949495, cost 10.7041, change in cost 0.000989914\n",
      "step 9130, training accuracy 0.949495, cost 10.7031, change in cost 0.000986099\n",
      "step 9140, training accuracy 0.949495, cost 10.7021, change in cost 0.000986099\n",
      "step 9150, training accuracy 0.949495, cost 10.7011, change in cost 0.000984192\n",
      "step 9160, training accuracy 0.949495, cost 10.7002, change in cost 0.000985146\n",
      "step 9170, training accuracy 0.949495, cost 10.6992, change in cost 0.000981331\n",
      "step 9180, training accuracy 0.949495, cost 10.6982, change in cost 0.000980377\n",
      "step 9190, training accuracy 0.949495, cost 10.6972, change in cost 0.000977516\n",
      "step 9200, training accuracy 0.949495, cost 10.6962, change in cost 0.000977516\n",
      "step 9210, training accuracy 0.949495, cost 10.6953, change in cost 0.000976562\n",
      "step 9220, training accuracy 0.949495, cost 10.6943, change in cost 0.000974655\n",
      "step 9230, training accuracy 0.949495, cost 10.6933, change in cost 0.000973701\n",
      "step 9240, training accuracy 0.949495, cost 10.6923, change in cost 0.00097084\n",
      "step 9250, training accuracy 0.949495, cost 10.6914, change in cost 0.000969887\n",
      "step 9260, training accuracy 0.949495, cost 10.6904, change in cost 0.000968933\n",
      "step 9270, training accuracy 0.949495, cost 10.6894, change in cost 0.000968933\n",
      "step 9280, training accuracy 0.949495, cost 10.6885, change in cost 0.000964165\n",
      "step 9290, training accuracy 0.949495, cost 10.6875, change in cost 0.000965118\n",
      "step 9300, training accuracy 0.949495, cost 10.6865, change in cost 0.000963211\n",
      "step 9310, training accuracy 0.949495, cost 10.6856, change in cost 0.00096035\n",
      "step 9320, training accuracy 0.949495, cost 10.6846, change in cost 0.000961304\n",
      "step 9330, training accuracy 0.949495, cost 10.6837, change in cost 0.000958443\n",
      "step 9340, training accuracy 0.949495, cost 10.6827, change in cost 0.000958443\n",
      "step 9350, training accuracy 0.949495, cost 10.6818, change in cost 0.000956535\n",
      "step 9360, training accuracy 0.949495, cost 10.6808, change in cost 0.000951767\n",
      "step 9370, training accuracy 0.949495, cost 10.6798, change in cost 0.000955582\n",
      "step 9380, training accuracy 0.949495, cost 10.6789, change in cost 0.00094986\n",
      "step 9390, training accuracy 0.949495, cost 10.6779, change in cost 0.000950813\n",
      "step 9400, training accuracy 0.949495, cost 10.677, change in cost 0.000950813\n",
      "step 9410, training accuracy 0.949495, cost 10.676, change in cost 0.000948906\n",
      "step 9420, training accuracy 0.949495, cost 10.6751, change in cost 0.000945091\n",
      "step 9430, training accuracy 0.949495, cost 10.6742, change in cost 0.000944138\n",
      "step 9440, training accuracy 0.949495, cost 10.6732, change in cost 0.000944138\n",
      "step 9450, training accuracy 0.949495, cost 10.6723, change in cost 0.000943184\n",
      "step 9460, training accuracy 0.949495, cost 10.6713, change in cost 0.000941277\n",
      "step 9470, training accuracy 0.949495, cost 10.6704, change in cost 0.000938416\n",
      "step 9480, training accuracy 0.949495, cost 10.6694, change in cost 0.000938416\n",
      "step 9490, training accuracy 0.949495, cost 10.6685, change in cost 0.000937462\n",
      "step 9500, training accuracy 0.949495, cost 10.6676, change in cost 0.000935555\n",
      "step 9510, training accuracy 0.949495, cost 10.6666, change in cost 0.000934601\n",
      "step 9520, training accuracy 0.949495, cost 10.6657, change in cost 0.00093174\n",
      "step 9530, training accuracy 0.949495, cost 10.6648, change in cost 0.000932693\n",
      "step 9540, training accuracy 0.949495, cost 10.6638, change in cost 0.000929832\n",
      "step 9550, training accuracy 0.949495, cost 10.6629, change in cost 0.000929832\n",
      "step 9560, training accuracy 0.949495, cost 10.662, change in cost 0.000926971\n",
      "step 9570, training accuracy 0.949495, cost 10.6611, change in cost 0.000926971\n",
      "step 9580, training accuracy 0.949495, cost 10.6601, change in cost 0.00092411\n",
      "step 9590, training accuracy 0.949495, cost 10.6592, change in cost 0.00092411\n",
      "step 9600, training accuracy 0.949495, cost 10.6583, change in cost 0.000923157\n",
      "step 9610, training accuracy 0.949495, cost 10.6574, change in cost 0.000920296\n",
      "step 9620, training accuracy 0.949495, cost 10.6565, change in cost 0.000920296\n",
      "step 9630, training accuracy 0.949495, cost 10.6555, change in cost 0.000916481\n",
      "step 9640, training accuracy 0.949495, cost 10.6546, change in cost 0.000920296\n",
      "step 9650, training accuracy 0.949495, cost 10.6537, change in cost 0.000914574\n",
      "step 9660, training accuracy 0.949495, cost 10.6528, change in cost 0.000914574\n",
      "step 9670, training accuracy 0.949495, cost 10.6519, change in cost 0.000912666\n",
      "step 9680, training accuracy 0.949495, cost 10.651, change in cost 0.00091362\n",
      "step 9690, training accuracy 0.949495, cost 10.65, change in cost 0.000910759\n",
      "step 9700, training accuracy 0.949495, cost 10.6491, change in cost 0.000908852\n",
      "step 9710, training accuracy 0.949495, cost 10.6482, change in cost 0.000907898\n",
      "step 9720, training accuracy 0.949495, cost 10.6473, change in cost 0.000907898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9730, training accuracy 0.949495, cost 10.6464, change in cost 0.000905037\n",
      "step 9740, training accuracy 0.949495, cost 10.6455, change in cost 0.000905037\n",
      "step 9750, training accuracy 0.949495, cost 10.6446, change in cost 0.000904083\n",
      "step 9760, training accuracy 0.949495, cost 10.6437, change in cost 0.000900269\n",
      "step 9770, training accuracy 0.949495, cost 10.6428, change in cost 0.000901222\n",
      "step 9780, training accuracy 0.949495, cost 10.6419, change in cost 0.000899315\n",
      "step 9790, training accuracy 0.949495, cost 10.641, change in cost 0.000899315\n",
      "step 9800, training accuracy 0.949495, cost 10.6401, change in cost 0.000896454\n",
      "step 9810, training accuracy 0.949495, cost 10.6392, change in cost 0.0008955\n",
      "step 9820, training accuracy 0.949495, cost 10.6383, change in cost 0.0008955\n",
      "step 9830, training accuracy 0.949495, cost 10.6374, change in cost 0.000893593\n",
      "step 9840, training accuracy 0.949495, cost 10.6365, change in cost 0.000890732\n",
      "step 9850, training accuracy 0.949495, cost 10.6356, change in cost 0.000891685\n",
      "step 9860, training accuracy 0.949495, cost 10.6348, change in cost 0.000888824\n",
      "step 9870, training accuracy 0.949495, cost 10.6339, change in cost 0.000889778\n",
      "step 9880, training accuracy 0.949495, cost 10.633, change in cost 0.000886917\n",
      "step 9890, training accuracy 0.949495, cost 10.6321, change in cost 0.000886917\n",
      "step 9900, training accuracy 0.949495, cost 10.6312, change in cost 0.000884056\n",
      "step 9910, training accuracy 0.949495, cost 10.6303, change in cost 0.000884056\n",
      "step 9920, training accuracy 0.949495, cost 10.6294, change in cost 0.000882149\n",
      "step 9930, training accuracy 0.949495, cost 10.6286, change in cost 0.000881195\n",
      "step 9940, training accuracy 0.949495, cost 10.6277, change in cost 0.000881195\n",
      "step 9950, training accuracy 0.949495, cost 10.6268, change in cost 0.000878334\n",
      "step 9960, training accuracy 0.949495, cost 10.6259, change in cost 0.00087738\n",
      "step 9970, training accuracy 0.949495, cost 10.625, change in cost 0.000875473\n",
      "step 9980, training accuracy 0.949495, cost 10.6242, change in cost 0.000876427\n",
      "step 9990, training accuracy 0.949495, cost 10.6233, change in cost 0.000874519\n",
      "step 10000, training accuracy 0.949495, cost 10.6224, change in cost 0.000871658\n",
      "step 10010, training accuracy 0.949495, cost 10.6216, change in cost 0.000871658\n",
      "step 10020, training accuracy 0.949495, cost 10.6207, change in cost 0.000871658\n",
      "step 10030, training accuracy 0.949495, cost 10.6198, change in cost 0.000869751\n",
      "step 10040, training accuracy 0.949495, cost 10.6189, change in cost 0.000867844\n",
      "step 10050, training accuracy 0.949495, cost 10.6181, change in cost 0.000867844\n",
      "step 10060, training accuracy 0.949495, cost 10.6172, change in cost 0.000864983\n",
      "step 10070, training accuracy 0.949495, cost 10.6163, change in cost 0.000864983\n",
      "step 10080, training accuracy 0.949495, cost 10.6155, change in cost 0.000863075\n",
      "step 10090, training accuracy 0.949495, cost 10.6146, change in cost 0.000863075\n",
      "step 10100, training accuracy 0.949495, cost 10.6138, change in cost 0.000862122\n",
      "step 10110, training accuracy 0.949495, cost 10.6129, change in cost 0.000859261\n",
      "step 10120, training accuracy 0.949495, cost 10.612, change in cost 0.000859261\n",
      "step 10130, training accuracy 0.949495, cost 10.6112, change in cost 0.000857353\n",
      "step 10140, training accuracy 0.949495, cost 10.6103, change in cost 0.000857353\n",
      "step 10150, training accuracy 0.949495, cost 10.6095, change in cost 0.000854492\n",
      "step 10160, training accuracy 0.949495, cost 10.6086, change in cost 0.000855446\n",
      "step 10170, training accuracy 0.949495, cost 10.6078, change in cost 0.000853539\n",
      "step 10180, training accuracy 0.949495, cost 10.6069, change in cost 0.000851631\n",
      "step 10190, training accuracy 0.949495, cost 10.6061, change in cost 0.000851631\n",
      "step 10200, training accuracy 0.949495, cost 10.6052, change in cost 0.000850677\n",
      "step 10210, training accuracy 0.949495, cost 10.6044, change in cost 0.000847816\n",
      "step 10220, training accuracy 0.949495, cost 10.6035, change in cost 0.00084877\n",
      "step 10230, training accuracy 0.949495, cost 10.6027, change in cost 0.000845909\n",
      "step 10240, training accuracy 0.949495, cost 10.6018, change in cost 0.000846863\n",
      "step 10250, training accuracy 0.949495, cost 10.601, change in cost 0.000844002\n",
      "step 10260, training accuracy 0.949495, cost 10.6001, change in cost 0.000842094\n",
      "step 10270, training accuracy 0.949495, cost 10.5993, change in cost 0.000843048\n",
      "step 10280, training accuracy 0.949495, cost 10.5984, change in cost 0.000841141\n",
      "step 10290, training accuracy 0.949495, cost 10.5976, change in cost 0.000840187\n",
      "step 10300, training accuracy 0.949495, cost 10.5968, change in cost 0.000840187\n",
      "step 10310, training accuracy 0.949495, cost 10.5959, change in cost 0.000837326\n",
      "step 10320, training accuracy 0.949495, cost 10.5951, change in cost 0.000835419\n",
      "step 10330, training accuracy 0.949495, cost 10.5943, change in cost 0.000834465\n",
      "step 10340, training accuracy 0.949495, cost 10.5934, change in cost 0.000836372\n",
      "step 10350, training accuracy 0.949495, cost 10.5926, change in cost 0.000833511\n",
      "step 10360, training accuracy 0.949495, cost 10.5918, change in cost 0.000832558\n",
      "step 10370, training accuracy 0.949495, cost 10.5909, change in cost 0.000829697\n",
      "step 10380, training accuracy 0.949495, cost 10.5901, change in cost 0.000832558\n",
      "step 10390, training accuracy 0.949495, cost 10.5893, change in cost 0.000826836\n",
      "step 10400, training accuracy 0.949495, cost 10.5884, change in cost 0.00083065\n",
      "step 10410, training accuracy 0.949495, cost 10.5876, change in cost 0.000826836\n",
      "step 10420, training accuracy 0.949495, cost 10.5868, change in cost 0.000824928\n",
      "step 10430, training accuracy 0.949495, cost 10.586, change in cost 0.000824928\n",
      "step 10440, training accuracy 0.949495, cost 10.5851, change in cost 0.000825882\n",
      "step 10450, training accuracy 0.949495, cost 10.5843, change in cost 0.000821114\n",
      "step 10460, training accuracy 0.949495, cost 10.5835, change in cost 0.000823975\n",
      "step 10470, training accuracy 0.949495, cost 10.5827, change in cost 0.000818253\n",
      "step 10480, training accuracy 0.949495, cost 10.5819, change in cost 0.00082016\n",
      "step 10490, training accuracy 0.949495, cost 10.581, change in cost 0.000819206\n",
      "step 10500, training accuracy 0.949495, cost 10.5802, change in cost 0.000818253\n",
      "step 10510, training accuracy 0.949495, cost 10.5794, change in cost 0.000816345\n",
      "step 10520, training accuracy 0.949495, cost 10.5786, change in cost 0.000815392\n",
      "step 10530, training accuracy 0.949495, cost 10.5778, change in cost 0.000814438\n",
      "step 10540, training accuracy 0.949495, cost 10.577, change in cost 0.000812531\n",
      "step 10550, training accuracy 0.949495, cost 10.5761, change in cost 0.000813484\n",
      "step 10560, training accuracy 0.949495, cost 10.5753, change in cost 0.000811577\n",
      "step 10570, training accuracy 0.949495, cost 10.5745, change in cost 0.000811577\n",
      "step 10580, training accuracy 0.949495, cost 10.5737, change in cost 0.000809669\n",
      "step 10590, training accuracy 0.949495, cost 10.5729, change in cost 0.000805855\n",
      "step 10600, training accuracy 0.949495, cost 10.5721, change in cost 0.000807762\n",
      "step 10610, training accuracy 0.949495, cost 10.5713, change in cost 0.000806808\n",
      "step 10620, training accuracy 0.949495, cost 10.5705, change in cost 0.000805855\n",
      "step 10630, training accuracy 0.949495, cost 10.5697, change in cost 0.000802994\n",
      "step 10640, training accuracy 0.949495, cost 10.5689, change in cost 0.000803947\n",
      "step 10650, training accuracy 0.949495, cost 10.5681, change in cost 0.000802994\n",
      "step 10660, training accuracy 0.949495, cost 10.5673, change in cost 0.000801086\n",
      "step 10670, training accuracy 0.949495, cost 10.5665, change in cost 0.000799179\n",
      "step 10680, training accuracy 0.949495, cost 10.5657, change in cost 0.000799179\n",
      "step 10690, training accuracy 0.949495, cost 10.5649, change in cost 0.000799179\n",
      "step 10700, training accuracy 0.949495, cost 10.5641, change in cost 0.000797272\n",
      "step 10710, training accuracy 0.949495, cost 10.5633, change in cost 0.000796318\n",
      "step 10720, training accuracy 0.949495, cost 10.5625, change in cost 0.000795364\n",
      "step 10730, training accuracy 0.949495, cost 10.5617, change in cost 0.000793457\n",
      "step 10740, training accuracy 0.949495, cost 10.5609, change in cost 0.000793457\n",
      "step 10750, training accuracy 0.949495, cost 10.5601, change in cost 0.000793457\n",
      "step 10760, training accuracy 0.949495, cost 10.5593, change in cost 0.00079155\n",
      "step 10770, training accuracy 0.949495, cost 10.5585, change in cost 0.000789642\n",
      "step 10780, training accuracy 0.949495, cost 10.5577, change in cost 0.000790596\n",
      "step 10790, training accuracy 0.949495, cost 10.5569, change in cost 0.000788689\n",
      "step 10800, training accuracy 0.949495, cost 10.5562, change in cost 0.000786781\n",
      "step 10810, training accuracy 0.949495, cost 10.5554, change in cost 0.000785828\n",
      "step 10820, training accuracy 0.949495, cost 10.5546, change in cost 0.000785828\n",
      "step 10830, training accuracy 0.949495, cost 10.5538, change in cost 0.000785828\n",
      "step 10840, training accuracy 0.949495, cost 10.553, change in cost 0.000782013\n",
      "step 10850, training accuracy 0.949495, cost 10.5522, change in cost 0.000782967\n",
      "step 10860, training accuracy 0.949495, cost 10.5515, change in cost 0.000782013\n",
      "step 10870, training accuracy 0.949495, cost 10.5507, change in cost 0.000781059\n",
      "step 10880, training accuracy 0.949495, cost 10.5499, change in cost 0.000780106\n",
      "step 10890, training accuracy 0.949495, cost 10.5491, change in cost 0.000778198\n",
      "step 10900, training accuracy 0.949495, cost 10.5483, change in cost 0.000778198\n",
      "step 10910, training accuracy 0.949495, cost 10.5476, change in cost 0.000776291\n",
      "step 10920, training accuracy 0.949495, cost 10.5468, change in cost 0.000775337\n",
      "step 10930, training accuracy 0.949495, cost 10.546, change in cost 0.000775337\n",
      "step 10940, training accuracy 0.949495, cost 10.5452, change in cost 0.000774384\n",
      "step 10950, training accuracy 0.949495, cost 10.5445, change in cost 0.000772476\n",
      "step 10960, training accuracy 0.949495, cost 10.5437, change in cost 0.000772476\n",
      "step 10970, training accuracy 0.949495, cost 10.5429, change in cost 0.000770569\n",
      "step 10980, training accuracy 0.949495, cost 10.5421, change in cost 0.000770569\n",
      "step 10990, training accuracy 0.949495, cost 10.5414, change in cost 0.000770569\n",
      "step 11000, training accuracy 0.949495, cost 10.5406, change in cost 0.000768661\n",
      "step 11010, training accuracy 0.949495, cost 10.5398, change in cost 0.000766754\n",
      "step 11020, training accuracy 0.949495, cost 10.5391, change in cost 0.000766754\n",
      "step 11030, training accuracy 0.949495, cost 10.5383, change in cost 0.000766754\n",
      "step 11040, training accuracy 0.949495, cost 10.5375, change in cost 0.000764847\n",
      "step 11050, training accuracy 0.949495, cost 10.5368, change in cost 0.000761986\n",
      "step 11060, training accuracy 0.949495, cost 10.536, change in cost 0.000762939\n",
      "step 11070, training accuracy 0.949495, cost 10.5353, change in cost 0.000761986\n",
      "step 11080, training accuracy 0.949495, cost 10.5345, change in cost 0.000761986\n",
      "step 11090, training accuracy 0.949495, cost 10.5337, change in cost 0.000759125\n",
      "step 11100, training accuracy 0.949495, cost 10.533, change in cost 0.000761032\n",
      "step 11110, training accuracy 0.949495, cost 10.5322, change in cost 0.000758171\n",
      "step 11120, training accuracy 0.949495, cost 10.5315, change in cost 0.00075531\n",
      "step 11130, training accuracy 0.949495, cost 10.5307, change in cost 0.000759125\n",
      "step 11140, training accuracy 0.949495, cost 10.5299, change in cost 0.000754356\n",
      "step 11150, training accuracy 0.949495, cost 10.5292, change in cost 0.00075531\n",
      "step 11160, training accuracy 0.949495, cost 10.5284, change in cost 0.000754356\n",
      "step 11170, training accuracy 0.949495, cost 10.5277, change in cost 0.000752449\n",
      "step 11180, training accuracy 0.949495, cost 10.5269, change in cost 0.000750542\n",
      "step 11190, training accuracy 0.949495, cost 10.5262, change in cost 0.000752449\n",
      "step 11200, training accuracy 0.949495, cost 10.5254, change in cost 0.000751495\n",
      "step 11210, training accuracy 0.949495, cost 10.5247, change in cost 0.000747681\n",
      "step 11220, training accuracy 0.949495, cost 10.5239, change in cost 0.000747681\n",
      "step 11230, training accuracy 0.949495, cost 10.5232, change in cost 0.000747681\n",
      "step 11240, training accuracy 0.949495, cost 10.5224, change in cost 0.000748634\n",
      "step 11250, training accuracy 0.949495, cost 10.5217, change in cost 0.000745773\n",
      "step 11260, training accuracy 0.949495, cost 10.521, change in cost 0.000742912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11270, training accuracy 0.949495, cost 10.5202, change in cost 0.000745773\n",
      "step 11280, training accuracy 0.949495, cost 10.5195, change in cost 0.000742912\n",
      "step 11290, training accuracy 0.949495, cost 10.5187, change in cost 0.000741959\n",
      "step 11300, training accuracy 0.949495, cost 10.518, change in cost 0.000741959\n",
      "step 11310, training accuracy 0.949495, cost 10.5172, change in cost 0.000741005\n",
      "step 11320, training accuracy 0.949495, cost 10.5165, change in cost 0.000739098\n",
      "step 11330, training accuracy 0.949495, cost 10.5158, change in cost 0.000739098\n",
      "step 11340, training accuracy 0.949495, cost 10.515, change in cost 0.000739098\n",
      "step 11350, training accuracy 0.949495, cost 10.5143, change in cost 0.000735283\n",
      "step 11360, training accuracy 0.949495, cost 10.5135, change in cost 0.00073719\n",
      "step 11370, training accuracy 0.949495, cost 10.5128, change in cost 0.000734329\n",
      "step 11380, training accuracy 0.949495, cost 10.5121, change in cost 0.000735283\n",
      "step 11390, training accuracy 0.949495, cost 10.5113, change in cost 0.000734329\n",
      "step 11400, training accuracy 0.949495, cost 10.5106, change in cost 0.000731468\n",
      "step 11410, training accuracy 0.949495, cost 10.5099, change in cost 0.000732422\n",
      "step 11420, training accuracy 0.949495, cost 10.5092, change in cost 0.000730515\n",
      "step 11430, training accuracy 0.949495, cost 10.5084, change in cost 0.000730515\n",
      "step 11440, training accuracy 0.949495, cost 10.5077, change in cost 0.000729561\n",
      "step 11450, training accuracy 0.949495, cost 10.507, change in cost 0.000728607\n",
      "step 11460, training accuracy 0.949495, cost 10.5062, change in cost 0.000727654\n",
      "step 11470, training accuracy 0.949495, cost 10.5055, change in cost 0.000728607\n",
      "step 11480, training accuracy 0.949495, cost 10.5048, change in cost 0.000724792\n",
      "step 11490, training accuracy 0.949495, cost 10.5041, change in cost 0.000725746\n",
      "step 11500, training accuracy 0.949495, cost 10.5033, change in cost 0.000723839\n",
      "step 11510, training accuracy 0.949495, cost 10.5026, change in cost 0.000724792\n",
      "step 11520, training accuracy 0.949495, cost 10.5019, change in cost 0.000721931\n",
      "step 11530, training accuracy 0.949495, cost 10.5012, change in cost 0.000720978\n",
      "step 11540, training accuracy 0.949495, cost 10.5004, change in cost 0.000721931\n",
      "step 11550, training accuracy 0.949495, cost 10.4997, change in cost 0.000720024\n",
      "step 11560, training accuracy 0.949495, cost 10.499, change in cost 0.000720024\n",
      "step 11570, training accuracy 0.949495, cost 10.4983, change in cost 0.000720024\n",
      "step 11580, training accuracy 0.949495, cost 10.4976, change in cost 0.000715256\n",
      "step 11590, training accuracy 0.949495, cost 10.4968, change in cost 0.000718117\n",
      "step 11600, training accuracy 0.949495, cost 10.4961, change in cost 0.000716209\n",
      "step 11610, training accuracy 0.949495, cost 10.4954, change in cost 0.000715256\n",
      "step 11620, training accuracy 0.949495, cost 10.4947, change in cost 0.000714302\n",
      "step 11630, training accuracy 0.949495, cost 10.494, change in cost 0.000714302\n",
      "step 11640, training accuracy 0.949495, cost 10.4933, change in cost 0.000711441\n",
      "step 11650, training accuracy 0.949495, cost 10.4926, change in cost 0.000713348\n",
      "step 11660, training accuracy 0.949495, cost 10.4919, change in cost 0.000711441\n",
      "step 11670, training accuracy 0.949495, cost 10.4911, change in cost 0.000710487\n",
      "step 11680, training accuracy 0.949495, cost 10.4904, change in cost 0.000710487\n",
      "step 11690, training accuracy 0.949495, cost 10.4897, change in cost 0.000706673\n",
      "step 11700, training accuracy 0.949495, cost 10.489, change in cost 0.000709534\n",
      "step 11710, training accuracy 0.949495, cost 10.4883, change in cost 0.000706673\n",
      "step 11720, training accuracy 0.949495, cost 10.4876, change in cost 0.000705719\n",
      "step 11730, training accuracy 0.949495, cost 10.4869, change in cost 0.000706673\n",
      "step 11740, training accuracy 0.949495, cost 10.4862, change in cost 0.000704765\n",
      "step 11750, training accuracy 0.949495, cost 10.4855, change in cost 0.000704765\n",
      "step 11760, training accuracy 0.949495, cost 10.4848, change in cost 0.000702858\n",
      "step 11770, training accuracy 0.949495, cost 10.4841, change in cost 0.000701904\n",
      "step 11780, training accuracy 0.949495, cost 10.4834, change in cost 0.000701904\n",
      "step 11790, training accuracy 0.949495, cost 10.4827, change in cost 0.000700951\n",
      "step 11800, training accuracy 0.949495, cost 10.482, change in cost 0.000700951\n",
      "step 11810, training accuracy 0.949495, cost 10.4813, change in cost 0.000699043\n",
      "step 11820, training accuracy 0.949495, cost 10.4806, change in cost 0.00069809\n",
      "step 11830, training accuracy 0.949495, cost 10.4799, change in cost 0.00069809\n",
      "step 11840, training accuracy 0.949495, cost 10.4792, change in cost 0.000697136\n",
      "step 11850, training accuracy 0.949495, cost 10.4785, change in cost 0.000696182\n",
      "step 11860, training accuracy 0.949495, cost 10.4778, change in cost 0.000694275\n",
      "step 11870, training accuracy 0.949495, cost 10.4771, change in cost 0.000696182\n",
      "step 11880, training accuracy 0.949495, cost 10.4764, change in cost 0.000693321\n",
      "step 11890, training accuracy 0.949495, cost 10.4757, change in cost 0.000694275\n",
      "step 11900, training accuracy 0.949495, cost 10.475, change in cost 0.000693321\n",
      "step 11910, training accuracy 0.949495, cost 10.4743, change in cost 0.000689507\n",
      "step 11920, training accuracy 0.949495, cost 10.4736, change in cost 0.000691414\n",
      "step 11930, training accuracy 0.949495, cost 10.4729, change in cost 0.000689507\n",
      "step 11940, training accuracy 0.949495, cost 10.4723, change in cost 0.000689507\n",
      "step 11950, training accuracy 0.949495, cost 10.4716, change in cost 0.000689507\n",
      "step 11960, training accuracy 0.949495, cost 10.4709, change in cost 0.000686646\n",
      "step 11970, training accuracy 0.949495, cost 10.4702, change in cost 0.000687599\n",
      "step 11980, training accuracy 0.949495, cost 10.4695, change in cost 0.000686646\n",
      "step 11990, training accuracy 0.949495, cost 10.4688, change in cost 0.000685692\n",
      "step 12000, training accuracy 0.949495, cost 10.4681, change in cost 0.000683784\n",
      "step 12010, training accuracy 0.949495, cost 10.4675, change in cost 0.000683784\n",
      "step 12020, training accuracy 0.949495, cost 10.4668, change in cost 0.000683784\n",
      "step 12030, training accuracy 0.949495, cost 10.4661, change in cost 0.000681877\n",
      "step 12040, training accuracy 0.949495, cost 10.4654, change in cost 0.000682831\n",
      "step 12050, training accuracy 0.949495, cost 10.4647, change in cost 0.000681877\n",
      "step 12060, training accuracy 0.949495, cost 10.464, change in cost 0.00067997\n",
      "step 12070, training accuracy 0.949495, cost 10.4634, change in cost 0.000679016\n",
      "step 12080, training accuracy 0.949495, cost 10.4627, change in cost 0.000679016\n",
      "step 12090, training accuracy 0.949495, cost 10.462, change in cost 0.000679016\n",
      "step 12100, training accuracy 0.949495, cost 10.4613, change in cost 0.000676155\n",
      "step 12110, training accuracy 0.949495, cost 10.4607, change in cost 0.000676155\n",
      "step 12120, training accuracy 0.949495, cost 10.46, change in cost 0.000677109\n",
      "step 12130, training accuracy 0.949495, cost 10.4593, change in cost 0.000673294\n",
      "step 12140, training accuracy 0.949495, cost 10.4586, change in cost 0.000674248\n",
      "step 12150, training accuracy 0.949495, cost 10.458, change in cost 0.000673294\n",
      "step 12160, training accuracy 0.949495, cost 10.4573, change in cost 0.000673294\n",
      "step 12170, training accuracy 0.949495, cost 10.4566, change in cost 0.000673294\n",
      "step 12180, training accuracy 0.949495, cost 10.4559, change in cost 0.000670433\n",
      "step 12190, training accuracy 0.949495, cost 10.4553, change in cost 0.000670433\n",
      "step 12200, training accuracy 0.949495, cost 10.4546, change in cost 0.00067234\n",
      "step 12210, training accuracy 0.949495, cost 10.4539, change in cost 0.000667572\n",
      "step 12220, training accuracy 0.949495, cost 10.4533, change in cost 0.000667572\n",
      "step 12230, training accuracy 0.949495, cost 10.4526, change in cost 0.000668526\n",
      "step 12240, training accuracy 0.949495, cost 10.4519, change in cost 0.000666618\n",
      "step 12250, training accuracy 0.949495, cost 10.4513, change in cost 0.000666618\n",
      "step 12260, training accuracy 0.949495, cost 10.4506, change in cost 0.000666618\n",
      "step 12270, training accuracy 0.949495, cost 10.4499, change in cost 0.000664711\n",
      "step 12280, training accuracy 0.949495, cost 10.4493, change in cost 0.000663757\n",
      "step 12290, training accuracy 0.949495, cost 10.4486, change in cost 0.000663757\n",
      "step 12300, training accuracy 0.949495, cost 10.4479, change in cost 0.00066185\n",
      "step 12310, training accuracy 0.949495, cost 10.4473, change in cost 0.000662804\n",
      "step 12320, training accuracy 0.949495, cost 10.4466, change in cost 0.000662804\n",
      "step 12330, training accuracy 0.949495, cost 10.446, change in cost 0.000658989\n",
      "step 12340, training accuracy 0.949495, cost 10.4453, change in cost 0.000660896\n",
      "step 12350, training accuracy 0.949495, cost 10.4446, change in cost 0.000658989\n",
      "step 12360, training accuracy 0.949495, cost 10.444, change in cost 0.000658989\n",
      "step 12370, training accuracy 0.949495, cost 10.4433, change in cost 0.000657082\n",
      "step 12380, training accuracy 0.949495, cost 10.4427, change in cost 0.000658035\n",
      "step 12390, training accuracy 0.949495, cost 10.442, change in cost 0.000656128\n",
      "step 12400, training accuracy 0.949495, cost 10.4413, change in cost 0.000656128\n",
      "step 12410, training accuracy 0.949495, cost 10.4407, change in cost 0.000655174\n",
      "step 12420, training accuracy 0.949495, cost 10.44, change in cost 0.000655174\n",
      "step 12430, training accuracy 0.949495, cost 10.4394, change in cost 0.000652313\n",
      "step 12440, training accuracy 0.949495, cost 10.4387, change in cost 0.000652313\n",
      "step 12450, training accuracy 0.949495, cost 10.4381, change in cost 0.000653267\n",
      "step 12460, training accuracy 0.949495, cost 10.4374, change in cost 0.00065136\n",
      "step 12470, training accuracy 0.949495, cost 10.4368, change in cost 0.00065136\n",
      "step 12480, training accuracy 0.949495, cost 10.4361, change in cost 0.000650406\n",
      "step 12490, training accuracy 0.949495, cost 10.4355, change in cost 0.000649452\n",
      "step 12500, training accuracy 0.949495, cost 10.4348, change in cost 0.000648499\n",
      "step 12510, training accuracy 0.949495, cost 10.4342, change in cost 0.000649452\n",
      "step 12520, training accuracy 0.949495, cost 10.4335, change in cost 0.000646591\n",
      "step 12530, training accuracy 0.949495, cost 10.4329, change in cost 0.000646591\n",
      "step 12540, training accuracy 0.949495, cost 10.4322, change in cost 0.000646591\n",
      "step 12550, training accuracy 0.949495, cost 10.4316, change in cost 0.000645638\n",
      "step 12560, training accuracy 0.949495, cost 10.431, change in cost 0.00064373\n",
      "step 12570, training accuracy 0.949495, cost 10.4303, change in cost 0.000644684\n",
      "step 12580, training accuracy 0.949495, cost 10.4297, change in cost 0.000642776\n",
      "step 12590, training accuracy 0.949495, cost 10.429, change in cost 0.00064373\n",
      "step 12600, training accuracy 0.949495, cost 10.4284, change in cost 0.000641823\n",
      "step 12610, training accuracy 0.949495, cost 10.4277, change in cost 0.000641823\n",
      "step 12620, training accuracy 0.949495, cost 10.4271, change in cost 0.000639915\n",
      "step 12630, training accuracy 0.949495, cost 10.4265, change in cost 0.000640869\n",
      "step 12640, training accuracy 0.949495, cost 10.4258, change in cost 0.000638962\n",
      "step 12650, training accuracy 0.949495, cost 10.4252, change in cost 0.000638962\n",
      "step 12660, training accuracy 0.949495, cost 10.4245, change in cost 0.000638008\n",
      "step 12670, training accuracy 0.949495, cost 10.4239, change in cost 0.000638008\n",
      "step 12680, training accuracy 0.949495, cost 10.4233, change in cost 0.000638008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12690, training accuracy 0.949495, cost 10.4226, change in cost 0.00063324\n",
      "step 12700, training accuracy 0.949495, cost 10.422, change in cost 0.000638008\n",
      "step 12710, training accuracy 0.949495, cost 10.4214, change in cost 0.000632286\n",
      "step 12720, training accuracy 0.949495, cost 10.4207, change in cost 0.000634193\n",
      "step 12730, training accuracy 0.949495, cost 10.4201, change in cost 0.000636101\n",
      "step 12740, training accuracy 0.949495, cost 10.4195, change in cost 0.000631332\n",
      "step 12750, training accuracy 0.949495, cost 10.4188, change in cost 0.000632286\n",
      "step 12760, training accuracy 0.949495, cost 10.4182, change in cost 0.000631332\n",
      "step 12770, training accuracy 0.949495, cost 10.4176, change in cost 0.000631332\n",
      "step 12780, training accuracy 0.949495, cost 10.4169, change in cost 0.000629425\n",
      "step 12790, training accuracy 0.949495, cost 10.4163, change in cost 0.000630379\n",
      "step 12800, training accuracy 0.949495, cost 10.4157, change in cost 0.000628471\n",
      "step 12810, training accuracy 0.949495, cost 10.415, change in cost 0.000629425\n",
      "step 12820, training accuracy 0.949495, cost 10.4144, change in cost 0.000626564\n",
      "step 12830, training accuracy 0.949495, cost 10.4138, change in cost 0.000626564\n",
      "step 12840, training accuracy 0.949495, cost 10.4132, change in cost 0.000626564\n",
      "step 12850, training accuracy 0.949495, cost 10.4125, change in cost 0.000627518\n",
      "step 12860, training accuracy 0.949495, cost 10.4119, change in cost 0.000624657\n",
      "step 12870, training accuracy 0.949495, cost 10.4113, change in cost 0.000624657\n",
      "step 12880, training accuracy 0.949495, cost 10.4107, change in cost 0.000623703\n",
      "step 12890, training accuracy 0.949495, cost 10.41, change in cost 0.000623703\n",
      "step 12900, training accuracy 0.949495, cost 10.4094, change in cost 0.000620842\n",
      "step 12910, training accuracy 0.949495, cost 10.4088, change in cost 0.000621796\n",
      "step 12920, training accuracy 0.949495, cost 10.4082, change in cost 0.000622749\n",
      "step 12930, training accuracy 0.949495, cost 10.4076, change in cost 0.000619888\n",
      "step 12940, training accuracy 0.949495, cost 10.4069, change in cost 0.000619888\n",
      "step 12950, training accuracy 0.949495, cost 10.4063, change in cost 0.000619888\n",
      "step 12960, training accuracy 0.949495, cost 10.4057, change in cost 0.000617981\n",
      "step 12970, training accuracy 0.949495, cost 10.4051, change in cost 0.000617981\n",
      "step 12980, training accuracy 0.949495, cost 10.4045, change in cost 0.000617981\n",
      "step 12990, training accuracy 0.949495, cost 10.4038, change in cost 0.000617027\n",
      "step 13000, training accuracy 0.949495, cost 10.4032, change in cost 0.000616074\n",
      "step 13010, training accuracy 0.949495, cost 10.4026, change in cost 0.00061512\n",
      "step 13020, training accuracy 0.949495, cost 10.402, change in cost 0.00061512\n",
      "step 13030, training accuracy 0.949495, cost 10.4014, change in cost 0.000614166\n",
      "step 13040, training accuracy 0.949495, cost 10.4008, change in cost 0.00061512\n",
      "step 13050, training accuracy 0.949495, cost 10.4002, change in cost 0.000610352\n",
      "step 13060, training accuracy 0.949495, cost 10.3995, change in cost 0.00061512\n",
      "step 13070, training accuracy 0.949495, cost 10.3989, change in cost 0.000611305\n",
      "step 13080, training accuracy 0.949495, cost 10.3983, change in cost 0.000610352\n",
      "step 13090, training accuracy 0.949495, cost 10.3977, change in cost 0.000611305\n",
      "step 13100, training accuracy 0.949495, cost 10.3971, change in cost 0.000610352\n",
      "step 13110, training accuracy 0.949495, cost 10.3965, change in cost 0.000609398\n",
      "step 13120, training accuracy 0.949495, cost 10.3959, change in cost 0.000609398\n",
      "step 13130, training accuracy 0.949495, cost 10.3953, change in cost 0.000606537\n",
      "step 13140, training accuracy 0.949495, cost 10.3947, change in cost 0.000607491\n",
      "step 13150, training accuracy 0.949495, cost 10.3941, change in cost 0.000607491\n",
      "step 13160, training accuracy 0.949495, cost 10.3935, change in cost 0.000606537\n",
      "step 13170, training accuracy 0.949495, cost 10.3928, change in cost 0.000606537\n",
      "step 13180, training accuracy 0.949495, cost 10.3922, change in cost 0.000603676\n",
      "step 13190, training accuracy 0.949495, cost 10.3916, change in cost 0.000606537\n",
      "step 13200, training accuracy 0.949495, cost 10.391, change in cost 0.000601768\n",
      "step 13210, training accuracy 0.949495, cost 10.3904, change in cost 0.000603676\n",
      "step 13220, training accuracy 0.949495, cost 10.3898, change in cost 0.00060463\n",
      "step 13230, training accuracy 0.949495, cost 10.3892, change in cost 0.000599861\n",
      "step 13240, training accuracy 0.949495, cost 10.3886, change in cost 0.000602722\n",
      "step 13250, training accuracy 0.949495, cost 10.388, change in cost 0.000601768\n",
      "step 13260, training accuracy 0.949495, cost 10.3874, change in cost 0.000599861\n",
      "step 13270, training accuracy 0.949495, cost 10.3868, change in cost 0.000600815\n",
      "step 13280, training accuracy 0.949495, cost 10.3862, change in cost 0.000598907\n",
      "step 13290, training accuracy 0.949495, cost 10.3856, change in cost 0.000597\n",
      "step 13300, training accuracy 0.949495, cost 10.385, change in cost 0.000598907\n",
      "step 13310, training accuracy 0.949495, cost 10.3844, change in cost 0.000597\n",
      "step 13320, training accuracy 0.949495, cost 10.3838, change in cost 0.000597954\n",
      "step 13330, training accuracy 0.949495, cost 10.3832, change in cost 0.000595093\n",
      "step 13340, training accuracy 0.949495, cost 10.3826, change in cost 0.000596046\n",
      "step 13350, training accuracy 0.949495, cost 10.382, change in cost 0.000595093\n",
      "step 13360, training accuracy 0.949495, cost 10.3815, change in cost 0.000594139\n",
      "step 13370, training accuracy 0.949495, cost 10.3809, change in cost 0.000593185\n",
      "step 13380, training accuracy 0.949495, cost 10.3803, change in cost 0.000594139\n",
      "step 13390, training accuracy 0.949495, cost 10.3797, change in cost 0.000592232\n",
      "step 13400, training accuracy 0.949495, cost 10.3791, change in cost 0.000593185\n",
      "step 13410, training accuracy 0.949495, cost 10.3785, change in cost 0.000591278\n",
      "step 13420, training accuracy 0.949495, cost 10.3779, change in cost 0.000590324\n",
      "step 13430, training accuracy 0.949495, cost 10.3773, change in cost 0.000591278\n",
      "step 13440, training accuracy 0.949495, cost 10.3767, change in cost 0.000589371\n",
      "step 13450, training accuracy 0.949495, cost 10.3761, change in cost 0.000588417\n",
      "step 13460, training accuracy 0.949495, cost 10.3755, change in cost 0.000589371\n",
      "step 13470, training accuracy 0.949495, cost 10.375, change in cost 0.000588417\n",
      "step 13480, training accuracy 0.949495, cost 10.3744, change in cost 0.00058651\n",
      "step 13490, training accuracy 0.949495, cost 10.3738, change in cost 0.000587463\n",
      "step 13500, training accuracy 0.949495, cost 10.3732, change in cost 0.000585556\n",
      "step 13510, training accuracy 0.949495, cost 10.3726, change in cost 0.00058651\n",
      "step 13520, training accuracy 0.949495, cost 10.372, change in cost 0.000585556\n",
      "step 13530, training accuracy 0.949495, cost 10.3714, change in cost 0.000584602\n",
      "step 13540, training accuracy 0.949495, cost 10.3709, change in cost 0.000583649\n",
      "step 13550, training accuracy 0.949495, cost 10.3703, change in cost 0.000583649\n",
      "step 13560, training accuracy 0.949495, cost 10.3697, change in cost 0.000582695\n",
      "step 13570, training accuracy 0.949495, cost 10.3691, change in cost 0.000581741\n",
      "step 13580, training accuracy 0.949495, cost 10.3685, change in cost 0.000582695\n",
      "step 13590, training accuracy 0.949495, cost 10.3679, change in cost 0.000581741\n",
      "step 13600, training accuracy 0.949495, cost 10.3674, change in cost 0.000579834\n",
      "step 13610, training accuracy 0.949495, cost 10.3668, change in cost 0.000580788\n",
      "step 13620, training accuracy 0.949495, cost 10.3662, change in cost 0.000579834\n",
      "step 13630, training accuracy 0.949495, cost 10.3656, change in cost 0.00057888\n",
      "step 13640, training accuracy 0.949495, cost 10.365, change in cost 0.00057888\n",
      "step 13650, training accuracy 0.949495, cost 10.3645, change in cost 0.000576973\n",
      "step 13660, training accuracy 0.949495, cost 10.3639, change in cost 0.000576973\n",
      "step 13670, training accuracy 0.949495, cost 10.3633, change in cost 0.000577927\n",
      "step 13680, training accuracy 0.949495, cost 10.3627, change in cost 0.000575066\n",
      "step 13690, training accuracy 0.949495, cost 10.3622, change in cost 0.000576019\n",
      "step 13700, training accuracy 0.949495, cost 10.3616, change in cost 0.000576019\n",
      "step 13710, training accuracy 0.949495, cost 10.361, change in cost 0.000575066\n",
      "step 13720, training accuracy 0.949495, cost 10.3604, change in cost 0.000572205\n",
      "step 13730, training accuracy 0.949495, cost 10.3599, change in cost 0.000573158\n",
      "step 13740, training accuracy 0.949495, cost 10.3593, change in cost 0.000573158\n",
      "step 13750, training accuracy 0.949495, cost 10.3587, change in cost 0.000573158\n",
      "step 13760, training accuracy 0.949495, cost 10.3581, change in cost 0.000572205\n",
      "step 13770, training accuracy 0.949495, cost 10.3576, change in cost 0.000571251\n",
      "step 13780, training accuracy 0.949495, cost 10.357, change in cost 0.000570297\n",
      "step 13790, training accuracy 0.949495, cost 10.3564, change in cost 0.000570297\n",
      "step 13800, training accuracy 0.949495, cost 10.3559, change in cost 0.000569344\n",
      "step 13810, training accuracy 0.949495, cost 10.3553, change in cost 0.000570297\n",
      "step 13820, training accuracy 0.949495, cost 10.3547, change in cost 0.000567436\n",
      "step 13830, training accuracy 0.949495, cost 10.3542, change in cost 0.000567436\n",
      "step 13840, training accuracy 0.949495, cost 10.3536, change in cost 0.00056839\n",
      "step 13850, training accuracy 0.949495, cost 10.353, change in cost 0.000567436\n",
      "step 13860, training accuracy 0.949495, cost 10.3525, change in cost 0.000565529\n",
      "step 13870, training accuracy 0.949495, cost 10.3519, change in cost 0.000564575\n",
      "step 13880, training accuracy 0.949495, cost 10.3513, change in cost 0.000566483\n",
      "step 13890, training accuracy 0.949495, cost 10.3508, change in cost 0.000564575\n",
      "step 13900, training accuracy 0.949495, cost 10.3502, change in cost 0.000563622\n",
      "step 13910, training accuracy 0.949495, cost 10.3496, change in cost 0.000563622\n",
      "step 13920, training accuracy 0.949495, cost 10.3491, change in cost 0.000564575\n",
      "step 13930, training accuracy 0.949495, cost 10.3485, change in cost 0.000561714\n",
      "step 13940, training accuracy 0.949495, cost 10.3479, change in cost 0.000562668\n",
      "step 13950, training accuracy 0.949495, cost 10.3474, change in cost 0.000563622\n",
      "step 13960, training accuracy 0.949495, cost 10.3468, change in cost 0.000558853\n",
      "step 13970, training accuracy 0.949495, cost 10.3463, change in cost 0.00056076\n",
      "step 13980, training accuracy 0.949495, cost 10.3457, change in cost 0.000558853\n",
      "step 13990, training accuracy 0.949495, cost 10.3451, change in cost 0.000559807\n",
      "step 14000, training accuracy 0.949495, cost 10.3446, change in cost 0.000559807\n",
      "step 14010, training accuracy 0.949495, cost 10.344, change in cost 0.000557899\n",
      "step 14020, training accuracy 0.949495, cost 10.3435, change in cost 0.000556946\n",
      "step 14030, training accuracy 0.949495, cost 10.3429, change in cost 0.000558853\n",
      "step 14040, training accuracy 0.949495, cost 10.3423, change in cost 0.000557899\n",
      "step 14050, training accuracy 0.949495, cost 10.3418, change in cost 0.000555038\n",
      "step 14060, training accuracy 0.949495, cost 10.3412, change in cost 0.000555992\n",
      "step 14070, training accuracy 0.949495, cost 10.3407, change in cost 0.000554085\n",
      "step 14080, training accuracy 0.949495, cost 10.3401, change in cost 0.000555038\n",
      "step 14090, training accuracy 0.949495, cost 10.3396, change in cost 0.000554085\n",
      "step 14100, training accuracy 0.949495, cost 10.339, change in cost 0.000555038\n",
      "step 14110, training accuracy 0.949495, cost 10.3385, change in cost 0.000553131\n",
      "step 14120, training accuracy 0.949495, cost 10.3379, change in cost 0.000552177\n",
      "step 14130, training accuracy 0.949495, cost 10.3374, change in cost 0.000552177\n",
      "step 14140, training accuracy 0.949495, cost 10.3368, change in cost 0.000552177\n",
      "step 14150, training accuracy 0.949495, cost 10.3363, change in cost 0.000551224\n",
      "step 14160, training accuracy 0.949495, cost 10.3357, change in cost 0.000551224\n",
      "step 14170, training accuracy 0.949495, cost 10.3352, change in cost 0.000547409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14180, training accuracy 0.949495, cost 10.3346, change in cost 0.000551224\n",
      "step 14190, training accuracy 0.949495, cost 10.3341, change in cost 0.000548363\n",
      "step 14200, training accuracy 0.949495, cost 10.3335, change in cost 0.000549316\n",
      "step 14210, training accuracy 0.949495, cost 10.333, change in cost 0.000548363\n",
      "step 14220, training accuracy 0.949495, cost 10.3324, change in cost 0.000547409\n",
      "step 14230, training accuracy 0.949495, cost 10.3319, change in cost 0.000547409\n",
      "step 14240, training accuracy 0.949495, cost 10.3313, change in cost 0.000546455\n",
      "step 14250, training accuracy 0.949495, cost 10.3308, change in cost 0.000544548\n",
      "step 14260, training accuracy 0.949495, cost 10.3302, change in cost 0.000546455\n",
      "step 14270, training accuracy 0.949495, cost 10.3297, change in cost 0.000546455\n",
      "step 14280, training accuracy 0.949495, cost 10.3291, change in cost 0.000543594\n",
      "step 14290, training accuracy 0.949495, cost 10.3286, change in cost 0.000543594\n",
      "step 14300, training accuracy 0.949495, cost 10.3281, change in cost 0.000544548\n",
      "step 14310, training accuracy 0.949495, cost 10.3275, change in cost 0.000542641\n",
      "step 14320, training accuracy 0.949495, cost 10.327, change in cost 0.000541687\n",
      "step 14330, training accuracy 0.949495, cost 10.3264, change in cost 0.000541687\n",
      "step 14340, training accuracy 0.949495, cost 10.3259, change in cost 0.000541687\n",
      "step 14350, training accuracy 0.949495, cost 10.3253, change in cost 0.000541687\n",
      "step 14360, training accuracy 0.949495, cost 10.3248, change in cost 0.000540733\n",
      "step 14370, training accuracy 0.949495, cost 10.3243, change in cost 0.00053978\n",
      "step 14380, training accuracy 0.949495, cost 10.3237, change in cost 0.00053978\n",
      "step 14390, training accuracy 0.949495, cost 10.3232, change in cost 0.00053978\n",
      "step 14400, training accuracy 0.949495, cost 10.3226, change in cost 0.000538826\n",
      "step 14410, training accuracy 0.949495, cost 10.3221, change in cost 0.000537872\n",
      "step 14420, training accuracy 0.949495, cost 10.3216, change in cost 0.000536919\n",
      "step 14430, training accuracy 0.949495, cost 10.321, change in cost 0.000536919\n",
      "step 14440, training accuracy 0.949495, cost 10.3205, change in cost 0.000536919\n",
      "step 14450, training accuracy 0.949495, cost 10.32, change in cost 0.000534058\n",
      "step 14460, training accuracy 0.949495, cost 10.3194, change in cost 0.000537872\n",
      "step 14470, training accuracy 0.949495, cost 10.3189, change in cost 0.000534058\n",
      "step 14480, training accuracy 0.949495, cost 10.3184, change in cost 0.000535011\n",
      "step 14490, training accuracy 0.949495, cost 10.3178, change in cost 0.000534058\n",
      "step 14500, training accuracy 0.949495, cost 10.3173, change in cost 0.000534058\n",
      "step 14510, training accuracy 0.949495, cost 10.3168, change in cost 0.000534058\n",
      "step 14520, training accuracy 0.949495, cost 10.3162, change in cost 0.000530243\n",
      "step 14530, training accuracy 0.949495, cost 10.3157, change in cost 0.000535011\n",
      "step 14540, training accuracy 0.949495, cost 10.3152, change in cost 0.000531197\n",
      "step 14550, training accuracy 0.949495, cost 10.3146, change in cost 0.00053215\n",
      "step 14560, training accuracy 0.949495, cost 10.3141, change in cost 0.000529289\n",
      "step 14570, training accuracy 0.949495, cost 10.3136, change in cost 0.000530243\n",
      "step 14580, training accuracy 0.949495, cost 10.313, change in cost 0.000529289\n",
      "step 14590, training accuracy 0.949495, cost 10.3125, change in cost 0.000529289\n",
      "step 14600, training accuracy 0.949495, cost 10.312, change in cost 0.000529289\n",
      "step 14610, training accuracy 0.949495, cost 10.3114, change in cost 0.000528336\n",
      "step 14620, training accuracy 0.949495, cost 10.3109, change in cost 0.000528336\n",
      "step 14630, training accuracy 0.949495, cost 10.3104, change in cost 0.000526428\n",
      "step 14640, training accuracy 0.949495, cost 10.3099, change in cost 0.000527382\n",
      "step 14650, training accuracy 0.949495, cost 10.3093, change in cost 0.000527382\n",
      "step 14660, training accuracy 0.949495, cost 10.3088, change in cost 0.000525475\n",
      "step 14670, training accuracy 0.949495, cost 10.3083, change in cost 0.000525475\n",
      "step 14680, training accuracy 0.949495, cost 10.3078, change in cost 0.000524521\n",
      "step 14690, training accuracy 0.949495, cost 10.3072, change in cost 0.000524521\n",
      "step 14700, training accuracy 0.949495, cost 10.3067, change in cost 0.000524521\n",
      "step 14710, training accuracy 0.949495, cost 10.3062, change in cost 0.000522614\n",
      "step 14720, training accuracy 0.949495, cost 10.3057, change in cost 0.000524521\n",
      "step 14730, training accuracy 0.949495, cost 10.3051, change in cost 0.00052166\n",
      "step 14740, training accuracy 0.949495, cost 10.3046, change in cost 0.000522614\n",
      "step 14750, training accuracy 0.949495, cost 10.3041, change in cost 0.00052166\n",
      "step 14760, training accuracy 0.949495, cost 10.3036, change in cost 0.000522614\n",
      "step 14770, training accuracy 0.949495, cost 10.3031, change in cost 0.000518799\n",
      "step 14780, training accuracy 0.949495, cost 10.3025, change in cost 0.000520706\n",
      "step 14790, training accuracy 0.949495, cost 10.302, change in cost 0.000520706\n",
      "step 14800, training accuracy 0.949495, cost 10.3015, change in cost 0.000519753\n",
      "step 14810, training accuracy 0.949495, cost 10.301, change in cost 0.000518799\n",
      "step 14820, training accuracy 0.949495, cost 10.3005, change in cost 0.000518799\n",
      "step 14830, training accuracy 0.949495, cost 10.2999, change in cost 0.000516891\n",
      "step 14840, training accuracy 0.949495, cost 10.2994, change in cost 0.000516891\n",
      "step 14850, training accuracy 0.949495, cost 10.2989, change in cost 0.000516891\n",
      "step 14860, training accuracy 0.949495, cost 10.2984, change in cost 0.000517845\n",
      "step 14870, training accuracy 0.949495, cost 10.2979, change in cost 0.000515938\n",
      "step 14880, training accuracy 0.949495, cost 10.2974, change in cost 0.000515938\n",
      "step 14890, training accuracy 0.949495, cost 10.2968, change in cost 0.000516891\n",
      "step 14900, training accuracy 0.949495, cost 10.2963, change in cost 0.00051403\n",
      "step 14910, training accuracy 0.949495, cost 10.2958, change in cost 0.00051403\n",
      "step 14920, training accuracy 0.949495, cost 10.2953, change in cost 0.00051403\n",
      "step 14930, training accuracy 0.949495, cost 10.2948, change in cost 0.000513077\n",
      "step 14940, training accuracy 0.949495, cost 10.2943, change in cost 0.00051403\n",
      "step 14950, training accuracy 0.949495, cost 10.2938, change in cost 0.000511169\n",
      "step 14960, training accuracy 0.949495, cost 10.2933, change in cost 0.000512123\n",
      "step 14970, training accuracy 0.949495, cost 10.2927, change in cost 0.000512123\n",
      "step 14980, training accuracy 0.949495, cost 10.2922, change in cost 0.000510216\n",
      "step 14990, training accuracy 0.949495, cost 10.2917, change in cost 0.000511169\n",
      "step 15000, training accuracy 0.949495, cost 10.2912, change in cost 0.000510216\n",
      "step 15010, training accuracy 0.949495, cost 10.2907, change in cost 0.000511169\n",
      "step 15020, training accuracy 0.949495, cost 10.2902, change in cost 0.000511169\n",
      "step 15030, training accuracy 0.949495, cost 10.2897, change in cost 0.000507355\n",
      "step 15040, training accuracy 0.949495, cost 10.2892, change in cost 0.000508308\n",
      "step 15050, training accuracy 0.949495, cost 10.2887, change in cost 0.000507355\n",
      "step 15060, training accuracy 0.949495, cost 10.2882, change in cost 0.000507355\n",
      "step 15070, training accuracy 0.949495, cost 10.2876, change in cost 0.000507355\n",
      "step 15080, training accuracy 0.949495, cost 10.2871, change in cost 0.000507355\n",
      "step 15090, training accuracy 0.949495, cost 10.2866, change in cost 0.000505447\n",
      "step 15100, training accuracy 0.949495, cost 10.2861, change in cost 0.000507355\n",
      "step 15110, training accuracy 0.949495, cost 10.2856, change in cost 0.000505447\n",
      "step 15120, training accuracy 0.949495, cost 10.2851, change in cost 0.00050354\n",
      "step 15130, training accuracy 0.949495, cost 10.2846, change in cost 0.000506401\n",
      "step 15140, training accuracy 0.949495, cost 10.2841, change in cost 0.00050354\n",
      "step 15150, training accuracy 0.949495, cost 10.2836, change in cost 0.000502586\n",
      "step 15160, training accuracy 0.949495, cost 10.2831, change in cost 0.000504494\n",
      "step 15170, training accuracy 0.949495, cost 10.2826, change in cost 0.000502586\n",
      "step 15180, training accuracy 0.949495, cost 10.2821, change in cost 0.000501633\n",
      "step 15190, training accuracy 0.949495, cost 10.2816, change in cost 0.000501633\n",
      "step 15200, training accuracy 0.949495, cost 10.2811, change in cost 0.000501633\n",
      "step 15210, training accuracy 0.949495, cost 10.2806, change in cost 0.000501633\n",
      "step 15220, training accuracy 0.949495, cost 10.2801, change in cost 0.000499725\n",
      "step 15230, training accuracy 0.949495, cost 10.2796, change in cost 0.000499725\n",
      "step 15240, training accuracy 0.949495, cost 10.2791, change in cost 0.000499725\n",
      "step 15250, training accuracy 0.949495, cost 10.2786, change in cost 0.000499725\n",
      "step 15260, training accuracy 0.949495, cost 10.2781, change in cost 0.000498772\n",
      "step 15270, training accuracy 0.949495, cost 10.2776, change in cost 0.000498772\n",
      "step 15280, training accuracy 0.949495, cost 10.2771, change in cost 0.000497818\n",
      "step 15290, training accuracy 0.949495, cost 10.2766, change in cost 0.000495911\n",
      "step 15300, training accuracy 0.949495, cost 10.2761, change in cost 0.000497818\n",
      "step 15310, training accuracy 0.949495, cost 10.2756, change in cost 0.000495911\n",
      "step 15320, training accuracy 0.949495, cost 10.2751, change in cost 0.000498772\n",
      "step 15330, training accuracy 0.949495, cost 10.2746, change in cost 0.000494957\n",
      "step 15340, training accuracy 0.949495, cost 10.2741, change in cost 0.000494957\n",
      "step 15350, training accuracy 0.949495, cost 10.2736, change in cost 0.000495911\n",
      "step 15360, training accuracy 0.949495, cost 10.2731, change in cost 0.000494003\n",
      "step 15370, training accuracy 0.949495, cost 10.2726, change in cost 0.000494003\n",
      "step 15380, training accuracy 0.949495, cost 10.2721, change in cost 0.000494003\n",
      "step 15390, training accuracy 0.949495, cost 10.2716, change in cost 0.00049305\n",
      "step 15400, training accuracy 0.949495, cost 10.2712, change in cost 0.000492096\n",
      "step 15410, training accuracy 0.949495, cost 10.2707, change in cost 0.00049305\n",
      "step 15420, training accuracy 0.949495, cost 10.2702, change in cost 0.00049305\n",
      "step 15430, training accuracy 0.949495, cost 10.2697, change in cost 0.000489235\n",
      "step 15440, training accuracy 0.949495, cost 10.2692, change in cost 0.000492096\n",
      "step 15450, training accuracy 0.949495, cost 10.2687, change in cost 0.000490189\n",
      "step 15460, training accuracy 0.949495, cost 10.2682, change in cost 0.000491142\n",
      "step 15470, training accuracy 0.949495, cost 10.2677, change in cost 0.000490189\n",
      "step 15480, training accuracy 0.949495, cost 10.2672, change in cost 0.000489235\n",
      "step 15490, training accuracy 0.949495, cost 10.2667, change in cost 0.000489235\n",
      "step 15500, training accuracy 0.949495, cost 10.2663, change in cost 0.000488281\n",
      "step 15510, training accuracy 0.949495, cost 10.2658, change in cost 0.000487328\n",
      "step 15520, training accuracy 0.949495, cost 10.2653, change in cost 0.000487328\n",
      "step 15530, training accuracy 0.949495, cost 10.2648, change in cost 0.000489235\n",
      "step 15540, training accuracy 0.949495, cost 10.2643, change in cost 0.000486374\n",
      "step 15550, training accuracy 0.949495, cost 10.2638, change in cost 0.000486374\n",
      "step 15560, training accuracy 0.949495, cost 10.2633, change in cost 0.000486374\n",
      "step 15570, training accuracy 0.949495, cost 10.2628, change in cost 0.00048542\n",
      "step 15580, training accuracy 0.949495, cost 10.2624, change in cost 0.000486374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15590, training accuracy 0.949495, cost 10.2619, change in cost 0.000483513\n",
      "step 15600, training accuracy 0.949495, cost 10.2614, change in cost 0.00048542\n",
      "step 15610, training accuracy 0.949495, cost 10.2609, change in cost 0.000484467\n",
      "step 15620, training accuracy 0.949495, cost 10.2604, change in cost 0.000482559\n",
      "step 15630, training accuracy 0.949495, cost 10.2599, change in cost 0.000483513\n",
      "step 15640, training accuracy 0.949495, cost 10.2595, change in cost 0.000483513\n",
      "step 15650, training accuracy 0.949495, cost 10.259, change in cost 0.000480652\n",
      "step 15660, training accuracy 0.949495, cost 10.2585, change in cost 0.000481606\n",
      "step 15670, training accuracy 0.949495, cost 10.258, change in cost 0.000483513\n",
      "step 15680, training accuracy 0.949495, cost 10.2575, change in cost 0.000481606\n",
      "step 15690, training accuracy 0.949495, cost 10.257, change in cost 0.000479698\n",
      "step 15700, training accuracy 0.949495, cost 10.2566, change in cost 0.000480652\n",
      "step 15710, training accuracy 0.949495, cost 10.2561, change in cost 0.000480652\n",
      "step 15720, training accuracy 0.949495, cost 10.2556, change in cost 0.000477791\n",
      "step 15730, training accuracy 0.949495, cost 10.2551, change in cost 0.000479698\n",
      "step 15740, training accuracy 0.949495, cost 10.2546, change in cost 0.000478745\n",
      "step 15750, training accuracy 0.949495, cost 10.2542, change in cost 0.000478745\n",
      "step 15760, training accuracy 0.949495, cost 10.2537, change in cost 0.000478745\n",
      "step 15770, training accuracy 0.949495, cost 10.2532, change in cost 0.000476837\n",
      "step 15780, training accuracy 0.949495, cost 10.2527, change in cost 0.000476837\n",
      "step 15790, training accuracy 0.949495, cost 10.2523, change in cost 0.000476837\n",
      "step 15800, training accuracy 0.949495, cost 10.2518, change in cost 0.000477791\n",
      "step 15810, training accuracy 0.949495, cost 10.2513, change in cost 0.00047493\n",
      "step 15820, training accuracy 0.949495, cost 10.2508, change in cost 0.000475883\n",
      "step 15830, training accuracy 0.949495, cost 10.2504, change in cost 0.00047493\n",
      "step 15840, training accuracy 0.949495, cost 10.2499, change in cost 0.00047493\n",
      "step 15850, training accuracy 0.949495, cost 10.2494, change in cost 0.000473976\n",
      "step 15860, training accuracy 0.949495, cost 10.2489, change in cost 0.00047493\n",
      "step 15870, training accuracy 0.949495, cost 10.2485, change in cost 0.000470161\n",
      "step 15880, training accuracy 0.949495, cost 10.248, change in cost 0.000475883\n",
      "step 15890, training accuracy 0.949495, cost 10.2475, change in cost 0.000473022\n",
      "step 15900, training accuracy 0.949495, cost 10.247, change in cost 0.000471115\n",
      "step 15910, training accuracy 0.949495, cost 10.2466, change in cost 0.000473022\n",
      "step 15920, training accuracy 0.949495, cost 10.2461, change in cost 0.000473022\n",
      "step 15930, training accuracy 0.949495, cost 10.2456, change in cost 0.000470161\n",
      "step 15940, training accuracy 0.949495, cost 10.2452, change in cost 0.000470161\n",
      "step 15950, training accuracy 0.949495, cost 10.2447, change in cost 0.000471115\n",
      "step 15960, training accuracy 0.949495, cost 10.2442, change in cost 0.000470161\n",
      "step 15970, training accuracy 0.949495, cost 10.2437, change in cost 0.000468254\n",
      "step 15980, training accuracy 0.949495, cost 10.2433, change in cost 0.000470161\n",
      "step 15990, training accuracy 0.949495, cost 10.2428, change in cost 0.000468254\n",
      "step 16000, training accuracy 0.949495, cost 10.2423, change in cost 0.000469208\n",
      "step 16010, training accuracy 0.949495, cost 10.2419, change in cost 0.0004673\n",
      "step 16020, training accuracy 0.949495, cost 10.2414, change in cost 0.000468254\n",
      "step 16030, training accuracy 0.949495, cost 10.2409, change in cost 0.0004673\n",
      "step 16040, training accuracy 0.949495, cost 10.2405, change in cost 0.000466347\n",
      "step 16050, training accuracy 0.949495, cost 10.24, change in cost 0.000466347\n",
      "step 16060, training accuracy 0.949495, cost 10.2395, change in cost 0.000465393\n",
      "step 16070, training accuracy 0.949495, cost 10.2391, change in cost 0.000466347\n",
      "step 16080, training accuracy 0.949495, cost 10.2386, change in cost 0.000465393\n",
      "step 16090, training accuracy 0.949495, cost 10.2381, change in cost 0.000466347\n",
      "step 16100, training accuracy 0.949495, cost 10.2377, change in cost 0.000463486\n",
      "step 16110, training accuracy 0.949495, cost 10.2372, change in cost 0.000463486\n",
      "step 16120, training accuracy 0.949495, cost 10.2367, change in cost 0.000464439\n",
      "step 16130, training accuracy 0.949495, cost 10.2363, change in cost 0.000463486\n",
      "step 16140, training accuracy 0.949495, cost 10.2358, change in cost 0.000463486\n",
      "step 16150, training accuracy 0.949495, cost 10.2354, change in cost 0.000461578\n",
      "step 16160, training accuracy 0.949495, cost 10.2349, change in cost 0.000463486\n",
      "step 16170, training accuracy 0.949495, cost 10.2344, change in cost 0.000462532\n",
      "step 16180, training accuracy 0.949495, cost 10.234, change in cost 0.000459671\n",
      "step 16190, training accuracy 0.949495, cost 10.2335, change in cost 0.000461578\n",
      "step 16200, training accuracy 0.949495, cost 10.2331, change in cost 0.000460625\n",
      "step 16210, training accuracy 0.949495, cost 10.2326, change in cost 0.000461578\n",
      "step 16220, training accuracy 0.949495, cost 10.2321, change in cost 0.000458717\n",
      "step 16230, training accuracy 0.949495, cost 10.2317, change in cost 0.000460625\n",
      "step 16240, training accuracy 0.949495, cost 10.2312, change in cost 0.000460625\n",
      "step 16250, training accuracy 0.949495, cost 10.2308, change in cost 0.00045681\n",
      "step 16260, training accuracy 0.949495, cost 10.2303, change in cost 0.000459671\n",
      "step 16270, training accuracy 0.949495, cost 10.2298, change in cost 0.00045681\n",
      "step 16280, training accuracy 0.949495, cost 10.2294, change in cost 0.000459671\n",
      "step 16290, training accuracy 0.949495, cost 10.2289, change in cost 0.000454903\n",
      "step 16300, training accuracy 0.949495, cost 10.2285, change in cost 0.000459671\n",
      "step 16310, training accuracy 0.949495, cost 10.228, change in cost 0.000455856\n",
      "step 16320, training accuracy 0.949495, cost 10.2276, change in cost 0.000454903\n",
      "step 16330, training accuracy 0.949495, cost 10.2271, change in cost 0.00045681\n",
      "step 16340, training accuracy 0.949495, cost 10.2266, change in cost 0.000455856\n",
      "step 16350, training accuracy 0.949495, cost 10.2262, change in cost 0.000454903\n",
      "step 16360, training accuracy 0.959596, cost 10.2257, change in cost 0.000453949\n",
      "step 16370, training accuracy 0.959596, cost 10.2253, change in cost 0.000453949\n",
      "step 16380, training accuracy 0.959596, cost 10.2248, change in cost 0.000455856\n",
      "step 16390, training accuracy 0.959596, cost 10.2244, change in cost 0.000452995\n",
      "step 16400, training accuracy 0.959596, cost 10.2239, change in cost 0.000452995\n",
      "step 16410, training accuracy 0.959596, cost 10.2235, change in cost 0.000453949\n",
      "step 16420, training accuracy 0.959596, cost 10.223, change in cost 0.000451088\n",
      "step 16430, training accuracy 0.959596, cost 10.2226, change in cost 0.000452995\n",
      "step 16440, training accuracy 0.959596, cost 10.2221, change in cost 0.000452042\n",
      "step 16450, training accuracy 0.959596, cost 10.2217, change in cost 0.000452042\n",
      "step 16460, training accuracy 0.959596, cost 10.2212, change in cost 0.000450134\n",
      "step 16470, training accuracy 0.959596, cost 10.2208, change in cost 0.000452042\n",
      "step 16480, training accuracy 0.959596, cost 10.2203, change in cost 0.000448227\n",
      "step 16490, training accuracy 0.959596, cost 10.2199, change in cost 0.000452042\n",
      "step 16500, training accuracy 0.959596, cost 10.2194, change in cost 0.000447273\n",
      "step 16510, training accuracy 0.959596, cost 10.219, change in cost 0.000450134\n",
      "step 16520, training accuracy 0.959596, cost 10.2185, change in cost 0.000449181\n",
      "step 16530, training accuracy 0.959596, cost 10.2181, change in cost 0.000448227\n",
      "step 16540, training accuracy 0.959596, cost 10.2176, change in cost 0.000448227\n",
      "step 16550, training accuracy 0.959596, cost 10.2172, change in cost 0.000448227\n",
      "step 16560, training accuracy 0.959596, cost 10.2167, change in cost 0.000447273\n",
      "step 16570, training accuracy 0.959596, cost 10.2163, change in cost 0.000447273\n",
      "step 16580, training accuracy 0.959596, cost 10.2158, change in cost 0.000445366\n",
      "step 16590, training accuracy 0.959596, cost 10.2154, change in cost 0.000449181\n",
      "step 16600, training accuracy 0.959596, cost 10.2149, change in cost 0.000444412\n",
      "step 16610, training accuracy 0.959596, cost 10.2145, change in cost 0.00044632\n",
      "step 16620, training accuracy 0.959596, cost 10.214, change in cost 0.000444412\n",
      "step 16630, training accuracy 0.959596, cost 10.2136, change in cost 0.000444412\n",
      "step 16640, training accuracy 0.959596, cost 10.2131, change in cost 0.000445366\n",
      "step 16650, training accuracy 0.959596, cost 10.2127, change in cost 0.000443459\n",
      "step 16660, training accuracy 0.959596, cost 10.2123, change in cost 0.000445366\n",
      "step 16670, training accuracy 0.959596, cost 10.2118, change in cost 0.000443459\n",
      "step 16680, training accuracy 0.959596, cost 10.2114, change in cost 0.000442505\n",
      "step 16690, training accuracy 0.959596, cost 10.2109, change in cost 0.000442505\n",
      "step 16700, training accuracy 0.959596, cost 10.2105, change in cost 0.000441551\n",
      "step 16710, training accuracy 0.959596, cost 10.21, change in cost 0.000442505\n",
      "step 16720, training accuracy 0.959596, cost 10.2096, change in cost 0.000442505\n",
      "step 16730, training accuracy 0.959596, cost 10.2092, change in cost 0.000442505\n",
      "step 16740, training accuracy 0.959596, cost 10.2087, change in cost 0.000439644\n",
      "step 16750, training accuracy 0.959596, cost 10.2083, change in cost 0.000440598\n",
      "step 16760, training accuracy 0.959596, cost 10.2078, change in cost 0.000440598\n",
      "step 16770, training accuracy 0.959596, cost 10.2074, change in cost 0.000440598\n",
      "step 16780, training accuracy 0.959596, cost 10.207, change in cost 0.00043869\n",
      "step 16790, training accuracy 0.959596, cost 10.2065, change in cost 0.000440598\n",
      "step 16800, training accuracy 0.959596, cost 10.2061, change in cost 0.000437737\n",
      "step 16810, training accuracy 0.959596, cost 10.2056, change in cost 0.000439644\n",
      "step 16820, training accuracy 0.959596, cost 10.2052, change in cost 0.000436783\n",
      "step 16830, training accuracy 0.959596, cost 10.2048, change in cost 0.000439644\n",
      "step 16840, training accuracy 0.959596, cost 10.2043, change in cost 0.000436783\n",
      "step 16850, training accuracy 0.959596, cost 10.2039, change in cost 0.000436783\n",
      "step 16860, training accuracy 0.959596, cost 10.2035, change in cost 0.000436783\n",
      "step 16870, training accuracy 0.959596, cost 10.203, change in cost 0.000436783\n",
      "step 16880, training accuracy 0.959596, cost 10.2026, change in cost 0.000434875\n",
      "step 16890, training accuracy 0.959596, cost 10.2021, change in cost 0.000437737\n",
      "step 16900, training accuracy 0.959596, cost 10.2017, change in cost 0.000433922\n",
      "step 16910, training accuracy 0.959596, cost 10.2013, change in cost 0.000434875\n",
      "step 16920, training accuracy 0.959596, cost 10.2008, change in cost 0.000435829\n",
      "step 16930, training accuracy 0.959596, cost 10.2004, change in cost 0.000434875\n",
      "step 16940, training accuracy 0.959596, cost 10.2, change in cost 0.000432968\n",
      "step 16950, training accuracy 0.959596, cost 10.1995, change in cost 0.000432968\n",
      "step 16960, training accuracy 0.959596, cost 10.1991, change in cost 0.000434875\n",
      "step 16970, training accuracy 0.959596, cost 10.1987, change in cost 0.000432014\n",
      "step 16980, training accuracy 0.959596, cost 10.1982, change in cost 0.000433922\n",
      "step 16990, training accuracy 0.959596, cost 10.1978, change in cost 0.000431061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.959596, cost 10.1974, change in cost 0.000432014\n",
      "step 17010, training accuracy 0.959596, cost 10.1969, change in cost 0.000432968\n",
      "step 17020, training accuracy 0.959596, cost 10.1965, change in cost 0.000430107\n",
      "step 17030, training accuracy 0.959596, cost 10.1961, change in cost 0.000431061\n",
      "step 17040, training accuracy 0.959596, cost 10.1957, change in cost 0.000430107\n",
      "step 17050, training accuracy 0.959596, cost 10.1952, change in cost 0.000430107\n",
      "step 17060, training accuracy 0.959596, cost 10.1948, change in cost 0.000431061\n",
      "step 17070, training accuracy 0.959596, cost 10.1944, change in cost 0.0004282\n",
      "step 17080, training accuracy 0.959596, cost 10.1939, change in cost 0.000432014\n",
      "step 17090, training accuracy 0.959596, cost 10.1935, change in cost 0.000429153\n",
      "step 17100, training accuracy 0.959596, cost 10.1931, change in cost 0.0004282\n",
      "step 17110, training accuracy 0.959596, cost 10.1926, change in cost 0.000427246\n",
      "step 17120, training accuracy 0.959596, cost 10.1922, change in cost 0.000429153\n",
      "step 17130, training accuracy 0.959596, cost 10.1918, change in cost 0.000426292\n",
      "step 17140, training accuracy 0.959596, cost 10.1914, change in cost 0.0004282\n",
      "step 17150, training accuracy 0.959596, cost 10.1909, change in cost 0.000426292\n",
      "step 17160, training accuracy 0.959596, cost 10.1905, change in cost 0.000426292\n",
      "step 17170, training accuracy 0.959596, cost 10.1901, change in cost 0.000427246\n",
      "step 17180, training accuracy 0.959596, cost 10.1897, change in cost 0.000426292\n",
      "step 17190, training accuracy 0.959596, cost 10.1892, change in cost 0.000425339\n",
      "step 17200, training accuracy 0.959596, cost 10.1888, change in cost 0.000426292\n",
      "step 17210, training accuracy 0.959596, cost 10.1884, change in cost 0.000424385\n",
      "step 17220, training accuracy 0.959596, cost 10.188, change in cost 0.000422478\n",
      "step 17230, training accuracy 0.959596, cost 10.1875, change in cost 0.000426292\n",
      "step 17240, training accuracy 0.959596, cost 10.1871, change in cost 0.000423431\n",
      "step 17250, training accuracy 0.959596, cost 10.1867, change in cost 0.000423431\n",
      "step 17260, training accuracy 0.959596, cost 10.1863, change in cost 0.000422478\n",
      "step 17270, training accuracy 0.959596, cost 10.1858, change in cost 0.000424385\n",
      "step 17280, training accuracy 0.959596, cost 10.1854, change in cost 0.00042057\n",
      "step 17290, training accuracy 0.959596, cost 10.185, change in cost 0.000424385\n",
      "step 17300, training accuracy 0.959596, cost 10.1846, change in cost 0.000421524\n",
      "step 17310, training accuracy 0.959596, cost 10.1841, change in cost 0.000422478\n",
      "step 17320, training accuracy 0.959596, cost 10.1837, change in cost 0.000421524\n",
      "step 17330, training accuracy 0.959596, cost 10.1833, change in cost 0.000419617\n",
      "step 17340, training accuracy 0.959596, cost 10.1829, change in cost 0.000421524\n",
      "step 17350, training accuracy 0.959596, cost 10.1825, change in cost 0.00042057\n",
      "step 17360, training accuracy 0.959596, cost 10.182, change in cost 0.000419617\n",
      "step 17370, training accuracy 0.959596, cost 10.1816, change in cost 0.000418663\n",
      "step 17380, training accuracy 0.959596, cost 10.1812, change in cost 0.000421524\n",
      "step 17390, training accuracy 0.959596, cost 10.1808, change in cost 0.000418663\n",
      "step 17400, training accuracy 0.959596, cost 10.1804, change in cost 0.000418663\n",
      "step 17410, training accuracy 0.959596, cost 10.1799, change in cost 0.000417709\n",
      "step 17420, training accuracy 0.959596, cost 10.1795, change in cost 0.000418663\n",
      "step 17430, training accuracy 0.959596, cost 10.1791, change in cost 0.000416756\n",
      "step 17440, training accuracy 0.959596, cost 10.1787, change in cost 0.000418663\n",
      "step 17450, training accuracy 0.959596, cost 10.1783, change in cost 0.000415802\n",
      "step 17460, training accuracy 0.959596, cost 10.1779, change in cost 0.000418663\n",
      "step 17470, training accuracy 0.959596, cost 10.1774, change in cost 0.000414848\n",
      "step 17480, training accuracy 0.959596, cost 10.177, change in cost 0.000417709\n",
      "step 17490, training accuracy 0.959596, cost 10.1766, change in cost 0.000414848\n",
      "step 17500, training accuracy 0.959596, cost 10.1762, change in cost 0.000414848\n",
      "step 17510, training accuracy 0.959596, cost 10.1758, change in cost 0.000415802\n",
      "step 17520, training accuracy 0.959596, cost 10.1754, change in cost 0.000414848\n",
      "step 17530, training accuracy 0.959596, cost 10.175, change in cost 0.000412941\n",
      "step 17540, training accuracy 0.959596, cost 10.1745, change in cost 0.000414848\n",
      "step 17550, training accuracy 0.959596, cost 10.1741, change in cost 0.000415802\n",
      "step 17560, training accuracy 0.959596, cost 10.1737, change in cost 0.000411987\n",
      "step 17570, training accuracy 0.959596, cost 10.1733, change in cost 0.000413895\n",
      "step 17580, training accuracy 0.959596, cost 10.1729, change in cost 0.000413895\n",
      "step 17590, training accuracy 0.959596, cost 10.1725, change in cost 0.000411034\n",
      "step 17600, training accuracy 0.959596, cost 10.1721, change in cost 0.000412941\n",
      "step 17610, training accuracy 0.959596, cost 10.1716, change in cost 0.000411034\n",
      "step 17620, training accuracy 0.959596, cost 10.1712, change in cost 0.000412941\n",
      "step 17630, training accuracy 0.959596, cost 10.1708, change in cost 0.000411987\n",
      "step 17640, training accuracy 0.959596, cost 10.1704, change in cost 0.000411034\n",
      "step 17650, training accuracy 0.959596, cost 10.17, change in cost 0.00041008\n",
      "step 17660, training accuracy 0.959596, cost 10.1696, change in cost 0.000411034\n",
      "step 17670, training accuracy 0.959596, cost 10.1692, change in cost 0.00041008\n",
      "step 17680, training accuracy 0.959596, cost 10.1688, change in cost 0.00041008\n",
      "step 17690, training accuracy 0.959596, cost 10.1684, change in cost 0.000409126\n",
      "step 17700, training accuracy 0.959596, cost 10.168, change in cost 0.000408173\n",
      "step 17710, training accuracy 0.959596, cost 10.1675, change in cost 0.00041008\n",
      "step 17720, training accuracy 0.959596, cost 10.1671, change in cost 0.000408173\n",
      "step 17730, training accuracy 0.959596, cost 10.1667, change in cost 0.000408173\n",
      "step 17740, training accuracy 0.959596, cost 10.1663, change in cost 0.000409126\n",
      "step 17750, training accuracy 0.959596, cost 10.1659, change in cost 0.000407219\n",
      "step 17760, training accuracy 0.959596, cost 10.1655, change in cost 0.000405312\n",
      "step 17770, training accuracy 0.959596, cost 10.1651, change in cost 0.000408173\n",
      "step 17780, training accuracy 0.959596, cost 10.1647, change in cost 0.000407219\n",
      "step 17790, training accuracy 0.959596, cost 10.1643, change in cost 0.000405312\n",
      "step 17800, training accuracy 0.959596, cost 10.1639, change in cost 0.000406265\n",
      "step 17810, training accuracy 0.959596, cost 10.1635, change in cost 0.000407219\n",
      "step 17820, training accuracy 0.959596, cost 10.1631, change in cost 0.000405312\n",
      "step 17830, training accuracy 0.959596, cost 10.1627, change in cost 0.000404358\n",
      "step 17840, training accuracy 0.959596, cost 10.1623, change in cost 0.000405312\n",
      "step 17850, training accuracy 0.959596, cost 10.1619, change in cost 0.000403404\n",
      "step 17860, training accuracy 0.959596, cost 10.1614, change in cost 0.000405312\n",
      "step 17870, training accuracy 0.959596, cost 10.161, change in cost 0.000403404\n",
      "step 17880, training accuracy 0.959596, cost 10.1606, change in cost 0.000402451\n",
      "step 17890, training accuracy 0.959596, cost 10.1602, change in cost 0.000403404\n",
      "step 17900, training accuracy 0.959596, cost 10.1598, change in cost 0.000403404\n",
      "step 17910, training accuracy 0.959596, cost 10.1594, change in cost 0.000403404\n",
      "step 17920, training accuracy 0.959596, cost 10.159, change in cost 0.000401497\n",
      "step 17930, training accuracy 0.959596, cost 10.1586, change in cost 0.000402451\n",
      "step 17940, training accuracy 0.959596, cost 10.1582, change in cost 0.000401497\n",
      "step 17950, training accuracy 0.959596, cost 10.1578, change in cost 0.000403404\n",
      "step 17960, training accuracy 0.959596, cost 10.1574, change in cost 0.000400543\n",
      "step 17970, training accuracy 0.959596, cost 10.157, change in cost 0.000400543\n",
      "step 17980, training accuracy 0.959596, cost 10.1566, change in cost 0.000401497\n",
      "step 17990, training accuracy 0.959596, cost 10.1562, change in cost 0.000400543\n",
      "step 18000, training accuracy 0.959596, cost 10.1558, change in cost 0.00039959\n",
      "step 18010, training accuracy 0.959596, cost 10.1554, change in cost 0.00039959\n",
      "step 18020, training accuracy 0.959596, cost 10.155, change in cost 0.000397682\n",
      "step 18030, training accuracy 0.959596, cost 10.1546, change in cost 0.000400543\n",
      "step 18040, training accuracy 0.959596, cost 10.1542, change in cost 0.00039959\n",
      "step 18050, training accuracy 0.959596, cost 10.1538, change in cost 0.000398636\n",
      "step 18060, training accuracy 0.959596, cost 10.1534, change in cost 0.000398636\n",
      "step 18070, training accuracy 0.959596, cost 10.153, change in cost 0.000396729\n",
      "step 18080, training accuracy 0.959596, cost 10.1526, change in cost 0.000396729\n",
      "step 18090, training accuracy 0.959596, cost 10.1522, change in cost 0.000398636\n",
      "step 18100, training accuracy 0.959596, cost 10.1518, change in cost 0.000397682\n",
      "step 18110, training accuracy 0.959596, cost 10.1514, change in cost 0.000395775\n",
      "step 18120, training accuracy 0.959596, cost 10.151, change in cost 0.000397682\n",
      "step 18130, training accuracy 0.959596, cost 10.1506, change in cost 0.000395775\n",
      "step 18140, training accuracy 0.959596, cost 10.1503, change in cost 0.000395775\n",
      "step 18150, training accuracy 0.959596, cost 10.1499, change in cost 0.000394821\n",
      "step 18160, training accuracy 0.959596, cost 10.1495, change in cost 0.000395775\n",
      "step 18170, training accuracy 0.959596, cost 10.1491, change in cost 0.000394821\n",
      "step 18180, training accuracy 0.959596, cost 10.1487, change in cost 0.000394821\n",
      "step 18190, training accuracy 0.959596, cost 10.1483, change in cost 0.000393867\n",
      "step 18200, training accuracy 0.959596, cost 10.1479, change in cost 0.000393867\n",
      "step 18210, training accuracy 0.959596, cost 10.1475, change in cost 0.000394821\n",
      "step 18220, training accuracy 0.959596, cost 10.1471, change in cost 0.00039196\n",
      "step 18230, training accuracy 0.959596, cost 10.1467, change in cost 0.000394821\n",
      "step 18240, training accuracy 0.959596, cost 10.1463, change in cost 0.000391006\n",
      "step 18250, training accuracy 0.959596, cost 10.1459, change in cost 0.000392914\n",
      "step 18260, training accuracy 0.959596, cost 10.1455, change in cost 0.000393867\n",
      "step 18270, training accuracy 0.959596, cost 10.1451, change in cost 0.00039196\n",
      "step 18280, training accuracy 0.959596, cost 10.1447, change in cost 0.000391006\n",
      "step 18290, training accuracy 0.959596, cost 10.1444, change in cost 0.000391006\n",
      "step 18300, training accuracy 0.959596, cost 10.144, change in cost 0.000391006\n",
      "step 18310, training accuracy 0.959596, cost 10.1436, change in cost 0.00039196\n",
      "step 18320, training accuracy 0.959596, cost 10.1432, change in cost 0.000389099\n",
      "step 18330, training accuracy 0.959596, cost 10.1428, change in cost 0.00039196\n",
      "step 18340, training accuracy 0.959596, cost 10.1424, change in cost 0.000390053\n",
      "step 18350, training accuracy 0.959596, cost 10.142, change in cost 0.000389099\n",
      "step 18360, training accuracy 0.959596, cost 10.1416, change in cost 0.000389099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18370, training accuracy 0.959596, cost 10.1412, change in cost 0.000388145\n",
      "step 18380, training accuracy 0.959596, cost 10.1408, change in cost 0.000389099\n",
      "step 18390, training accuracy 0.959596, cost 10.1405, change in cost 0.000390053\n",
      "step 18400, training accuracy 0.959596, cost 10.1401, change in cost 0.000387192\n",
      "step 18410, training accuracy 0.959596, cost 10.1397, change in cost 0.000389099\n",
      "step 18420, training accuracy 0.959596, cost 10.1393, change in cost 0.000387192\n",
      "step 18430, training accuracy 0.959596, cost 10.1389, change in cost 0.000387192\n",
      "step 18440, training accuracy 0.959596, cost 10.1385, change in cost 0.000387192\n",
      "step 18450, training accuracy 0.959596, cost 10.1381, change in cost 0.000386238\n",
      "step 18460, training accuracy 0.959596, cost 10.1377, change in cost 0.000385284\n",
      "step 18470, training accuracy 0.959596, cost 10.1374, change in cost 0.000388145\n",
      "step 18480, training accuracy 0.959596, cost 10.137, change in cost 0.000386238\n",
      "step 18490, training accuracy 0.959596, cost 10.1366, change in cost 0.000386238\n",
      "step 18500, training accuracy 0.959596, cost 10.1362, change in cost 0.000384331\n",
      "step 18510, training accuracy 0.959596, cost 10.1358, change in cost 0.000386238\n",
      "step 18520, training accuracy 0.959596, cost 10.1354, change in cost 0.000383377\n",
      "step 18530, training accuracy 0.959596, cost 10.135, change in cost 0.000384331\n",
      "step 18540, training accuracy 0.959596, cost 10.1347, change in cost 0.000385284\n",
      "step 18550, training accuracy 0.959596, cost 10.1343, change in cost 0.000383377\n",
      "step 18560, training accuracy 0.959596, cost 10.1339, change in cost 0.000385284\n",
      "step 18570, training accuracy 0.959596, cost 10.1335, change in cost 0.000383377\n",
      "step 18580, training accuracy 0.959596, cost 10.1331, change in cost 0.000382423\n",
      "step 18590, training accuracy 0.959596, cost 10.1327, change in cost 0.000383377\n",
      "step 18600, training accuracy 0.959596, cost 10.1324, change in cost 0.00038147\n",
      "step 18610, training accuracy 0.959596, cost 10.132, change in cost 0.000382423\n",
      "step 18620, training accuracy 0.959596, cost 10.1316, change in cost 0.000380516\n",
      "step 18630, training accuracy 0.959596, cost 10.1312, change in cost 0.000383377\n",
      "step 18640, training accuracy 0.959596, cost 10.1308, change in cost 0.000382423\n",
      "step 18650, training accuracy 0.959596, cost 10.1304, change in cost 0.00038147\n",
      "step 18660, training accuracy 0.959596, cost 10.1301, change in cost 0.00038147\n",
      "step 18670, training accuracy 0.959596, cost 10.1297, change in cost 0.000378609\n",
      "step 18680, training accuracy 0.959596, cost 10.1293, change in cost 0.000380516\n",
      "step 18690, training accuracy 0.959596, cost 10.1289, change in cost 0.000379562\n",
      "step 18700, training accuracy 0.959596, cost 10.1285, change in cost 0.000380516\n",
      "step 18710, training accuracy 0.959596, cost 10.1282, change in cost 0.000379562\n",
      "step 18720, training accuracy 0.959596, cost 10.1278, change in cost 0.000379562\n",
      "step 18730, training accuracy 0.959596, cost 10.1274, change in cost 0.000379562\n",
      "step 18740, training accuracy 0.959596, cost 10.127, change in cost 0.000378609\n",
      "step 18750, training accuracy 0.959596, cost 10.1267, change in cost 0.000378609\n",
      "step 18760, training accuracy 0.959596, cost 10.1263, change in cost 0.000376701\n",
      "step 18770, training accuracy 0.959596, cost 10.1259, change in cost 0.000378609\n",
      "step 18780, training accuracy 0.959596, cost 10.1255, change in cost 0.000377655\n",
      "step 18790, training accuracy 0.959596, cost 10.1251, change in cost 0.000376701\n",
      "step 18800, training accuracy 0.959596, cost 10.1248, change in cost 0.000378609\n",
      "step 18810, training accuracy 0.959596, cost 10.1244, change in cost 0.000374794\n",
      "step 18820, training accuracy 0.959596, cost 10.124, change in cost 0.000376701\n",
      "step 18830, training accuracy 0.959596, cost 10.1236, change in cost 0.000376701\n",
      "step 18840, training accuracy 0.959596, cost 10.1233, change in cost 0.000375748\n",
      "step 18850, training accuracy 0.959596, cost 10.1229, change in cost 0.000375748\n",
      "step 18860, training accuracy 0.959596, cost 10.1225, change in cost 0.000374794\n",
      "step 18870, training accuracy 0.959596, cost 10.1221, change in cost 0.000376701\n",
      "step 18880, training accuracy 0.959596, cost 10.1218, change in cost 0.000375748\n",
      "step 18890, training accuracy 0.959596, cost 10.1214, change in cost 0.00037384\n",
      "step 18900, training accuracy 0.959596, cost 10.121, change in cost 0.000372887\n",
      "step 18910, training accuracy 0.959596, cost 10.1206, change in cost 0.000375748\n",
      "step 18920, training accuracy 0.959596, cost 10.1203, change in cost 0.000372887\n",
      "step 18930, training accuracy 0.959596, cost 10.1199, change in cost 0.000372887\n",
      "step 18940, training accuracy 0.959596, cost 10.1195, change in cost 0.000374794\n",
      "step 18950, training accuracy 0.959596, cost 10.1191, change in cost 0.000372887\n",
      "step 18960, training accuracy 0.959596, cost 10.1188, change in cost 0.000372887\n",
      "step 18970, training accuracy 0.959596, cost 10.1184, change in cost 0.000371933\n",
      "step 18980, training accuracy 0.959596, cost 10.118, change in cost 0.000371933\n",
      "step 18990, training accuracy 0.959596, cost 10.1177, change in cost 0.000371933\n",
      "step 19000, training accuracy 0.959596, cost 10.1173, change in cost 0.000371933\n",
      "step 19010, training accuracy 0.959596, cost 10.1169, change in cost 0.000370026\n",
      "step 19020, training accuracy 0.959596, cost 10.1165, change in cost 0.000371933\n",
      "step 19030, training accuracy 0.959596, cost 10.1162, change in cost 0.000370026\n",
      "step 19040, training accuracy 0.959596, cost 10.1158, change in cost 0.000372887\n",
      "step 19050, training accuracy 0.959596, cost 10.1154, change in cost 0.000369072\n",
      "step 19060, training accuracy 0.959596, cost 10.1151, change in cost 0.000369072\n",
      "step 19070, training accuracy 0.959596, cost 10.1147, change in cost 0.000370979\n",
      "step 19080, training accuracy 0.959596, cost 10.1143, change in cost 0.000369072\n",
      "step 19090, training accuracy 0.959596, cost 10.1139, change in cost 0.000370026\n",
      "step 19100, training accuracy 0.959596, cost 10.1136, change in cost 0.000370026\n",
      "step 19110, training accuracy 0.959596, cost 10.1132, change in cost 0.000367165\n",
      "step 19120, training accuracy 0.959596, cost 10.1128, change in cost 0.000369072\n",
      "step 19130, training accuracy 0.959596, cost 10.1125, change in cost 0.000369072\n",
      "step 19140, training accuracy 0.959596, cost 10.1121, change in cost 0.000366211\n",
      "step 19150, training accuracy 0.959596, cost 10.1117, change in cost 0.000368118\n",
      "step 19160, training accuracy 0.959596, cost 10.1114, change in cost 0.000367165\n",
      "step 19170, training accuracy 0.959596, cost 10.111, change in cost 0.000367165\n",
      "step 19180, training accuracy 0.959596, cost 10.1106, change in cost 0.000366211\n",
      "step 19190, training accuracy 0.959596, cost 10.1103, change in cost 0.000367165\n",
      "step 19200, training accuracy 0.959596, cost 10.1099, change in cost 0.000366211\n",
      "step 19210, training accuracy 0.959596, cost 10.1095, change in cost 0.000365257\n",
      "step 19220, training accuracy 0.959596, cost 10.1092, change in cost 0.000367165\n",
      "step 19230, training accuracy 0.959596, cost 10.1088, change in cost 0.000365257\n",
      "step 19240, training accuracy 0.959596, cost 10.1084, change in cost 0.000365257\n",
      "step 19250, training accuracy 0.959596, cost 10.1081, change in cost 0.000365257\n",
      "step 19260, training accuracy 0.959596, cost 10.1077, change in cost 0.000365257\n",
      "step 19270, training accuracy 0.959596, cost 10.1073, change in cost 0.00036335\n",
      "step 19280, training accuracy 0.959596, cost 10.107, change in cost 0.000364304\n",
      "step 19290, training accuracy 0.959596, cost 10.1066, change in cost 0.000364304\n",
      "step 19300, training accuracy 0.959596, cost 10.1063, change in cost 0.00036335\n",
      "step 19310, training accuracy 0.959596, cost 10.1059, change in cost 0.00036335\n",
      "step 19320, training accuracy 0.959596, cost 10.1055, change in cost 0.000364304\n",
      "step 19330, training accuracy 0.959596, cost 10.1052, change in cost 0.000361443\n",
      "step 19340, training accuracy 0.959596, cost 10.1048, change in cost 0.000362396\n",
      "step 19350, training accuracy 0.959596, cost 10.1044, change in cost 0.00036335\n",
      "step 19360, training accuracy 0.959596, cost 10.1041, change in cost 0.00036335\n",
      "step 19370, training accuracy 0.959596, cost 10.1037, change in cost 0.000362396\n",
      "step 19380, training accuracy 0.959596, cost 10.1034, change in cost 0.000360489\n",
      "step 19390, training accuracy 0.959596, cost 10.103, change in cost 0.000361443\n",
      "step 19400, training accuracy 0.959596, cost 10.1026, change in cost 0.000360489\n",
      "step 19410, training accuracy 0.959596, cost 10.1023, change in cost 0.000360489\n",
      "step 19420, training accuracy 0.959596, cost 10.1019, change in cost 0.000361443\n",
      "step 19430, training accuracy 0.959596, cost 10.1015, change in cost 0.000360489\n",
      "step 19440, training accuracy 0.959596, cost 10.1012, change in cost 0.000360489\n",
      "step 19450, training accuracy 0.959596, cost 10.1008, change in cost 0.000360489\n",
      "step 19460, training accuracy 0.959596, cost 10.1005, change in cost 0.000358582\n",
      "step 19470, training accuracy 0.959596, cost 10.1001, change in cost 0.000359535\n",
      "step 19480, training accuracy 0.959596, cost 10.0998, change in cost 0.000359535\n",
      "step 19490, training accuracy 0.959596, cost 10.0994, change in cost 0.000359535\n",
      "step 19500, training accuracy 0.959596, cost 10.099, change in cost 0.000357628\n",
      "step 19510, training accuracy 0.959596, cost 10.0987, change in cost 0.000359535\n",
      "step 19520, training accuracy 0.959596, cost 10.0983, change in cost 0.000357628\n",
      "step 19530, training accuracy 0.959596, cost 10.098, change in cost 0.000359535\n",
      "step 19540, training accuracy 0.959596, cost 10.0976, change in cost 0.000356674\n",
      "step 19550, training accuracy 0.959596, cost 10.0972, change in cost 0.000357628\n",
      "step 19560, training accuracy 0.959596, cost 10.0969, change in cost 0.000356674\n",
      "step 19570, training accuracy 0.959596, cost 10.0965, change in cost 0.000354767\n",
      "step 19580, training accuracy 0.959596, cost 10.0962, change in cost 0.000358582\n",
      "step 19590, training accuracy 0.959596, cost 10.0958, change in cost 0.000355721\n",
      "step 19600, training accuracy 0.959596, cost 10.0955, change in cost 0.000355721\n",
      "step 19610, training accuracy 0.959596, cost 10.0951, change in cost 0.000357628\n",
      "step 19620, training accuracy 0.959596, cost 10.0947, change in cost 0.000353813\n",
      "step 19630, training accuracy 0.959596, cost 10.0944, change in cost 0.000356674\n",
      "step 19640, training accuracy 0.959596, cost 10.094, change in cost 0.000354767\n",
      "step 19650, training accuracy 0.959596, cost 10.0937, change in cost 0.000354767\n",
      "step 19660, training accuracy 0.959596, cost 10.0933, change in cost 0.000354767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19670, training accuracy 0.959596, cost 10.093, change in cost 0.000354767\n",
      "step 19680, training accuracy 0.959596, cost 10.0926, change in cost 0.000352859\n",
      "step 19690, training accuracy 0.959596, cost 10.0923, change in cost 0.000355721\n",
      "step 19700, training accuracy 0.959596, cost 10.0919, change in cost 0.000353813\n",
      "step 19710, training accuracy 0.959596, cost 10.0916, change in cost 0.000353813\n",
      "step 19720, training accuracy 0.959596, cost 10.0912, change in cost 0.000351906\n",
      "step 19730, training accuracy 0.959596, cost 10.0909, change in cost 0.000353813\n",
      "step 19740, training accuracy 0.959596, cost 10.0905, change in cost 0.000352859\n",
      "step 19750, training accuracy 0.959596, cost 10.0901, change in cost 0.000351906\n",
      "step 19760, training accuracy 0.959596, cost 10.0898, change in cost 0.000349998\n",
      "step 19770, training accuracy 0.959596, cost 10.0894, change in cost 0.000353813\n",
      "step 19780, training accuracy 0.959596, cost 10.0891, change in cost 0.000350952\n",
      "step 19790, training accuracy 0.959596, cost 10.0887, change in cost 0.000350952\n",
      "step 19800, training accuracy 0.959596, cost 10.0884, change in cost 0.000351906\n",
      "step 19810, training accuracy 0.959596, cost 10.088, change in cost 0.000349998\n",
      "step 19820, training accuracy 0.959596, cost 10.0877, change in cost 0.000351906\n",
      "step 19830, training accuracy 0.959596, cost 10.0873, change in cost 0.000350952\n",
      "step 19840, training accuracy 0.959596, cost 10.087, change in cost 0.000349998\n",
      "step 19850, training accuracy 0.959596, cost 10.0866, change in cost 0.000349045\n",
      "step 19860, training accuracy 0.959596, cost 10.0863, change in cost 0.000349998\n",
      "step 19870, training accuracy 0.959596, cost 10.0859, change in cost 0.000349045\n",
      "step 19880, training accuracy 0.959596, cost 10.0856, change in cost 0.000350952\n",
      "step 19890, training accuracy 0.959596, cost 10.0852, change in cost 0.000348091\n",
      "step 19900, training accuracy 0.959596, cost 10.0849, change in cost 0.000348091\n",
      "step 19910, training accuracy 0.959596, cost 10.0845, change in cost 0.000349998\n",
      "step 19920, training accuracy 0.959596, cost 10.0842, change in cost 0.000348091\n",
      "step 19930, training accuracy 0.959596, cost 10.0838, change in cost 0.000347137\n",
      "step 19940, training accuracy 0.959596, cost 10.0835, change in cost 0.000347137\n",
      "step 19950, training accuracy 0.959596, cost 10.0832, change in cost 0.000347137\n",
      "step 19960, training accuracy 0.959596, cost 10.0828, change in cost 0.000347137\n",
      "step 19970, training accuracy 0.959596, cost 10.0825, change in cost 0.000349045\n",
      "step 19980, training accuracy 0.959596, cost 10.0821, change in cost 0.000344276\n",
      "step 19990, training accuracy 0.959596, cost 10.0818, change in cost 0.000349045\n",
      "step 20000, training accuracy 0.959596, cost 10.0814, change in cost 0.000346184\n",
      "step 20010, training accuracy 0.959596, cost 10.0811, change in cost 0.000347137\n",
      "step 20020, training accuracy 0.959596, cost 10.0807, change in cost 0.00034523\n",
      "step 20030, training accuracy 0.959596, cost 10.0804, change in cost 0.000346184\n",
      "step 20040, training accuracy 0.959596, cost 10.08, change in cost 0.000343323\n",
      "step 20050, training accuracy 0.959596, cost 10.0797, change in cost 0.000346184\n",
      "step 20060, training accuracy 0.959596, cost 10.0793, change in cost 0.000344276\n",
      "step 20070, training accuracy 0.959596, cost 10.079, change in cost 0.00034523\n",
      "step 20080, training accuracy 0.959596, cost 10.0787, change in cost 0.000344276\n",
      "step 20090, training accuracy 0.959596, cost 10.0783, change in cost 0.000344276\n",
      "step 20100, training accuracy 0.959596, cost 10.078, change in cost 0.000342369\n",
      "step 20110, training accuracy 0.959596, cost 10.0776, change in cost 0.00034523\n",
      "step 20120, training accuracy 0.959596, cost 10.0773, change in cost 0.000343323\n",
      "step 20130, training accuracy 0.959596, cost 10.0769, change in cost 0.000343323\n",
      "step 20140, training accuracy 0.959596, cost 10.0766, change in cost 0.000343323\n",
      "step 20150, training accuracy 0.959596, cost 10.0762, change in cost 0.000343323\n",
      "step 20160, training accuracy 0.959596, cost 10.0759, change in cost 0.000341415\n",
      "step 20170, training accuracy 0.959596, cost 10.0756, change in cost 0.000342369\n",
      "step 20180, training accuracy 0.959596, cost 10.0752, change in cost 0.000341415\n",
      "step 20190, training accuracy 0.959596, cost 10.0749, change in cost 0.000342369\n",
      "step 20200, training accuracy 0.959596, cost 10.0745, change in cost 0.000342369\n",
      "step 20210, training accuracy 0.959596, cost 10.0742, change in cost 0.000341415\n",
      "step 20220, training accuracy 0.959596, cost 10.0739, change in cost 0.000341415\n",
      "step 20230, training accuracy 0.959596, cost 10.0735, change in cost 0.000340462\n",
      "step 20240, training accuracy 0.959596, cost 10.0732, change in cost 0.000340462\n",
      "step 20250, training accuracy 0.959596, cost 10.0728, change in cost 0.000341415\n",
      "step 20260, training accuracy 0.959596, cost 10.0725, change in cost 0.000338554\n",
      "step 20270, training accuracy 0.959596, cost 10.0722, change in cost 0.000340462\n",
      "step 20280, training accuracy 0.959596, cost 10.0718, change in cost 0.000338554\n",
      "step 20290, training accuracy 0.959596, cost 10.0715, change in cost 0.000340462\n",
      "step 20300, training accuracy 0.959596, cost 10.0711, change in cost 0.000338554\n",
      "step 20310, training accuracy 0.959596, cost 10.0708, change in cost 0.000339508\n",
      "step 20320, training accuracy 0.959596, cost 10.0705, change in cost 0.000339508\n",
      "step 20330, training accuracy 0.959596, cost 10.0701, change in cost 0.000338554\n",
      "step 20340, training accuracy 0.959596, cost 10.0698, change in cost 0.000339508\n",
      "step 20350, training accuracy 0.959596, cost 10.0694, change in cost 0.000337601\n",
      "step 20360, training accuracy 0.959596, cost 10.0691, change in cost 0.000336647\n",
      "step 20370, training accuracy 0.959596, cost 10.0688, change in cost 0.000337601\n",
      "step 20380, training accuracy 0.959596, cost 10.0684, change in cost 0.000338554\n",
      "step 20390, training accuracy 0.959596, cost 10.0681, change in cost 0.000338554\n",
      "step 20400, training accuracy 0.959596, cost 10.0678, change in cost 0.00033474\n",
      "step 20410, training accuracy 0.959596, cost 10.0674, change in cost 0.000336647\n",
      "step 20420, training accuracy 0.959596, cost 10.0671, change in cost 0.000337601\n",
      "step 20430, training accuracy 0.959596, cost 10.0667, change in cost 0.000335693\n",
      "step 20440, training accuracy 0.959596, cost 10.0664, change in cost 0.00033474\n",
      "step 20450, training accuracy 0.959596, cost 10.0661, change in cost 0.000337601\n",
      "step 20460, training accuracy 0.959596, cost 10.0657, change in cost 0.00033474\n",
      "step 20470, training accuracy 0.959596, cost 10.0654, change in cost 0.00033474\n",
      "step 20480, training accuracy 0.959596, cost 10.0651, change in cost 0.000335693\n",
      "step 20490, training accuracy 0.959596, cost 10.0647, change in cost 0.00033474\n",
      "step 20500, training accuracy 0.959596, cost 10.0644, change in cost 0.000335693\n",
      "step 20510, training accuracy 0.959596, cost 10.0641, change in cost 0.00033474\n",
      "step 20520, training accuracy 0.959596, cost 10.0637, change in cost 0.000333786\n",
      "step 20530, training accuracy 0.959596, cost 10.0634, change in cost 0.000332832\n",
      "step 20540, training accuracy 0.959596, cost 10.0631, change in cost 0.00033474\n",
      "step 20550, training accuracy 0.959596, cost 10.0627, change in cost 0.00033474\n",
      "step 20560, training accuracy 0.959596, cost 10.0624, change in cost 0.000331879\n",
      "step 20570, training accuracy 0.959596, cost 10.0621, change in cost 0.000332832\n",
      "step 20580, training accuracy 0.959596, cost 10.0617, change in cost 0.000331879\n",
      "step 20590, training accuracy 0.959596, cost 10.0614, change in cost 0.000333786\n",
      "step 20600, training accuracy 0.959596, cost 10.0611, change in cost 0.000330925\n",
      "step 20610, training accuracy 0.959596, cost 10.0607, change in cost 0.000332832\n",
      "step 20620, training accuracy 0.959596, cost 10.0604, change in cost 0.000331879\n",
      "step 20630, training accuracy 0.959596, cost 10.0601, change in cost 0.000333786\n",
      "step 20640, training accuracy 0.959596, cost 10.0597, change in cost 0.000330925\n",
      "step 20650, training accuracy 0.959596, cost 10.0594, change in cost 0.000330925\n",
      "step 20660, training accuracy 0.959596, cost 10.0591, change in cost 0.000329971\n",
      "step 20670, training accuracy 0.959596, cost 10.0587, change in cost 0.000331879\n",
      "step 20680, training accuracy 0.959596, cost 10.0584, change in cost 0.000329018\n",
      "step 20690, training accuracy 0.959596, cost 10.0581, change in cost 0.000329971\n",
      "step 20700, training accuracy 0.959596, cost 10.0578, change in cost 0.000332832\n",
      "step 20710, training accuracy 0.959596, cost 10.0574, change in cost 0.000329018\n",
      "step 20720, training accuracy 0.959596, cost 10.0571, change in cost 0.000330925\n",
      "step 20730, training accuracy 0.959596, cost 10.0568, change in cost 0.000328064\n",
      "step 20740, training accuracy 0.959596, cost 10.0564, change in cost 0.000329018\n",
      "step 20750, training accuracy 0.959596, cost 10.0561, change in cost 0.000329018\n",
      "step 20760, training accuracy 0.959596, cost 10.0558, change in cost 0.000329018\n",
      "step 20770, training accuracy 0.959596, cost 10.0554, change in cost 0.000329018\n",
      "step 20780, training accuracy 0.959596, cost 10.0551, change in cost 0.000328064\n",
      "step 20790, training accuracy 0.959596, cost 10.0548, change in cost 0.000328064\n",
      "step 20800, training accuracy 0.959596, cost 10.0545, change in cost 0.000329971\n",
      "step 20810, training accuracy 0.959596, cost 10.0541, change in cost 0.000326157\n",
      "step 20820, training accuracy 0.959596, cost 10.0538, change in cost 0.000328064\n",
      "step 20830, training accuracy 0.959596, cost 10.0535, change in cost 0.00032711\n",
      "step 20840, training accuracy 0.959596, cost 10.0532, change in cost 0.00032711\n",
      "step 20850, training accuracy 0.959596, cost 10.0528, change in cost 0.000326157\n",
      "step 20860, training accuracy 0.959596, cost 10.0525, change in cost 0.00032711\n",
      "step 20870, training accuracy 0.959596, cost 10.0522, change in cost 0.000326157\n",
      "step 20880, training accuracy 0.959596, cost 10.0518, change in cost 0.00032711\n",
      "step 20890, training accuracy 0.959596, cost 10.0515, change in cost 0.000326157\n",
      "step 20900, training accuracy 0.959596, cost 10.0512, change in cost 0.000326157\n",
      "step 20910, training accuracy 0.959596, cost 10.0509, change in cost 0.000325203\n",
      "step 20920, training accuracy 0.959596, cost 10.0505, change in cost 0.000323296\n",
      "step 20930, training accuracy 0.959596, cost 10.0502, change in cost 0.00032711\n",
      "step 20940, training accuracy 0.959596, cost 10.0499, change in cost 0.000323296\n",
      "step 20950, training accuracy 0.959596, cost 10.0496, change in cost 0.00032711\n",
      "step 20960, training accuracy 0.959596, cost 10.0492, change in cost 0.000323296\n",
      "step 20970, training accuracy 0.959596, cost 10.0489, change in cost 0.000324249\n",
      "step 20980, training accuracy 0.959596, cost 10.0486, change in cost 0.000325203\n",
      "step 20990, training accuracy 0.959596, cost 10.0483, change in cost 0.000321388\n",
      "step 21000, training accuracy 0.959596, cost 10.048, change in cost 0.000324249\n",
      "step 21010, training accuracy 0.959596, cost 10.0476, change in cost 0.000323296\n",
      "step 21020, training accuracy 0.959596, cost 10.0473, change in cost 0.000323296\n",
      "step 21030, training accuracy 0.959596, cost 10.047, change in cost 0.000322342\n",
      "step 21040, training accuracy 0.959596, cost 10.0467, change in cost 0.000324249\n",
      "step 21050, training accuracy 0.959596, cost 10.0463, change in cost 0.000322342\n",
      "step 21060, training accuracy 0.959596, cost 10.046, change in cost 0.000323296\n",
      "step 21070, training accuracy 0.959596, cost 10.0457, change in cost 0.000321388\n",
      "step 21080, training accuracy 0.959596, cost 10.0454, change in cost 0.000321388\n",
      "step 21090, training accuracy 0.959596, cost 10.045, change in cost 0.000321388\n",
      "step 21100, training accuracy 0.959596, cost 10.0447, change in cost 0.000322342\n",
      "step 21110, training accuracy 0.959596, cost 10.0444, change in cost 0.000320435\n",
      "step 21120, training accuracy 0.959596, cost 10.0441, change in cost 0.000320435\n",
      "step 21130, training accuracy 0.959596, cost 10.0438, change in cost 0.000321388\n",
      "step 21140, training accuracy 0.959596, cost 10.0434, change in cost 0.000321388\n",
      "step 21150, training accuracy 0.959596, cost 10.0431, change in cost 0.000320435\n",
      "step 21160, training accuracy 0.959596, cost 10.0428, change in cost 0.000320435\n",
      "step 21170, training accuracy 0.959596, cost 10.0425, change in cost 0.000319481\n",
      "step 21180, training accuracy 0.959596, cost 10.0422, change in cost 0.000318527\n",
      "step 21190, training accuracy 0.959596, cost 10.0418, change in cost 0.000319481\n",
      "step 21200, training accuracy 0.959596, cost 10.0415, change in cost 0.000320435\n",
      "step 21210, training accuracy 0.959596, cost 10.0412, change in cost 0.000320435\n",
      "step 21220, training accuracy 0.959596, cost 10.0409, change in cost 0.000317574\n",
      "step 21230, training accuracy 0.959596, cost 10.0406, change in cost 0.000318527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21240, training accuracy 0.959596, cost 10.0402, change in cost 0.000319481\n",
      "step 21250, training accuracy 0.959596, cost 10.0399, change in cost 0.000317574\n",
      "step 21260, training accuracy 0.959596, cost 10.0396, change in cost 0.000318527\n",
      "step 21270, training accuracy 0.959596, cost 10.0393, change in cost 0.000315666\n",
      "step 21280, training accuracy 0.959596, cost 10.039, change in cost 0.000319481\n",
      "step 21290, training accuracy 0.959596, cost 10.0387, change in cost 0.000317574\n",
      "step 21300, training accuracy 0.959596, cost 10.0383, change in cost 0.000317574\n",
      "step 21310, training accuracy 0.959596, cost 10.038, change in cost 0.00031662\n",
      "step 21320, training accuracy 0.959596, cost 10.0377, change in cost 0.000315666\n",
      "step 21330, training accuracy 0.959596, cost 10.0374, change in cost 0.00031662\n",
      "step 21340, training accuracy 0.959596, cost 10.0371, change in cost 0.00031662\n",
      "step 21350, training accuracy 0.959596, cost 10.0368, change in cost 0.000317574\n",
      "step 21360, training accuracy 0.959596, cost 10.0364, change in cost 0.000314713\n",
      "step 21370, training accuracy 0.959596, cost 10.0361, change in cost 0.000315666\n",
      "step 21380, training accuracy 0.959596, cost 10.0358, change in cost 0.000317574\n",
      "step 21390, training accuracy 0.959596, cost 10.0355, change in cost 0.000313759\n",
      "step 21400, training accuracy 0.959596, cost 10.0352, change in cost 0.000315666\n",
      "step 21410, training accuracy 0.959596, cost 10.0349, change in cost 0.000312805\n",
      "step 21420, training accuracy 0.959596, cost 10.0345, change in cost 0.000317574\n",
      "step 21430, training accuracy 0.959596, cost 10.0342, change in cost 0.000314713\n",
      "step 21440, training accuracy 0.959596, cost 10.0339, change in cost 0.000312805\n",
      "step 21450, training accuracy 0.959596, cost 10.0336, change in cost 0.000315666\n",
      "step 21460, training accuracy 0.959596, cost 10.0333, change in cost 0.000313759\n",
      "step 21470, training accuracy 0.959596, cost 10.033, change in cost 0.000314713\n",
      "step 21480, training accuracy 0.959596, cost 10.0327, change in cost 0.000312805\n",
      "step 21490, training accuracy 0.959596, cost 10.0324, change in cost 0.000313759\n",
      "step 21500, training accuracy 0.959596, cost 10.032, change in cost 0.000311852\n",
      "step 21510, training accuracy 0.959596, cost 10.0317, change in cost 0.000312805\n",
      "step 21520, training accuracy 0.959596, cost 10.0314, change in cost 0.000312805\n",
      "step 21530, training accuracy 0.959596, cost 10.0311, change in cost 0.000311852\n",
      "step 21540, training accuracy 0.959596, cost 10.0308, change in cost 0.000312805\n",
      "step 21550, training accuracy 0.959596, cost 10.0305, change in cost 0.000311852\n",
      "step 21560, training accuracy 0.959596, cost 10.0302, change in cost 0.000311852\n",
      "step 21570, training accuracy 0.959596, cost 10.0299, change in cost 0.000311852\n",
      "step 21580, training accuracy 0.959596, cost 10.0295, change in cost 0.000313759\n",
      "step 21590, training accuracy 0.959596, cost 10.0292, change in cost 0.000309944\n",
      "step 21600, training accuracy 0.959596, cost 10.0289, change in cost 0.000310898\n",
      "step 21610, training accuracy 0.959596, cost 10.0286, change in cost 0.000309944\n",
      "step 21620, training accuracy 0.959596, cost 10.0283, change in cost 0.000311852\n",
      "step 21630, training accuracy 0.959596, cost 10.028, change in cost 0.00030899\n",
      "step 21640, training accuracy 0.959596, cost 10.0277, change in cost 0.000310898\n",
      "step 21650, training accuracy 0.959596, cost 10.0274, change in cost 0.000310898\n",
      "step 21660, training accuracy 0.959596, cost 10.0271, change in cost 0.000309944\n",
      "step 21670, training accuracy 0.959596, cost 10.0267, change in cost 0.000308037\n",
      "step 21680, training accuracy 0.959596, cost 10.0264, change in cost 0.000309944\n",
      "step 21690, training accuracy 0.959596, cost 10.0261, change in cost 0.00030899\n",
      "step 21700, training accuracy 0.959596, cost 10.0258, change in cost 0.000309944\n",
      "step 21710, training accuracy 0.959596, cost 10.0255, change in cost 0.000307083\n",
      "step 21720, training accuracy 0.959596, cost 10.0252, change in cost 0.000308037\n",
      "step 21730, training accuracy 0.959596, cost 10.0249, change in cost 0.000310898\n",
      "step 21740, training accuracy 0.959596, cost 10.0246, change in cost 0.00030899\n",
      "step 21750, training accuracy 0.959596, cost 10.0243, change in cost 0.000307083\n",
      "step 21760, training accuracy 0.959596, cost 10.024, change in cost 0.000308037\n",
      "step 21770, training accuracy 0.959596, cost 10.0237, change in cost 0.000308037\n",
      "step 21780, training accuracy 0.959596, cost 10.0234, change in cost 0.000307083\n",
      "step 21790, training accuracy 0.959596, cost 10.023, change in cost 0.000308037\n",
      "step 21800, training accuracy 0.959596, cost 10.0227, change in cost 0.000308037\n",
      "step 21810, training accuracy 0.959596, cost 10.0224, change in cost 0.000306129\n",
      "step 21820, training accuracy 0.959596, cost 10.0221, change in cost 0.000306129\n",
      "step 21830, training accuracy 0.959596, cost 10.0218, change in cost 0.000307083\n",
      "step 21840, training accuracy 0.959596, cost 10.0215, change in cost 0.000305176\n",
      "step 21850, training accuracy 0.959596, cost 10.0212, change in cost 0.000307083\n",
      "step 21860, training accuracy 0.959596, cost 10.0209, change in cost 0.000304222\n",
      "step 21870, training accuracy 0.959596, cost 10.0206, change in cost 0.000305176\n",
      "step 21880, training accuracy 0.959596, cost 10.0203, change in cost 0.000306129\n",
      "step 21890, training accuracy 0.959596, cost 10.02, change in cost 0.000305176\n",
      "step 21900, training accuracy 0.959596, cost 10.0197, change in cost 0.000304222\n",
      "step 21910, training accuracy 0.959596, cost 10.0194, change in cost 0.000306129\n",
      "step 21920, training accuracy 0.959596, cost 10.0191, change in cost 0.000305176\n",
      "step 21930, training accuracy 0.959596, cost 10.0188, change in cost 0.000304222\n",
      "step 21940, training accuracy 0.959596, cost 10.0185, change in cost 0.000304222\n",
      "step 21950, training accuracy 0.959596, cost 10.0182, change in cost 0.000302315\n",
      "step 21960, training accuracy 0.959596, cost 10.0179, change in cost 0.000305176\n",
      "step 21970, training accuracy 0.959596, cost 10.0176, change in cost 0.000303268\n",
      "step 21980, training accuracy 0.959596, cost 10.0172, change in cost 0.000303268\n",
      "step 21990, training accuracy 0.959596, cost 10.0169, change in cost 0.000304222\n",
      "step 22000, training accuracy 0.959596, cost 10.0166, change in cost 0.000304222\n",
      "step 22010, training accuracy 0.959596, cost 10.0163, change in cost 0.000301361\n",
      "step 22020, training accuracy 0.959596, cost 10.016, change in cost 0.000302315\n",
      "step 22030, training accuracy 0.959596, cost 10.0157, change in cost 0.000302315\n",
      "step 22040, training accuracy 0.959596, cost 10.0154, change in cost 0.000302315\n",
      "step 22050, training accuracy 0.959596, cost 10.0151, change in cost 0.000303268\n",
      "step 22060, training accuracy 0.959596, cost 10.0148, change in cost 0.000301361\n",
      "step 22070, training accuracy 0.959596, cost 10.0145, change in cost 0.000303268\n",
      "step 22080, training accuracy 0.959596, cost 10.0142, change in cost 0.000299454\n",
      "step 22090, training accuracy 0.959596, cost 10.0139, change in cost 0.000303268\n",
      "step 22100, training accuracy 0.959596, cost 10.0136, change in cost 0.000300407\n",
      "step 22110, training accuracy 0.959596, cost 10.0133, change in cost 0.000300407\n",
      "step 22120, training accuracy 0.959596, cost 10.013, change in cost 0.000300407\n",
      "step 22130, training accuracy 0.959596, cost 10.0127, change in cost 0.000301361\n",
      "step 22140, training accuracy 0.959596, cost 10.0124, change in cost 0.000301361\n",
      "step 22150, training accuracy 0.959596, cost 10.0121, change in cost 0.000299454\n",
      "step 22160, training accuracy 0.959596, cost 10.0118, change in cost 0.000300407\n",
      "step 22170, training accuracy 0.959596, cost 10.0115, change in cost 0.000300407\n",
      "step 22180, training accuracy 0.959596, cost 10.0112, change in cost 0.000300407\n",
      "step 22190, training accuracy 0.959596, cost 10.0109, change in cost 0.000297546\n",
      "step 22200, training accuracy 0.959596, cost 10.0106, change in cost 0.0002985\n",
      "step 22210, training accuracy 0.959596, cost 10.0103, change in cost 0.000300407\n",
      "step 22220, training accuracy 0.959596, cost 10.01, change in cost 0.000297546\n",
      "step 22230, training accuracy 0.959596, cost 10.0097, change in cost 0.0002985\n",
      "step 22240, training accuracy 0.959596, cost 10.0094, change in cost 0.000299454\n",
      "step 22250, training accuracy 0.959596, cost 10.0091, change in cost 0.000297546\n",
      "step 22260, training accuracy 0.969697, cost 10.0088, change in cost 0.000297546\n",
      "step 22270, training accuracy 0.969697, cost 10.0085, change in cost 0.0002985\n",
      "step 22280, training accuracy 0.969697, cost 10.0082, change in cost 0.000297546\n",
      "step 22290, training accuracy 0.969697, cost 10.0079, change in cost 0.000296593\n",
      "step 22300, training accuracy 0.969697, cost 10.0076, change in cost 0.000297546\n",
      "step 22310, training accuracy 0.969697, cost 10.0073, change in cost 0.0002985\n",
      "step 22320, training accuracy 0.969697, cost 10.007, change in cost 0.000296593\n",
      "step 22330, training accuracy 0.969697, cost 10.0067, change in cost 0.0002985\n",
      "step 22340, training accuracy 0.969697, cost 10.0065, change in cost 0.000294685\n",
      "step 22350, training accuracy 0.969697, cost 10.0062, change in cost 0.000296593\n",
      "step 22360, training accuracy 0.969697, cost 10.0059, change in cost 0.000296593\n",
      "step 22370, training accuracy 0.969697, cost 10.0056, change in cost 0.000296593\n",
      "step 22380, training accuracy 0.969697, cost 10.0053, change in cost 0.000295639\n",
      "step 22390, training accuracy 0.969697, cost 10.005, change in cost 0.000294685\n",
      "step 22400, training accuracy 0.969697, cost 10.0047, change in cost 0.000296593\n",
      "step 22410, training accuracy 0.969697, cost 10.0044, change in cost 0.000295639\n",
      "step 22420, training accuracy 0.969697, cost 10.0041, change in cost 0.000293732\n",
      "step 22430, training accuracy 0.969697, cost 10.0038, change in cost 0.000294685\n",
      "step 22440, training accuracy 0.969697, cost 10.0035, change in cost 0.000294685\n",
      "step 22450, training accuracy 0.969697, cost 10.0032, change in cost 0.000293732\n",
      "step 22460, training accuracy 0.969697, cost 10.0029, change in cost 0.000295639\n",
      "step 22470, training accuracy 0.969697, cost 10.0026, change in cost 0.000293732\n",
      "step 22480, training accuracy 0.969697, cost 10.0023, change in cost 0.000293732\n",
      "step 22490, training accuracy 0.969697, cost 10.002, change in cost 0.000293732\n",
      "step 22500, training accuracy 0.969697, cost 10.0017, change in cost 0.000293732\n",
      "step 22510, training accuracy 0.969697, cost 10.0014, change in cost 0.000293732\n",
      "step 22520, training accuracy 0.969697, cost 10.0011, change in cost 0.000292778\n",
      "step 22530, training accuracy 0.969697, cost 10.0008, change in cost 0.000294685\n",
      "step 22540, training accuracy 0.969697, cost 10.0006, change in cost 0.000292778\n",
      "step 22550, training accuracy 0.969697, cost 10.0003, change in cost 0.000291824\n",
      "step 22560, training accuracy 0.969697, cost 9.99997, change in cost 0.000292778\n",
      "step 22570, training accuracy 0.969697, cost 9.99968, change in cost 0.000292778\n",
      "step 22580, training accuracy 0.969697, cost 9.99939, change in cost 0.000292778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22590, training accuracy 0.969697, cost 9.9991, change in cost 0.000289917\n",
      "step 22600, training accuracy 0.969697, cost 9.9988, change in cost 0.000292778\n",
      "step 22610, training accuracy 0.969697, cost 9.99851, change in cost 0.000289917\n",
      "step 22620, training accuracy 0.969697, cost 9.99822, change in cost 0.000292778\n",
      "step 22630, training accuracy 0.969697, cost 9.99793, change in cost 0.000290871\n",
      "step 22640, training accuracy 0.969697, cost 9.99764, change in cost 0.000290871\n",
      "step 22650, training accuracy 0.969697, cost 9.99735, change in cost 0.000289917\n",
      "step 22660, training accuracy 0.969697, cost 9.99706, change in cost 0.000290871\n",
      "step 22670, training accuracy 0.969697, cost 9.99677, change in cost 0.000289917\n",
      "step 22680, training accuracy 0.969697, cost 9.99648, change in cost 0.000290871\n",
      "step 22690, training accuracy 0.969697, cost 9.99619, change in cost 0.000290871\n",
      "step 22700, training accuracy 0.969697, cost 9.9959, change in cost 0.000289917\n",
      "step 22710, training accuracy 0.969697, cost 9.99561, change in cost 0.000288963\n",
      "step 22720, training accuracy 0.969697, cost 9.99532, change in cost 0.000290871\n",
      "step 22730, training accuracy 0.969697, cost 9.99503, change in cost 0.00028801\n",
      "step 22740, training accuracy 0.969697, cost 9.99474, change in cost 0.000290871\n",
      "step 22750, training accuracy 0.969697, cost 9.99445, change in cost 0.000288963\n",
      "step 22760, training accuracy 0.969697, cost 9.99416, change in cost 0.00028801\n",
      "step 22770, training accuracy 0.969697, cost 9.99387, change in cost 0.000289917\n",
      "step 22780, training accuracy 0.969697, cost 9.99358, change in cost 0.00028801\n",
      "step 22790, training accuracy 0.969697, cost 9.9933, change in cost 0.00028801\n",
      "step 22800, training accuracy 0.969697, cost 9.99301, change in cost 0.000287056\n",
      "step 22810, training accuracy 0.969697, cost 9.99272, change in cost 0.000288963\n",
      "step 22820, training accuracy 0.969697, cost 9.99243, change in cost 0.000287056\n",
      "step 22830, training accuracy 0.969697, cost 9.99215, change in cost 0.000287056\n",
      "step 22840, training accuracy 0.969697, cost 9.99186, change in cost 0.00028801\n",
      "step 22850, training accuracy 0.969697, cost 9.99157, change in cost 0.000286102\n",
      "step 22860, training accuracy 0.969697, cost 9.99128, change in cost 0.00028801\n",
      "step 22870, training accuracy 0.969697, cost 9.991, change in cost 0.00028801\n",
      "step 22880, training accuracy 0.969697, cost 9.99071, change in cost 0.000284195\n",
      "step 22890, training accuracy 0.969697, cost 9.99042, change in cost 0.000287056\n",
      "step 22900, training accuracy 0.969697, cost 9.99014, change in cost 0.000286102\n",
      "step 22910, training accuracy 0.969697, cost 9.98985, change in cost 0.000286102\n",
      "step 22920, training accuracy 0.969697, cost 9.98957, change in cost 0.000286102\n",
      "step 22930, training accuracy 0.969697, cost 9.98928, change in cost 0.000286102\n",
      "step 22940, training accuracy 0.969697, cost 9.98899, change in cost 0.000286102\n",
      "step 22950, training accuracy 0.969697, cost 9.98871, change in cost 0.000285149\n",
      "step 22960, training accuracy 0.969697, cost 9.98842, change in cost 0.000285149\n",
      "step 22970, training accuracy 0.969697, cost 9.98814, change in cost 0.000287056\n",
      "step 22980, training accuracy 0.969697, cost 9.98785, change in cost 0.000284195\n",
      "step 22990, training accuracy 0.969697, cost 9.98757, change in cost 0.000283241\n",
      "step 23000, training accuracy 0.969697, cost 9.98729, change in cost 0.000283241\n",
      "step 23010, training accuracy 0.969697, cost 9.987, change in cost 0.000287056\n",
      "step 23020, training accuracy 0.969697, cost 9.98672, change in cost 0.000282288\n",
      "step 23030, training accuracy 0.969697, cost 9.98643, change in cost 0.000283241\n",
      "step 23040, training accuracy 0.969697, cost 9.98615, change in cost 0.000285149\n",
      "step 23050, training accuracy 0.969697, cost 9.98586, change in cost 0.000283241\n",
      "step 23060, training accuracy 0.969697, cost 9.98558, change in cost 0.000284195\n",
      "step 23070, training accuracy 0.969697, cost 9.9853, change in cost 0.000282288\n",
      "step 23080, training accuracy 0.969697, cost 9.98501, change in cost 0.000283241\n",
      "step 23090, training accuracy 0.969697, cost 9.98473, change in cost 0.000283241\n",
      "step 23100, training accuracy 0.969697, cost 9.98445, change in cost 0.000281334\n",
      "step 23110, training accuracy 0.969697, cost 9.98417, change in cost 0.000282288\n",
      "step 23120, training accuracy 0.969697, cost 9.98388, change in cost 0.000284195\n",
      "step 23130, training accuracy 0.969697, cost 9.9836, change in cost 0.000282288\n",
      "step 23140, training accuracy 0.969697, cost 9.98332, change in cost 0.000282288\n",
      "step 23150, training accuracy 0.969697, cost 9.98304, change in cost 0.000281334\n",
      "step 23160, training accuracy 0.969697, cost 9.98276, change in cost 0.000281334\n",
      "step 23170, training accuracy 0.969697, cost 9.98247, change in cost 0.000282288\n",
      "step 23180, training accuracy 0.969697, cost 9.98219, change in cost 0.00028038\n",
      "step 23190, training accuracy 0.969697, cost 9.98191, change in cost 0.000281334\n",
      "step 23200, training accuracy 0.969697, cost 9.98163, change in cost 0.000281334\n",
      "step 23210, training accuracy 0.969697, cost 9.98135, change in cost 0.00028038\n",
      "step 23220, training accuracy 0.969697, cost 9.98107, change in cost 0.00028038\n",
      "step 23230, training accuracy 0.969697, cost 9.98079, change in cost 0.000281334\n",
      "step 23240, training accuracy 0.969697, cost 9.98051, change in cost 0.000279427\n",
      "step 23250, training accuracy 0.969697, cost 9.98023, change in cost 0.000281334\n",
      "step 23260, training accuracy 0.969697, cost 9.97995, change in cost 0.000278473\n",
      "step 23270, training accuracy 0.969697, cost 9.97967, change in cost 0.000279427\n",
      "step 23280, training accuracy 0.969697, cost 9.97939, change in cost 0.000281334\n",
      "step 23290, training accuracy 0.969697, cost 9.97911, change in cost 0.000277519\n",
      "step 23300, training accuracy 0.969697, cost 9.97883, change in cost 0.000279427\n",
      "step 23310, training accuracy 0.969697, cost 9.97855, change in cost 0.00028038\n",
      "step 23320, training accuracy 0.969697, cost 9.97827, change in cost 0.000278473\n",
      "step 23330, training accuracy 0.969697, cost 9.97799, change in cost 0.000279427\n",
      "step 23340, training accuracy 0.969697, cost 9.97772, change in cost 0.000278473\n",
      "step 23350, training accuracy 0.969697, cost 9.97744, change in cost 0.000278473\n",
      "step 23360, training accuracy 0.969697, cost 9.97716, change in cost 0.000279427\n",
      "step 23370, training accuracy 0.969697, cost 9.97688, change in cost 0.000275612\n",
      "step 23380, training accuracy 0.969697, cost 9.9766, change in cost 0.000279427\n",
      "step 23390, training accuracy 0.969697, cost 9.97633, change in cost 0.000277519\n",
      "step 23400, training accuracy 0.969697, cost 9.97605, change in cost 0.000277519\n",
      "step 23410, training accuracy 0.969697, cost 9.97577, change in cost 0.000278473\n",
      "step 23420, training accuracy 0.969697, cost 9.97549, change in cost 0.000276566\n",
      "step 23430, training accuracy 0.969697, cost 9.97522, change in cost 0.000275612\n",
      "step 23440, training accuracy 0.969697, cost 9.97494, change in cost 0.000277519\n",
      "step 23450, training accuracy 0.969697, cost 9.97466, change in cost 0.000276566\n",
      "step 23460, training accuracy 0.969697, cost 9.97439, change in cost 0.000275612\n",
      "step 23470, training accuracy 0.969697, cost 9.97411, change in cost 0.000275612\n",
      "step 23480, training accuracy 0.969697, cost 9.97383, change in cost 0.000276566\n",
      "step 23490, training accuracy 0.969697, cost 9.97356, change in cost 0.000275612\n",
      "step 23500, training accuracy 0.969697, cost 9.97328, change in cost 0.000276566\n",
      "step 23510, training accuracy 0.969697, cost 9.97301, change in cost 0.000275612\n",
      "step 23520, training accuracy 0.969697, cost 9.97273, change in cost 0.000276566\n",
      "step 23530, training accuracy 0.969697, cost 9.97246, change in cost 0.000274658\n",
      "step 23540, training accuracy 0.969697, cost 9.97218, change in cost 0.000275612\n",
      "step 23550, training accuracy 0.969697, cost 9.9719, change in cost 0.000275612\n",
      "step 23560, training accuracy 0.969697, cost 9.97163, change in cost 0.000275612\n",
      "step 23570, training accuracy 0.969697, cost 9.97135, change in cost 0.000274658\n",
      "step 23580, training accuracy 0.969697, cost 9.97108, change in cost 0.000273705\n",
      "step 23590, training accuracy 0.969697, cost 9.97081, change in cost 0.000275612\n",
      "step 23600, training accuracy 0.969697, cost 9.97053, change in cost 0.000274658\n",
      "step 23610, training accuracy 0.969697, cost 9.97026, change in cost 0.000271797\n",
      "step 23620, training accuracy 0.969697, cost 9.96998, change in cost 0.000274658\n",
      "step 23630, training accuracy 0.969697, cost 9.96971, change in cost 0.000272751\n",
      "step 23640, training accuracy 0.969697, cost 9.96944, change in cost 0.000273705\n",
      "step 23650, training accuracy 0.969697, cost 9.96916, change in cost 0.000272751\n",
      "step 23660, training accuracy 0.969697, cost 9.96889, change in cost 0.000273705\n",
      "step 23670, training accuracy 0.969697, cost 9.96862, change in cost 0.000272751\n",
      "step 23680, training accuracy 0.969697, cost 9.96835, change in cost 0.000272751\n",
      "step 23690, training accuracy 0.969697, cost 9.96807, change in cost 0.000273705\n",
      "step 23700, training accuracy 0.969697, cost 9.9678, change in cost 0.000270844\n",
      "step 23710, training accuracy 0.969697, cost 9.96753, change in cost 0.000273705\n",
      "step 23720, training accuracy 0.969697, cost 9.96725, change in cost 0.000273705\n",
      "step 23730, training accuracy 0.969697, cost 9.96698, change in cost 0.000270844\n",
      "step 23740, training accuracy 0.969697, cost 9.96671, change in cost 0.000271797\n",
      "step 23750, training accuracy 0.969697, cost 9.96644, change in cost 0.000272751\n",
      "step 23760, training accuracy 0.969697, cost 9.96617, change in cost 0.00026989\n",
      "step 23770, training accuracy 0.969697, cost 9.9659, change in cost 0.000270844\n",
      "step 23780, training accuracy 0.969697, cost 9.96563, change in cost 0.000271797\n",
      "step 23790, training accuracy 0.969697, cost 9.96535, change in cost 0.000271797\n",
      "step 23800, training accuracy 0.969697, cost 9.96508, change in cost 0.000270844\n",
      "step 23810, training accuracy 0.969697, cost 9.96481, change in cost 0.000270844\n",
      "step 23820, training accuracy 0.969697, cost 9.96454, change in cost 0.00026989\n",
      "step 23830, training accuracy 0.969697, cost 9.96427, change in cost 0.000270844\n",
      "step 23840, training accuracy 0.969697, cost 9.964, change in cost 0.000268936\n",
      "step 23850, training accuracy 0.969697, cost 9.96373, change in cost 0.000270844\n",
      "step 23860, training accuracy 0.969697, cost 9.96346, change in cost 0.000270844\n",
      "step 23870, training accuracy 0.969697, cost 9.96319, change in cost 0.000268936\n",
      "step 23880, training accuracy 0.969697, cost 9.96292, change in cost 0.000268936\n",
      "step 23890, training accuracy 0.969697, cost 9.96265, change in cost 0.00026989\n",
      "step 23900, training accuracy 0.969697, cost 9.96238, change in cost 0.000268936\n",
      "step 23910, training accuracy 0.969697, cost 9.96212, change in cost 0.000268936\n",
      "step 23920, training accuracy 0.969697, cost 9.96185, change in cost 0.000267982\n",
      "step 23930, training accuracy 0.969697, cost 9.96158, change in cost 0.00026989\n",
      "step 23940, training accuracy 0.969697, cost 9.96131, change in cost 0.000267982\n",
      "step 23950, training accuracy 0.969697, cost 9.96104, change in cost 0.000268936\n",
      "step 23960, training accuracy 0.969697, cost 9.96077, change in cost 0.000267982\n",
      "step 23970, training accuracy 0.969697, cost 9.9605, change in cost 0.000267982\n",
      "step 23980, training accuracy 0.969697, cost 9.96024, change in cost 0.000267029\n",
      "step 23990, training accuracy 0.969697, cost 9.95997, change in cost 0.000268936\n",
      "step 24000, training accuracy 0.969697, cost 9.9597, change in cost 0.000268936\n",
      "step 24010, training accuracy 0.969697, cost 9.95943, change in cost 0.000267029\n",
      "step 24020, training accuracy 0.969697, cost 9.95917, change in cost 0.000267029\n",
      "step 24030, training accuracy 0.969697, cost 9.9589, change in cost 0.000266075\n",
      "step 24040, training accuracy 0.969697, cost 9.95863, change in cost 0.000267982\n",
      "step 24050, training accuracy 0.969697, cost 9.95836, change in cost 0.000267029\n",
      "step 24060, training accuracy 0.969697, cost 9.9581, change in cost 0.000265121\n",
      "step 24070, training accuracy 0.969697, cost 9.95783, change in cost 0.000266075\n",
      "step 24080, training accuracy 0.969697, cost 9.95757, change in cost 0.000267982\n",
      "step 24090, training accuracy 0.969697, cost 9.9573, change in cost 0.000265121\n",
      "step 24100, training accuracy 0.969697, cost 9.95703, change in cost 0.000266075\n",
      "step 24110, training accuracy 0.969697, cost 9.95677, change in cost 0.000265121\n",
      "step 24120, training accuracy 0.969697, cost 9.9565, change in cost 0.000267029\n",
      "step 24130, training accuracy 0.969697, cost 9.95624, change in cost 0.000265121\n",
      "step 24140, training accuracy 0.969697, cost 9.95597, change in cost 0.000265121\n",
      "step 24150, training accuracy 0.969697, cost 9.95571, change in cost 0.000265121\n",
      "step 24160, training accuracy 0.969697, cost 9.95544, change in cost 0.000264168\n",
      "step 24170, training accuracy 0.969697, cost 9.95518, change in cost 0.000265121\n",
      "step 24180, training accuracy 0.969697, cost 9.95491, change in cost 0.000264168\n",
      "step 24190, training accuracy 0.969697, cost 9.95465, change in cost 0.000265121\n",
      "step 24200, training accuracy 0.969697, cost 9.95438, change in cost 0.000263214\n",
      "step 24210, training accuracy 0.969697, cost 9.95412, change in cost 0.000265121\n",
      "step 24220, training accuracy 0.969697, cost 9.95386, change in cost 0.000263214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24230, training accuracy 0.969697, cost 9.95359, change in cost 0.000266075\n",
      "step 24240, training accuracy 0.969697, cost 9.95333, change in cost 0.000264168\n",
      "step 24250, training accuracy 0.969697, cost 9.95306, change in cost 0.00026226\n",
      "step 24260, training accuracy 0.969697, cost 9.9528, change in cost 0.000263214\n",
      "step 24270, training accuracy 0.969697, cost 9.95254, change in cost 0.000263214\n",
      "step 24280, training accuracy 0.969697, cost 9.95227, change in cost 0.000263214\n",
      "step 24290, training accuracy 0.969697, cost 9.95201, change in cost 0.00026226\n",
      "step 24300, training accuracy 0.969697, cost 9.95175, change in cost 0.000264168\n",
      "step 24310, training accuracy 0.969697, cost 9.95149, change in cost 0.00026226\n",
      "step 24320, training accuracy 0.969697, cost 9.95122, change in cost 0.000261307\n",
      "step 24330, training accuracy 0.969697, cost 9.95096, change in cost 0.000264168\n",
      "step 24340, training accuracy 0.969697, cost 9.9507, change in cost 0.000260353\n",
      "step 24350, training accuracy 0.969697, cost 9.95044, change in cost 0.00026226\n",
      "step 24360, training accuracy 0.969697, cost 9.95018, change in cost 0.000260353\n",
      "step 24370, training accuracy 0.969697, cost 9.94991, change in cost 0.000264168\n",
      "step 24380, training accuracy 0.969697, cost 9.94965, change in cost 0.000259399\n",
      "step 24390, training accuracy 0.969697, cost 9.94939, change in cost 0.000263214\n",
      "step 24400, training accuracy 0.969697, cost 9.94913, change in cost 0.000261307\n",
      "step 24410, training accuracy 0.969697, cost 9.94887, change in cost 0.000261307\n",
      "step 24420, training accuracy 0.969697, cost 9.94861, change in cost 0.000260353\n",
      "step 24430, training accuracy 0.969697, cost 9.94835, change in cost 0.000261307\n",
      "step 24440, training accuracy 0.969697, cost 9.94809, change in cost 0.000260353\n",
      "step 24450, training accuracy 0.969697, cost 9.94783, change in cost 0.000259399\n",
      "step 24460, training accuracy 0.969697, cost 9.94756, change in cost 0.00026226\n",
      "step 24470, training accuracy 0.969697, cost 9.9473, change in cost 0.000259399\n",
      "step 24480, training accuracy 0.969697, cost 9.94704, change in cost 0.000260353\n",
      "step 24490, training accuracy 0.969697, cost 9.94678, change in cost 0.000260353\n",
      "step 24500, training accuracy 0.969697, cost 9.94652, change in cost 0.000259399\n",
      "step 24510, training accuracy 0.969697, cost 9.94627, change in cost 0.000259399\n",
      "step 24520, training accuracy 0.969697, cost 9.94601, change in cost 0.000258446\n",
      "step 24530, training accuracy 0.969697, cost 9.94575, change in cost 0.000258446\n",
      "step 24540, training accuracy 0.969697, cost 9.94549, change in cost 0.000258446\n",
      "step 24550, training accuracy 0.969697, cost 9.94523, change in cost 0.000259399\n",
      "step 24560, training accuracy 0.969697, cost 9.94497, change in cost 0.000259399\n",
      "step 24570, training accuracy 0.969697, cost 9.94471, change in cost 0.000258446\n",
      "step 24580, training accuracy 0.969697, cost 9.94446, change in cost 0.000257492\n",
      "step 24590, training accuracy 0.969697, cost 9.9442, change in cost 0.000259399\n",
      "step 24600, training accuracy 0.969697, cost 9.94394, change in cost 0.000258446\n",
      "step 24610, training accuracy 0.969697, cost 9.94368, change in cost 0.000258446\n",
      "step 24620, training accuracy 0.969697, cost 9.94342, change in cost 0.000256538\n",
      "step 24630, training accuracy 0.969697, cost 9.94316, change in cost 0.000257492\n",
      "step 24640, training accuracy 0.969697, cost 9.94291, change in cost 0.000258446\n",
      "step 24650, training accuracy 0.969697, cost 9.94265, change in cost 0.000256538\n",
      "step 24660, training accuracy 0.969697, cost 9.94239, change in cost 0.000257492\n",
      "step 24670, training accuracy 0.969697, cost 9.94213, change in cost 0.000257492\n",
      "step 24680, training accuracy 0.969697, cost 9.94188, change in cost 0.000256538\n",
      "step 24690, training accuracy 0.969697, cost 9.94162, change in cost 0.000257492\n",
      "step 24700, training accuracy 0.969697, cost 9.94136, change in cost 0.000257492\n",
      "step 24710, training accuracy 0.969697, cost 9.94111, change in cost 0.000254631\n",
      "step 24720, training accuracy 0.969697, cost 9.94085, change in cost 0.000256538\n",
      "step 24730, training accuracy 0.969697, cost 9.94059, change in cost 0.000257492\n",
      "step 24740, training accuracy 0.969697, cost 9.94034, change in cost 0.000254631\n",
      "step 24750, training accuracy 0.969697, cost 9.94008, change in cost 0.000255585\n",
      "step 24760, training accuracy 0.969697, cost 9.93983, change in cost 0.000256538\n",
      "step 24770, training accuracy 0.969697, cost 9.93957, change in cost 0.000256538\n",
      "step 24780, training accuracy 0.969697, cost 9.93932, change in cost 0.000253677\n",
      "step 24790, training accuracy 0.969697, cost 9.93906, change in cost 0.000256538\n",
      "step 24800, training accuracy 0.969697, cost 9.93881, change in cost 0.000253677\n",
      "step 24810, training accuracy 0.969697, cost 9.93855, change in cost 0.000256538\n",
      "step 24820, training accuracy 0.969697, cost 9.9383, change in cost 0.000253677\n",
      "step 24830, training accuracy 0.969697, cost 9.93804, change in cost 0.000253677\n",
      "step 24840, training accuracy 0.969697, cost 9.93779, change in cost 0.000253677\n",
      "step 24850, training accuracy 0.969697, cost 9.93753, change in cost 0.000255585\n",
      "step 24860, training accuracy 0.969697, cost 9.93728, change in cost 0.000254631\n",
      "step 24870, training accuracy 0.969697, cost 9.93703, change in cost 0.00025177\n",
      "step 24880, training accuracy 0.969697, cost 9.93677, change in cost 0.000255585\n",
      "step 24890, training accuracy 0.969697, cost 9.93652, change in cost 0.000253677\n",
      "step 24900, training accuracy 0.969697, cost 9.93627, change in cost 0.000252724\n",
      "step 24910, training accuracy 0.969697, cost 9.93601, change in cost 0.000254631\n",
      "step 24920, training accuracy 0.969697, cost 9.93576, change in cost 0.00025177\n",
      "step 24930, training accuracy 0.969697, cost 9.9355, change in cost 0.000254631\n",
      "step 24940, training accuracy 0.969697, cost 9.93525, change in cost 0.000254631\n",
      "step 24950, training accuracy 0.969697, cost 9.935, change in cost 0.00025177\n",
      "step 24960, training accuracy 0.969697, cost 9.93475, change in cost 0.00025177\n",
      "step 24970, training accuracy 0.969697, cost 9.93449, change in cost 0.000252724\n",
      "step 24980, training accuracy 0.969697, cost 9.93424, change in cost 0.000252724\n",
      "step 24990, training accuracy 0.969697, cost 9.93399, change in cost 0.00025177\n",
      "step 25000, training accuracy 0.969697, cost 9.93374, change in cost 0.000252724\n",
      "step 25010, training accuracy 0.969697, cost 9.93349, change in cost 0.000249863\n",
      "step 25020, training accuracy 0.969697, cost 9.93324, change in cost 0.00025177\n",
      "step 25030, training accuracy 0.969697, cost 9.93298, change in cost 0.000252724\n",
      "step 25040, training accuracy 0.969697, cost 9.93273, change in cost 0.00025177\n",
      "step 25050, training accuracy 0.969697, cost 9.93248, change in cost 0.000249863\n",
      "step 25060, training accuracy 0.969697, cost 9.93223, change in cost 0.000253677\n",
      "step 25070, training accuracy 0.969697, cost 9.93198, change in cost 0.000249863\n",
      "step 25080, training accuracy 0.969697, cost 9.93173, change in cost 0.000250816\n",
      "step 25090, training accuracy 0.969697, cost 9.93148, change in cost 0.000249863\n",
      "step 25100, training accuracy 0.969697, cost 9.93122, change in cost 0.00025177\n",
      "step 25110, training accuracy 0.969697, cost 9.93098, change in cost 0.000248909\n",
      "step 25120, training accuracy 0.969697, cost 9.93073, change in cost 0.000250816\n",
      "step 25130, training accuracy 0.969697, cost 9.93047, change in cost 0.000250816\n",
      "step 25140, training accuracy 0.969697, cost 9.93022, change in cost 0.000249863\n",
      "step 25150, training accuracy 0.969697, cost 9.92997, change in cost 0.000249863\n",
      "step 25160, training accuracy 0.969697, cost 9.92972, change in cost 0.000250816\n",
      "step 25170, training accuracy 0.969697, cost 9.92947, change in cost 0.000248909\n",
      "step 25180, training accuracy 0.969697, cost 9.92922, change in cost 0.000250816\n",
      "step 25190, training accuracy 0.969697, cost 9.92898, change in cost 0.000247955\n",
      "step 25200, training accuracy 0.969697, cost 9.92873, change in cost 0.000249863\n",
      "step 25210, training accuracy 0.969697, cost 9.92848, change in cost 0.000247955\n",
      "step 25220, training accuracy 0.969697, cost 9.92823, change in cost 0.000248909\n",
      "step 25230, training accuracy 0.969697, cost 9.92798, change in cost 0.000248909\n",
      "step 25240, training accuracy 0.969697, cost 9.92773, change in cost 0.000248909\n",
      "step 25250, training accuracy 0.969697, cost 9.92748, change in cost 0.000247955\n",
      "step 25260, training accuracy 0.969697, cost 9.92723, change in cost 0.000248909\n",
      "step 25270, training accuracy 0.969697, cost 9.92699, change in cost 0.000247955\n",
      "step 25280, training accuracy 0.969697, cost 9.92674, change in cost 0.000249863\n",
      "step 25290, training accuracy 0.969697, cost 9.92649, change in cost 0.000245094\n",
      "step 25300, training accuracy 0.969697, cost 9.92624, change in cost 0.000249863\n",
      "step 25310, training accuracy 0.969697, cost 9.926, change in cost 0.000246048\n",
      "step 25320, training accuracy 0.969697, cost 9.92575, change in cost 0.000248909\n",
      "step 25330, training accuracy 0.969697, cost 9.9255, change in cost 0.000246048\n",
      "step 25340, training accuracy 0.969697, cost 9.92525, change in cost 0.000247002\n",
      "step 25350, training accuracy 0.969697, cost 9.92501, change in cost 0.000246048\n",
      "step 25360, training accuracy 0.969697, cost 9.92476, change in cost 0.000248909\n",
      "step 25370, training accuracy 0.969697, cost 9.92451, change in cost 0.000245094\n",
      "step 25380, training accuracy 0.969697, cost 9.92427, change in cost 0.000246048\n",
      "step 25390, training accuracy 0.969697, cost 9.92402, change in cost 0.000246048\n",
      "step 25400, training accuracy 0.969697, cost 9.92377, change in cost 0.000247002\n",
      "step 25410, training accuracy 0.969697, cost 9.92353, change in cost 0.000246048\n",
      "step 25420, training accuracy 0.969697, cost 9.92328, change in cost 0.000248909\n",
      "step 25430, training accuracy 0.969697, cost 9.92304, change in cost 0.000244141\n",
      "step 25440, training accuracy 0.969697, cost 9.92279, change in cost 0.000246048\n",
      "step 25450, training accuracy 0.969697, cost 9.92254, change in cost 0.000245094\n",
      "step 25460, training accuracy 0.969697, cost 9.9223, change in cost 0.000246048\n",
      "step 25470, training accuracy 0.969697, cost 9.92205, change in cost 0.000245094\n",
      "step 25480, training accuracy 0.969697, cost 9.92181, change in cost 0.000246048\n",
      "step 25490, training accuracy 0.969697, cost 9.92156, change in cost 0.000244141\n",
      "step 25500, training accuracy 0.969697, cost 9.92132, change in cost 0.000244141\n",
      "step 25510, training accuracy 0.969697, cost 9.92107, change in cost 0.000245094\n",
      "step 25520, training accuracy 0.969697, cost 9.92083, change in cost 0.000244141\n",
      "step 25530, training accuracy 0.969697, cost 9.92059, change in cost 0.000244141\n",
      "step 25540, training accuracy 0.969697, cost 9.92034, change in cost 0.000245094\n",
      "step 25550, training accuracy 0.969697, cost 9.9201, change in cost 0.000243187\n",
      "step 25560, training accuracy 0.969697, cost 9.91985, change in cost 0.000244141\n",
      "step 25570, training accuracy 0.969697, cost 9.91961, change in cost 0.000242233\n",
      "step 25580, training accuracy 0.969697, cost 9.91937, change in cost 0.000245094\n",
      "step 25590, training accuracy 0.969697, cost 9.91912, change in cost 0.000244141\n",
      "step 25600, training accuracy 0.969697, cost 9.91888, change in cost 0.000243187\n",
      "step 25610, training accuracy 0.969697, cost 9.91864, change in cost 0.000242233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25620, training accuracy 0.969697, cost 9.91839, change in cost 0.000244141\n",
      "step 25630, training accuracy 0.969697, cost 9.91815, change in cost 0.000243187\n",
      "step 25640, training accuracy 0.969697, cost 9.91791, change in cost 0.000243187\n",
      "step 25650, training accuracy 0.969697, cost 9.91766, change in cost 0.000242233\n",
      "step 25660, training accuracy 0.969697, cost 9.91742, change in cost 0.000243187\n",
      "step 25670, training accuracy 0.969697, cost 9.91718, change in cost 0.000243187\n",
      "step 25680, training accuracy 0.969697, cost 9.91694, change in cost 0.00024128\n",
      "step 25690, training accuracy 0.969697, cost 9.91669, change in cost 0.000243187\n",
      "step 25700, training accuracy 0.969697, cost 9.91645, change in cost 0.000242233\n",
      "step 25710, training accuracy 0.969697, cost 9.91621, change in cost 0.00024128\n",
      "step 25720, training accuracy 0.969697, cost 9.91597, change in cost 0.000242233\n",
      "step 25730, training accuracy 0.969697, cost 9.91573, change in cost 0.00024128\n",
      "step 25740, training accuracy 0.969697, cost 9.91548, change in cost 0.00024128\n",
      "step 25750, training accuracy 0.969697, cost 9.91524, change in cost 0.000240326\n",
      "step 25760, training accuracy 0.969697, cost 9.915, change in cost 0.00024128\n",
      "step 25770, training accuracy 0.969697, cost 9.91476, change in cost 0.000242233\n",
      "step 25780, training accuracy 0.969697, cost 9.91452, change in cost 0.000240326\n",
      "step 25790, training accuracy 0.969697, cost 9.91428, change in cost 0.00024128\n",
      "step 25800, training accuracy 0.969697, cost 9.91404, change in cost 0.000240326\n",
      "step 25810, training accuracy 0.969697, cost 9.9138, change in cost 0.000239372\n",
      "step 25820, training accuracy 0.969697, cost 9.91356, change in cost 0.000239372\n",
      "step 25830, training accuracy 0.969697, cost 9.91332, change in cost 0.000243187\n",
      "step 25840, training accuracy 0.969697, cost 9.91308, change in cost 0.000238419\n",
      "step 25850, training accuracy 0.969697, cost 9.91284, change in cost 0.000240326\n",
      "step 25860, training accuracy 0.969697, cost 9.9126, change in cost 0.000239372\n",
      "step 25870, training accuracy 0.969697, cost 9.91236, change in cost 0.000240326\n",
      "step 25880, training accuracy 0.969697, cost 9.91212, change in cost 0.000238419\n",
      "step 25890, training accuracy 0.969697, cost 9.91188, change in cost 0.00024128\n",
      "step 25900, training accuracy 0.969697, cost 9.91164, change in cost 0.000239372\n",
      "step 25910, training accuracy 0.969697, cost 9.9114, change in cost 0.000239372\n",
      "step 25920, training accuracy 0.969697, cost 9.91116, change in cost 0.000238419\n",
      "step 25930, training accuracy 0.969697, cost 9.91092, change in cost 0.000239372\n",
      "step 25940, training accuracy 0.969697, cost 9.91068, change in cost 0.000238419\n",
      "step 25950, training accuracy 0.969697, cost 9.91045, change in cost 0.000238419\n",
      "step 25960, training accuracy 0.969697, cost 9.91021, change in cost 0.000239372\n",
      "step 25970, training accuracy 0.969697, cost 9.90997, change in cost 0.000237465\n",
      "step 25980, training accuracy 0.969697, cost 9.90973, change in cost 0.000237465\n",
      "step 25990, training accuracy 0.969697, cost 9.90949, change in cost 0.000238419\n",
      "step 26000, training accuracy 0.969697, cost 9.90926, change in cost 0.000237465\n",
      "step 26010, training accuracy 0.969697, cost 9.90902, change in cost 0.000236511\n",
      "step 26020, training accuracy 0.969697, cost 9.90878, change in cost 0.000239372\n",
      "step 26030, training accuracy 0.969697, cost 9.90854, change in cost 0.000236511\n",
      "step 26040, training accuracy 0.969697, cost 9.9083, change in cost 0.000238419\n",
      "step 26050, training accuracy 0.969697, cost 9.90807, change in cost 0.000237465\n",
      "step 26060, training accuracy 0.969697, cost 9.90783, change in cost 0.000236511\n",
      "step 26070, training accuracy 0.969697, cost 9.90759, change in cost 0.000237465\n",
      "step 26080, training accuracy 0.969697, cost 9.90736, change in cost 0.000235558\n",
      "step 26090, training accuracy 0.969697, cost 9.90712, change in cost 0.000235558\n",
      "step 26100, training accuracy 0.969697, cost 9.90688, change in cost 0.000237465\n",
      "step 26110, training accuracy 0.969697, cost 9.90665, change in cost 0.000235558\n",
      "step 26120, training accuracy 0.969697, cost 9.90641, change in cost 0.000237465\n",
      "step 26130, training accuracy 0.969697, cost 9.90617, change in cost 0.000236511\n",
      "step 26140, training accuracy 0.969697, cost 9.90594, change in cost 0.000235558\n",
      "step 26150, training accuracy 0.969697, cost 9.9057, change in cost 0.000236511\n",
      "step 26160, training accuracy 0.969697, cost 9.90547, change in cost 0.000234604\n",
      "step 26170, training accuracy 0.969697, cost 9.90523, change in cost 0.000237465\n",
      "step 26180, training accuracy 0.969697, cost 9.905, change in cost 0.000234604\n",
      "step 26190, training accuracy 0.969697, cost 9.90476, change in cost 0.000235558\n",
      "step 26200, training accuracy 0.969697, cost 9.90453, change in cost 0.000234604\n",
      "step 26210, training accuracy 0.969697, cost 9.90429, change in cost 0.000237465\n",
      "step 26220, training accuracy 0.969697, cost 9.90405, change in cost 0.00023365\n",
      "step 26230, training accuracy 0.969697, cost 9.90382, change in cost 0.00023365\n",
      "step 26240, training accuracy 0.969697, cost 9.90359, change in cost 0.000235558\n",
      "step 26250, training accuracy 0.969697, cost 9.90335, change in cost 0.000234604\n",
      "step 26260, training accuracy 0.969697, cost 9.90312, change in cost 0.00023365\n",
      "step 26270, training accuracy 0.969697, cost 9.90288, change in cost 0.000234604\n",
      "step 26280, training accuracy 0.969697, cost 9.90265, change in cost 0.00023365\n",
      "step 26290, training accuracy 0.969697, cost 9.90241, change in cost 0.000234604\n",
      "step 26300, training accuracy 0.969697, cost 9.90218, change in cost 0.000234604\n",
      "step 26310, training accuracy 0.969697, cost 9.90195, change in cost 0.000232697\n",
      "step 26320, training accuracy 0.969697, cost 9.90171, change in cost 0.000232697\n",
      "step 26330, training accuracy 0.969697, cost 9.90148, change in cost 0.000234604\n",
      "step 26340, training accuracy 0.969697, cost 9.90125, change in cost 0.000232697\n",
      "step 26350, training accuracy 0.969697, cost 9.90101, change in cost 0.00023365\n",
      "step 26360, training accuracy 0.969697, cost 9.90078, change in cost 0.000232697\n",
      "step 26370, training accuracy 0.969697, cost 9.90055, change in cost 0.000232697\n",
      "step 26380, training accuracy 0.969697, cost 9.90031, change in cost 0.00023365\n",
      "step 26390, training accuracy 0.969697, cost 9.90008, change in cost 0.00023365\n",
      "step 26400, training accuracy 0.969697, cost 9.89985, change in cost 0.000230789\n",
      "step 26410, training accuracy 0.969697, cost 9.89962, change in cost 0.00023365\n",
      "step 26420, training accuracy 0.969697, cost 9.89938, change in cost 0.000231743\n",
      "step 26430, training accuracy 0.969697, cost 9.89915, change in cost 0.000232697\n",
      "step 26440, training accuracy 0.969697, cost 9.89892, change in cost 0.000231743\n",
      "step 26450, training accuracy 0.969697, cost 9.89869, change in cost 0.000231743\n",
      "step 26460, training accuracy 0.969697, cost 9.89846, change in cost 0.000231743\n",
      "step 26470, training accuracy 0.969697, cost 9.89822, change in cost 0.000231743\n",
      "step 26480, training accuracy 0.969697, cost 9.89799, change in cost 0.000230789\n",
      "step 26490, training accuracy 0.969697, cost 9.89776, change in cost 0.000231743\n",
      "step 26500, training accuracy 0.969697, cost 9.89753, change in cost 0.000230789\n",
      "step 26510, training accuracy 0.969697, cost 9.8973, change in cost 0.000229836\n",
      "step 26520, training accuracy 0.969697, cost 9.89707, change in cost 0.000232697\n",
      "step 26530, training accuracy 0.969697, cost 9.89684, change in cost 0.000230789\n",
      "step 26540, training accuracy 0.969697, cost 9.89661, change in cost 0.000228882\n",
      "step 26550, training accuracy 0.969697, cost 9.89638, change in cost 0.00023365\n",
      "step 26560, training accuracy 0.969697, cost 9.89615, change in cost 0.000228882\n",
      "step 26570, training accuracy 0.969697, cost 9.89592, change in cost 0.000229836\n",
      "step 26580, training accuracy 0.969697, cost 9.89569, change in cost 0.000230789\n",
      "step 26590, training accuracy 0.969697, cost 9.89546, change in cost 0.000229836\n",
      "step 26600, training accuracy 0.969697, cost 9.89523, change in cost 0.000230789\n",
      "step 26610, training accuracy 0.969697, cost 9.895, change in cost 0.000227928\n",
      "step 26620, training accuracy 0.969697, cost 9.89477, change in cost 0.000230789\n",
      "step 26630, training accuracy 0.969697, cost 9.89454, change in cost 0.000228882\n",
      "step 26640, training accuracy 0.969697, cost 9.89431, change in cost 0.000230789\n",
      "step 26650, training accuracy 0.969697, cost 9.89408, change in cost 0.000229836\n",
      "step 26660, training accuracy 0.969697, cost 9.89385, change in cost 0.000229836\n",
      "step 26670, training accuracy 0.969697, cost 9.89362, change in cost 0.000226974\n",
      "step 26680, training accuracy 0.969697, cost 9.89339, change in cost 0.000229836\n",
      "step 26690, training accuracy 0.969697, cost 9.89316, change in cost 0.000228882\n",
      "step 26700, training accuracy 0.969697, cost 9.89293, change in cost 0.000227928\n",
      "step 26710, training accuracy 0.969697, cost 9.8927, change in cost 0.000228882\n",
      "step 26720, training accuracy 0.969697, cost 9.89248, change in cost 0.000228882\n",
      "step 26730, training accuracy 0.969697, cost 9.89225, change in cost 0.000228882\n",
      "step 26740, training accuracy 0.969697, cost 9.89202, change in cost 0.000226974\n",
      "step 26750, training accuracy 0.969697, cost 9.89179, change in cost 0.000229836\n",
      "step 26760, training accuracy 0.969697, cost 9.89156, change in cost 0.000226974\n",
      "step 26770, training accuracy 0.969697, cost 9.89134, change in cost 0.000226021\n",
      "step 26780, training accuracy 0.969697, cost 9.89111, change in cost 0.000228882\n",
      "step 26790, training accuracy 0.969697, cost 9.89088, change in cost 0.000226974\n",
      "step 26800, training accuracy 0.969697, cost 9.89065, change in cost 0.000228882\n",
      "step 26810, training accuracy 0.969697, cost 9.89043, change in cost 0.000225067\n",
      "step 26820, training accuracy 0.969697, cost 9.8902, change in cost 0.000227928\n",
      "step 26830, training accuracy 0.969697, cost 9.88997, change in cost 0.000226021\n",
      "step 26840, training accuracy 0.969697, cost 9.88975, change in cost 0.000226021\n",
      "step 26850, training accuracy 0.969697, cost 9.88952, change in cost 0.000228882\n",
      "step 26860, training accuracy 0.969697, cost 9.88929, change in cost 0.000225067\n",
      "step 26870, training accuracy 0.969697, cost 9.88907, change in cost 0.000226021\n",
      "step 26880, training accuracy 0.969697, cost 9.88884, change in cost 0.000226974\n",
      "step 26890, training accuracy 0.969697, cost 9.88861, change in cost 0.000226974\n",
      "step 26900, training accuracy 0.969697, cost 9.88839, change in cost 0.000225067\n",
      "step 26910, training accuracy 0.969697, cost 9.88816, change in cost 0.000225067\n",
      "step 26920, training accuracy 0.969697, cost 9.88794, change in cost 0.000226974\n",
      "step 26930, training accuracy 0.969697, cost 9.88771, change in cost 0.000226974\n",
      "step 26940, training accuracy 0.969697, cost 9.88748, change in cost 0.000225067\n",
      "step 26950, training accuracy 0.969697, cost 9.88726, change in cost 0.000225067\n",
      "step 26960, training accuracy 0.969697, cost 9.88703, change in cost 0.000226021\n",
      "step 26970, training accuracy 0.969697, cost 9.88681, change in cost 0.000225067\n",
      "step 26980, training accuracy 0.969697, cost 9.88658, change in cost 0.000225067\n",
      "step 26990, training accuracy 0.969697, cost 9.88636, change in cost 0.000226021\n",
      "step 27000, training accuracy 0.969697, cost 9.88613, change in cost 0.00022316\n",
      "step 27010, training accuracy 0.969697, cost 9.88591, change in cost 0.000225067\n",
      "step 27020, training accuracy 0.969697, cost 9.88568, change in cost 0.000224113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27030, training accuracy 0.969697, cost 9.88546, change in cost 0.000226021\n",
      "step 27040, training accuracy 0.969697, cost 9.88524, change in cost 0.000222206\n",
      "step 27050, training accuracy 0.969697, cost 9.88501, change in cost 0.000224113\n",
      "step 27060, training accuracy 0.969697, cost 9.88479, change in cost 0.000225067\n",
      "step 27070, training accuracy 0.969697, cost 9.88456, change in cost 0.000226021\n",
      "step 27080, training accuracy 0.969697, cost 9.88434, change in cost 0.000222206\n",
      "step 27090, training accuracy 0.969697, cost 9.88412, change in cost 0.00022316\n",
      "step 27100, training accuracy 0.969697, cost 9.88389, change in cost 0.00022316\n",
      "step 27110, training accuracy 0.969697, cost 9.88367, change in cost 0.000224113\n",
      "step 27120, training accuracy 0.969697, cost 9.88345, change in cost 0.00022316\n",
      "step 27130, training accuracy 0.969697, cost 9.88322, change in cost 0.000224113\n",
      "step 27140, training accuracy 0.969697, cost 9.883, change in cost 0.000221252\n",
      "step 27150, training accuracy 0.969697, cost 9.88278, change in cost 0.000224113\n",
      "step 27160, training accuracy 0.969697, cost 9.88255, change in cost 0.000224113\n",
      "step 27170, training accuracy 0.969697, cost 9.88233, change in cost 0.000222206\n",
      "step 27180, training accuracy 0.969697, cost 9.88211, change in cost 0.00022316\n",
      "step 27190, training accuracy 0.969697, cost 9.88188, change in cost 0.000222206\n",
      "step 27200, training accuracy 0.969697, cost 9.88166, change in cost 0.000220299\n",
      "step 27210, training accuracy 0.969697, cost 9.88144, change in cost 0.00022316\n",
      "step 27220, training accuracy 0.969697, cost 9.88122, change in cost 0.000221252\n",
      "step 27230, training accuracy 0.969697, cost 9.881, change in cost 0.00022316\n",
      "step 27240, training accuracy 0.969697, cost 9.88077, change in cost 0.000222206\n",
      "step 27250, training accuracy 0.969697, cost 9.88055, change in cost 0.000222206\n",
      "step 27260, training accuracy 0.969697, cost 9.88033, change in cost 0.000221252\n",
      "step 27270, training accuracy 0.969697, cost 9.88011, change in cost 0.00022316\n",
      "step 27280, training accuracy 0.969697, cost 9.87989, change in cost 0.000220299\n",
      "step 27290, training accuracy 0.969697, cost 9.87967, change in cost 0.000221252\n",
      "step 27300, training accuracy 0.969697, cost 9.87945, change in cost 0.000220299\n",
      "step 27310, training accuracy 0.969697, cost 9.87923, change in cost 0.000219345\n",
      "step 27320, training accuracy 0.969697, cost 9.879, change in cost 0.00022316\n",
      "step 27330, training accuracy 0.969697, cost 9.87878, change in cost 0.000219345\n",
      "step 27340, training accuracy 0.969697, cost 9.87856, change in cost 0.000220299\n",
      "step 27350, training accuracy 0.969697, cost 9.87834, change in cost 0.000220299\n",
      "step 27360, training accuracy 0.969697, cost 9.87812, change in cost 0.000221252\n",
      "step 27370, training accuracy 0.969697, cost 9.8779, change in cost 0.000220299\n",
      "step 27380, training accuracy 0.969697, cost 9.87768, change in cost 0.000221252\n",
      "step 27390, training accuracy 0.969697, cost 9.87746, change in cost 0.000219345\n",
      "step 27400, training accuracy 0.969697, cost 9.87724, change in cost 0.000219345\n",
      "step 27410, training accuracy 0.969697, cost 9.87702, change in cost 0.000219345\n",
      "step 27420, training accuracy 0.969697, cost 9.8768, change in cost 0.000221252\n",
      "step 27430, training accuracy 0.969697, cost 9.87658, change in cost 0.000218391\n",
      "step 27440, training accuracy 0.969697, cost 9.87636, change in cost 0.000220299\n",
      "step 27450, training accuracy 0.969697, cost 9.87614, change in cost 0.000218391\n",
      "step 27460, training accuracy 0.969697, cost 9.87593, change in cost 0.000218391\n",
      "step 27470, training accuracy 0.969697, cost 9.8757, change in cost 0.000222206\n",
      "step 27480, training accuracy 0.969697, cost 9.87549, change in cost 0.000218391\n",
      "step 27490, training accuracy 0.969697, cost 9.87527, change in cost 0.000217438\n",
      "step 27500, training accuracy 0.969697, cost 9.87505, change in cost 0.000218391\n",
      "step 27510, training accuracy 0.969697, cost 9.87483, change in cost 0.000219345\n",
      "step 27520, training accuracy 0.969697, cost 9.87461, change in cost 0.000218391\n",
      "step 27530, training accuracy 0.969697, cost 9.87439, change in cost 0.000217438\n",
      "step 27540, training accuracy 0.969697, cost 9.87418, change in cost 0.000219345\n",
      "step 27550, training accuracy 0.969697, cost 9.87396, change in cost 0.000217438\n",
      "step 27560, training accuracy 0.969697, cost 9.87374, change in cost 0.000217438\n",
      "step 27570, training accuracy 0.969697, cost 9.87352, change in cost 0.000218391\n",
      "step 27580, training accuracy 0.969697, cost 9.87331, change in cost 0.000216484\n",
      "step 27590, training accuracy 0.969697, cost 9.87309, change in cost 0.000217438\n",
      "step 27600, training accuracy 0.969697, cost 9.87287, change in cost 0.000218391\n",
      "step 27610, training accuracy 0.969697, cost 9.87265, change in cost 0.00021553\n",
      "step 27620, training accuracy 0.969697, cost 9.87244, change in cost 0.000218391\n",
      "step 27630, training accuracy 0.969697, cost 9.87222, change in cost 0.000217438\n",
      "step 27640, training accuracy 0.969697, cost 9.872, change in cost 0.000217438\n",
      "step 27650, training accuracy 0.969697, cost 9.87178, change in cost 0.000217438\n",
      "step 27660, training accuracy 0.969697, cost 9.87157, change in cost 0.000216484\n",
      "step 27670, training accuracy 0.969697, cost 9.87135, change in cost 0.000216484\n",
      "step 27680, training accuracy 0.969697, cost 9.87113, change in cost 0.00021553\n",
      "step 27690, training accuracy 0.969697, cost 9.87092, change in cost 0.000216484\n",
      "step 27700, training accuracy 0.969697, cost 9.8707, change in cost 0.000217438\n",
      "step 27710, training accuracy 0.969697, cost 9.87049, change in cost 0.00021553\n",
      "step 27720, training accuracy 0.969697, cost 9.87027, change in cost 0.000216484\n",
      "step 27730, training accuracy 0.969697, cost 9.87005, change in cost 0.00021553\n",
      "step 27740, training accuracy 0.969697, cost 9.86984, change in cost 0.00021553\n",
      "step 27750, training accuracy 0.969697, cost 9.86962, change in cost 0.000216484\n",
      "step 27760, training accuracy 0.969697, cost 9.86941, change in cost 0.000214577\n",
      "step 27770, training accuracy 0.969697, cost 9.86919, change in cost 0.000217438\n",
      "step 27780, training accuracy 0.969697, cost 9.86897, change in cost 0.000214577\n",
      "step 27790, training accuracy 0.969697, cost 9.86876, change in cost 0.000214577\n",
      "step 27800, training accuracy 0.969697, cost 9.86854, change in cost 0.000216484\n",
      "step 27810, training accuracy 0.969697, cost 9.86833, change in cost 0.000214577\n",
      "step 27820, training accuracy 0.969697, cost 9.86811, change in cost 0.00021553\n",
      "step 27830, training accuracy 0.969697, cost 9.8679, change in cost 0.000213623\n",
      "step 27840, training accuracy 0.969697, cost 9.86768, change in cost 0.00021553\n",
      "step 27850, training accuracy 0.969697, cost 9.86747, change in cost 0.000214577\n",
      "step 27860, training accuracy 0.969697, cost 9.86726, change in cost 0.000213623\n",
      "step 27870, training accuracy 0.969697, cost 9.86704, change in cost 0.000216484\n",
      "step 27880, training accuracy 0.969697, cost 9.86683, change in cost 0.000212669\n",
      "step 27890, training accuracy 0.969697, cost 9.86661, change in cost 0.000214577\n",
      "step 27900, training accuracy 0.969697, cost 9.8664, change in cost 0.000213623\n",
      "step 27910, training accuracy 0.969697, cost 9.86619, change in cost 0.000213623\n",
      "step 27920, training accuracy 0.969697, cost 9.86597, change in cost 0.000213623\n",
      "step 27930, training accuracy 0.969697, cost 9.86576, change in cost 0.00021553\n",
      "step 27940, training accuracy 0.969697, cost 9.86554, change in cost 0.000211716\n",
      "step 27950, training accuracy 0.969697, cost 9.86533, change in cost 0.000214577\n",
      "step 27960, training accuracy 0.969697, cost 9.86512, change in cost 0.000213623\n",
      "step 27970, training accuracy 0.969697, cost 9.8649, change in cost 0.000211716\n",
      "step 27980, training accuracy 0.969697, cost 9.86469, change in cost 0.000212669\n",
      "step 27990, training accuracy 0.969697, cost 9.86448, change in cost 0.000212669\n",
      "step 28000, training accuracy 0.969697, cost 9.86427, change in cost 0.000213623\n",
      "step 28010, training accuracy 0.969697, cost 9.86405, change in cost 0.000211716\n",
      "step 28020, training accuracy 0.969697, cost 9.86384, change in cost 0.000210762\n",
      "step 28030, training accuracy 0.969697, cost 9.86363, change in cost 0.000213623\n",
      "step 28040, training accuracy 0.969697, cost 9.86342, change in cost 0.000211716\n",
      "step 28050, training accuracy 0.969697, cost 9.86321, change in cost 0.000211716\n",
      "step 28060, training accuracy 0.969697, cost 9.86299, change in cost 0.000214577\n",
      "step 28070, training accuracy 0.969697, cost 9.86278, change in cost 0.000210762\n",
      "step 28080, training accuracy 0.969697, cost 9.86257, change in cost 0.000212669\n",
      "step 28090, training accuracy 0.969697, cost 9.86236, change in cost 0.000211716\n",
      "step 28100, training accuracy 0.969697, cost 9.86215, change in cost 0.000210762\n",
      "step 28110, training accuracy 0.969697, cost 9.86193, change in cost 0.000212669\n",
      "step 28120, training accuracy 0.969697, cost 9.86172, change in cost 0.000213623\n",
      "step 28130, training accuracy 0.969697, cost 9.86151, change in cost 0.000210762\n",
      "step 28140, training accuracy 0.969697, cost 9.8613, change in cost 0.000212669\n",
      "step 28150, training accuracy 0.969697, cost 9.86109, change in cost 0.000209808\n",
      "step 28160, training accuracy 0.969697, cost 9.86088, change in cost 0.000210762\n",
      "step 28170, training accuracy 0.969697, cost 9.86067, change in cost 0.000209808\n",
      "step 28180, training accuracy 0.969697, cost 9.86046, change in cost 0.000209808\n",
      "step 28190, training accuracy 0.969697, cost 9.86024, change in cost 0.000211716\n",
      "step 28200, training accuracy 0.969697, cost 9.86003, change in cost 0.000209808\n",
      "step 28210, training accuracy 0.969697, cost 9.85982, change in cost 0.000210762\n",
      "step 28220, training accuracy 0.969697, cost 9.85961, change in cost 0.000208855\n",
      "step 28230, training accuracy 0.969697, cost 9.8594, change in cost 0.000210762\n",
      "step 28240, training accuracy 0.969697, cost 9.85919, change in cost 0.000210762\n",
      "step 28250, training accuracy 0.969697, cost 9.85898, change in cost 0.000207901\n",
      "step 28260, training accuracy 0.969697, cost 9.85878, change in cost 0.000209808\n",
      "step 28270, training accuracy 0.969697, cost 9.85856, change in cost 0.000210762\n",
      "step 28280, training accuracy 0.969697, cost 9.85836, change in cost 0.000208855\n",
      "step 28290, training accuracy 0.969697, cost 9.85815, change in cost 0.000209808\n",
      "step 28300, training accuracy 0.969697, cost 9.85794, change in cost 0.000209808\n",
      "step 28310, training accuracy 0.969697, cost 9.85773, change in cost 0.000209808\n",
      "step 28320, training accuracy 0.969697, cost 9.85752, change in cost 0.000208855\n",
      "step 28330, training accuracy 0.969697, cost 9.85731, change in cost 0.000208855\n",
      "step 28340, training accuracy 0.969697, cost 9.8571, change in cost 0.000208855\n",
      "step 28350, training accuracy 0.969697, cost 9.85689, change in cost 0.000209808\n",
      "step 28360, training accuracy 0.969697, cost 9.85668, change in cost 0.000208855\n",
      "step 28370, training accuracy 0.969697, cost 9.85647, change in cost 0.000207901\n",
      "step 28380, training accuracy 0.969697, cost 9.85626, change in cost 0.000208855\n",
      "step 28390, training accuracy 0.969697, cost 9.85606, change in cost 0.000207901\n",
      "step 28400, training accuracy 0.969697, cost 9.85585, change in cost 0.000208855\n",
      "step 28410, training accuracy 0.969697, cost 9.85564, change in cost 0.000207901\n",
      "step 28420, training accuracy 0.969697, cost 9.85543, change in cost 0.000207901\n",
      "step 28430, training accuracy 0.969697, cost 9.85522, change in cost 0.000208855\n",
      "step 28440, training accuracy 0.969697, cost 9.85502, change in cost 0.000205994\n",
      "step 28450, training accuracy 0.969697, cost 9.85481, change in cost 0.000207901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28460, training accuracy 0.969697, cost 9.8546, change in cost 0.000208855\n",
      "step 28470, training accuracy 0.969697, cost 9.85439, change in cost 0.000207901\n",
      "step 28480, training accuracy 0.969697, cost 9.85419, change in cost 0.000206947\n",
      "step 28490, training accuracy 0.969697, cost 9.85398, change in cost 0.000205994\n",
      "step 28500, training accuracy 0.969697, cost 9.85377, change in cost 0.000206947\n",
      "step 28510, training accuracy 0.969697, cost 9.85356, change in cost 0.000207901\n",
      "step 28520, training accuracy 0.969697, cost 9.85336, change in cost 0.000205994\n",
      "step 28530, training accuracy 0.969697, cost 9.85315, change in cost 0.000205994\n",
      "step 28540, training accuracy 0.969697, cost 9.85294, change in cost 0.000208855\n",
      "step 28550, training accuracy 0.969697, cost 9.85274, change in cost 0.00020504\n",
      "step 28560, training accuracy 0.969697, cost 9.85253, change in cost 0.000206947\n",
      "step 28570, training accuracy 0.969697, cost 9.85232, change in cost 0.000206947\n",
      "step 28580, training accuracy 0.969697, cost 9.85212, change in cost 0.000205994\n",
      "step 28590, training accuracy 0.969697, cost 9.85191, change in cost 0.000206947\n",
      "step 28600, training accuracy 0.969697, cost 9.85171, change in cost 0.000205994\n",
      "step 28610, training accuracy 0.969697, cost 9.8515, change in cost 0.000204086\n",
      "step 28620, training accuracy 0.969697, cost 9.85129, change in cost 0.000206947\n",
      "step 28630, training accuracy 0.969697, cost 9.85109, change in cost 0.00020504\n",
      "step 28640, training accuracy 0.969697, cost 9.85088, change in cost 0.00020504\n",
      "step 28650, training accuracy 0.969697, cost 9.85068, change in cost 0.000205994\n",
      "step 28660, training accuracy 0.969697, cost 9.85047, change in cost 0.00020504\n",
      "step 28670, training accuracy 0.969697, cost 9.85027, change in cost 0.000204086\n",
      "step 28680, training accuracy 0.969697, cost 9.85006, change in cost 0.000205994\n",
      "step 28690, training accuracy 0.969697, cost 9.84986, change in cost 0.000205994\n",
      "step 28700, training accuracy 0.969697, cost 9.84965, change in cost 0.000205994\n",
      "step 28710, training accuracy 0.969697, cost 9.84945, change in cost 0.000204086\n",
      "step 28720, training accuracy 0.969697, cost 9.84924, change in cost 0.000204086\n",
      "step 28730, training accuracy 0.969697, cost 9.84904, change in cost 0.000205994\n",
      "step 28740, training accuracy 0.969697, cost 9.84883, change in cost 0.000204086\n",
      "step 28750, training accuracy 0.969697, cost 9.84863, change in cost 0.000204086\n",
      "step 28760, training accuracy 0.969697, cost 9.84842, change in cost 0.000205994\n",
      "step 28770, training accuracy 0.969697, cost 9.84822, change in cost 0.000203133\n",
      "step 28780, training accuracy 0.969697, cost 9.84801, change in cost 0.00020504\n",
      "step 28790, training accuracy 0.969697, cost 9.84781, change in cost 0.000202179\n",
      "step 28800, training accuracy 0.969697, cost 9.84761, change in cost 0.00020504\n",
      "step 28810, training accuracy 0.969697, cost 9.8474, change in cost 0.000203133\n",
      "step 28820, training accuracy 0.969697, cost 9.8472, change in cost 0.000203133\n",
      "step 28830, training accuracy 0.969697, cost 9.847, change in cost 0.000203133\n",
      "step 28840, training accuracy 0.969697, cost 9.8468, change in cost 0.000203133\n",
      "step 28850, training accuracy 0.969697, cost 9.84659, change in cost 0.000204086\n",
      "step 28860, training accuracy 0.969697, cost 9.84639, change in cost 0.000202179\n",
      "step 28870, training accuracy 0.969697, cost 9.84619, change in cost 0.000203133\n",
      "step 28880, training accuracy 0.969697, cost 9.84598, change in cost 0.000202179\n",
      "step 28890, training accuracy 0.969697, cost 9.84578, change in cost 0.000202179\n",
      "step 28900, training accuracy 0.969697, cost 9.84558, change in cost 0.000203133\n",
      "step 28910, training accuracy 0.969697, cost 9.84537, change in cost 0.000204086\n",
      "step 28920, training accuracy 0.969697, cost 9.84517, change in cost 0.000200272\n",
      "step 28930, training accuracy 0.969697, cost 9.84497, change in cost 0.000203133\n",
      "step 28940, training accuracy 0.969697, cost 9.84477, change in cost 0.000202179\n",
      "step 28950, training accuracy 0.969697, cost 9.84457, change in cost 0.000202179\n",
      "step 28960, training accuracy 0.969697, cost 9.84436, change in cost 0.000203133\n",
      "step 28970, training accuracy 0.969697, cost 9.84416, change in cost 0.000203133\n",
      "step 28980, training accuracy 0.969697, cost 9.84396, change in cost 0.000201225\n",
      "step 28990, training accuracy 0.969697, cost 9.84376, change in cost 0.000203133\n",
      "step 29000, training accuracy 0.969697, cost 9.84355, change in cost 0.000201225\n",
      "step 29010, training accuracy 0.969697, cost 9.84336, change in cost 0.000199318\n",
      "step 29020, training accuracy 0.969697, cost 9.84315, change in cost 0.000204086\n",
      "step 29030, training accuracy 0.969697, cost 9.84295, change in cost 0.000201225\n",
      "step 29040, training accuracy 0.969697, cost 9.84275, change in cost 0.000201225\n",
      "step 29050, training accuracy 0.969697, cost 9.84255, change in cost 0.000199318\n",
      "step 29060, training accuracy 0.969697, cost 9.84235, change in cost 0.000203133\n",
      "step 29070, training accuracy 0.969697, cost 9.84214, change in cost 0.000201225\n",
      "step 29080, training accuracy 0.969697, cost 9.84194, change in cost 0.000200272\n",
      "step 29090, training accuracy 0.969697, cost 9.84175, change in cost 0.000198364\n",
      "step 29100, training accuracy 0.969697, cost 9.84154, change in cost 0.000202179\n",
      "step 29110, training accuracy 0.969697, cost 9.84134, change in cost 0.000199318\n",
      "step 29120, training accuracy 0.969697, cost 9.84114, change in cost 0.000200272\n",
      "step 29130, training accuracy 0.969697, cost 9.84094, change in cost 0.000201225\n",
      "step 29140, training accuracy 0.969697, cost 9.84074, change in cost 0.000199318\n",
      "step 29150, training accuracy 0.969697, cost 9.84054, change in cost 0.000199318\n",
      "step 29160, training accuracy 0.969697, cost 9.84034, change in cost 0.000201225\n",
      "step 29170, training accuracy 0.969697, cost 9.84014, change in cost 0.000199318\n",
      "step 29180, training accuracy 0.969697, cost 9.83994, change in cost 0.000200272\n",
      "step 29190, training accuracy 0.969697, cost 9.83974, change in cost 0.000200272\n",
      "step 29200, training accuracy 0.969697, cost 9.83955, change in cost 0.000198364\n",
      "step 29210, training accuracy 0.969697, cost 9.83934, change in cost 0.000200272\n",
      "step 29220, training accuracy 0.969697, cost 9.83915, change in cost 0.000199318\n",
      "step 29230, training accuracy 0.969697, cost 9.83895, change in cost 0.000199318\n",
      "step 29240, training accuracy 0.969697, cost 9.83875, change in cost 0.000197411\n",
      "step 29250, training accuracy 0.969697, cost 9.83855, change in cost 0.000200272\n",
      "step 29260, training accuracy 0.969697, cost 9.83835, change in cost 0.000199318\n",
      "step 29270, training accuracy 0.969697, cost 9.83815, change in cost 0.000198364\n",
      "step 29280, training accuracy 0.969697, cost 9.83795, change in cost 0.000199318\n",
      "step 29290, training accuracy 0.969697, cost 9.83775, change in cost 0.000197411\n",
      "step 29300, training accuracy 0.969697, cost 9.83756, change in cost 0.000198364\n",
      "step 29310, training accuracy 0.969697, cost 9.83736, change in cost 0.000198364\n",
      "step 29320, training accuracy 0.969697, cost 9.83716, change in cost 0.000198364\n",
      "step 29330, training accuracy 0.969697, cost 9.83696, change in cost 0.000197411\n",
      "step 29340, training accuracy 0.969697, cost 9.83676, change in cost 0.000199318\n",
      "step 29350, training accuracy 0.969697, cost 9.83657, change in cost 0.000197411\n",
      "step 29360, training accuracy 0.969697, cost 9.83637, change in cost 0.000199318\n",
      "step 29370, training accuracy 0.969697, cost 9.83617, change in cost 0.000197411\n",
      "step 29380, training accuracy 0.969697, cost 9.83597, change in cost 0.000197411\n",
      "step 29390, training accuracy 0.969697, cost 9.83577, change in cost 0.000199318\n",
      "step 29400, training accuracy 0.969697, cost 9.83558, change in cost 0.000196457\n",
      "step 29410, training accuracy 0.969697, cost 9.83538, change in cost 0.000196457\n",
      "step 29420, training accuracy 0.969697, cost 9.83518, change in cost 0.000197411\n",
      "step 29430, training accuracy 0.969697, cost 9.83498, change in cost 0.000198364\n",
      "step 29440, training accuracy 0.969697, cost 9.83479, change in cost 0.000196457\n",
      "step 29450, training accuracy 0.969697, cost 9.83459, change in cost 0.000197411\n",
      "step 29460, training accuracy 0.969697, cost 9.83439, change in cost 0.000196457\n",
      "step 29470, training accuracy 0.969697, cost 9.8342, change in cost 0.000195503\n",
      "step 29480, training accuracy 0.969697, cost 9.834, change in cost 0.000197411\n",
      "step 29490, training accuracy 0.969697, cost 9.8338, change in cost 0.000195503\n",
      "step 29500, training accuracy 0.969697, cost 9.83361, change in cost 0.000195503\n",
      "step 29510, training accuracy 0.969697, cost 9.83341, change in cost 0.000197411\n",
      "step 29520, training accuracy 0.969697, cost 9.83322, change in cost 0.000195503\n",
      "step 29530, training accuracy 0.969697, cost 9.83302, change in cost 0.000196457\n",
      "step 29540, training accuracy 0.969697, cost 9.83282, change in cost 0.000197411\n",
      "step 29550, training accuracy 0.969697, cost 9.83263, change in cost 0.00019455\n",
      "step 29560, training accuracy 0.969697, cost 9.83243, change in cost 0.000195503\n",
      "step 29570, training accuracy 0.969697, cost 9.83224, change in cost 0.000196457\n",
      "step 29580, training accuracy 0.969697, cost 9.83204, change in cost 0.00019455\n",
      "step 29590, training accuracy 0.969697, cost 9.83184, change in cost 0.000196457\n",
      "step 29600, training accuracy 0.969697, cost 9.83165, change in cost 0.000195503\n",
      "step 29610, training accuracy 0.969697, cost 9.83145, change in cost 0.000195503\n",
      "step 29620, training accuracy 0.969697, cost 9.83126, change in cost 0.000195503\n",
      "step 29630, training accuracy 0.969697, cost 9.83106, change in cost 0.00019455\n",
      "step 29640, training accuracy 0.969697, cost 9.83087, change in cost 0.000195503\n",
      "step 29650, training accuracy 0.969697, cost 9.83067, change in cost 0.00019455\n",
      "step 29660, training accuracy 0.969697, cost 9.83048, change in cost 0.000195503\n",
      "step 29670, training accuracy 0.969697, cost 9.83028, change in cost 0.000195503\n",
      "step 29680, training accuracy 0.969697, cost 9.83009, change in cost 0.00019455\n",
      "step 29690, training accuracy 0.969697, cost 9.82989, change in cost 0.000193596\n",
      "step 29700, training accuracy 0.969697, cost 9.8297, change in cost 0.000193596\n",
      "step 29710, training accuracy 0.969697, cost 9.82951, change in cost 0.00019455\n",
      "step 29720, training accuracy 0.969697, cost 9.82931, change in cost 0.000192642\n",
      "step 29730, training accuracy 0.969697, cost 9.82912, change in cost 0.000195503\n",
      "step 29740, training accuracy 0.969697, cost 9.82893, change in cost 0.000192642\n",
      "step 29750, training accuracy 0.969697, cost 9.82873, change in cost 0.00019455\n",
      "step 29760, training accuracy 0.969697, cost 9.82854, change in cost 0.000193596\n",
      "step 29770, training accuracy 0.969697, cost 9.82834, change in cost 0.00019455\n",
      "step 29780, training accuracy 0.969697, cost 9.82815, change in cost 0.000191689\n",
      "step 29790, training accuracy 0.969697, cost 9.82796, change in cost 0.00019455\n",
      "step 29800, training accuracy 0.969697, cost 9.82776, change in cost 0.000193596\n",
      "step 29810, training accuracy 0.969697, cost 9.82757, change in cost 0.000191689\n",
      "step 29820, training accuracy 0.969697, cost 9.82738, change in cost 0.000193596\n",
      "step 29830, training accuracy 0.969697, cost 9.82718, change in cost 0.00019455\n",
      "step 29840, training accuracy 0.969697, cost 9.82699, change in cost 0.000192642\n",
      "step 29850, training accuracy 0.969697, cost 9.8268, change in cost 0.000193596\n",
      "step 29860, training accuracy 0.969697, cost 9.8266, change in cost 0.000192642\n",
      "step 29870, training accuracy 0.969697, cost 9.82641, change in cost 0.000191689\n",
      "step 29880, training accuracy 0.969697, cost 9.82622, change in cost 0.00019455\n",
      "step 29890, training accuracy 0.969697, cost 9.82603, change in cost 0.000190735\n",
      "step 29900, training accuracy 0.969697, cost 9.82583, change in cost 0.000192642\n",
      "step 29910, training accuracy 0.969697, cost 9.82564, change in cost 0.000193596\n",
      "step 29920, training accuracy 0.969697, cost 9.82545, change in cost 0.000190735\n",
      "step 29930, training accuracy 0.969697, cost 9.82526, change in cost 0.000191689\n",
      "step 29940, training accuracy 0.969697, cost 9.82507, change in cost 0.000192642\n",
      "step 29950, training accuracy 0.969697, cost 9.82487, change in cost 0.000191689\n",
      "step 29960, training accuracy 0.969697, cost 9.82468, change in cost 0.000190735\n",
      "step 29970, training accuracy 0.969697, cost 9.82449, change in cost 0.000192642\n",
      "step 29980, training accuracy 0.969697, cost 9.8243, change in cost 0.000190735\n",
      "step 29990, training accuracy 0.969697, cost 9.82411, change in cost 0.000191689\n",
      "step 30000, training accuracy 0.969697, cost 9.82392, change in cost 0.000191689\n",
      "step 30010, training accuracy 0.969697, cost 9.82373, change in cost 0.000190735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30020, training accuracy 0.969697, cost 9.82353, change in cost 0.000191689\n",
      "step 30030, training accuracy 0.969697, cost 9.82335, change in cost 0.000188828\n",
      "step 30040, training accuracy 0.969697, cost 9.82315, change in cost 0.000192642\n",
      "step 30050, training accuracy 0.969697, cost 9.82296, change in cost 0.000189781\n",
      "step 30060, training accuracy 0.969697, cost 9.82277, change in cost 0.000192642\n",
      "step 30070, training accuracy 0.969697, cost 9.82258, change in cost 0.000190735\n",
      "step 30080, training accuracy 0.969697, cost 9.82239, change in cost 0.000189781\n",
      "step 30090, training accuracy 0.969697, cost 9.8222, change in cost 0.000190735\n",
      "step 30100, training accuracy 0.969697, cost 9.82201, change in cost 0.000191689\n",
      "step 30110, training accuracy 0.969697, cost 9.82182, change in cost 0.000188828\n",
      "step 30120, training accuracy 0.969697, cost 9.82163, change in cost 0.000188828\n",
      "step 30130, training accuracy 0.969697, cost 9.82144, change in cost 0.000191689\n",
      "step 30140, training accuracy 0.969697, cost 9.82125, change in cost 0.00018692\n",
      "step 30150, training accuracy 0.969697, cost 9.82106, change in cost 0.000191689\n",
      "step 30160, training accuracy 0.969697, cost 9.82087, change in cost 0.000190735\n",
      "step 30170, training accuracy 0.969697, cost 9.82068, change in cost 0.000188828\n",
      "step 30180, training accuracy 0.969697, cost 9.82049, change in cost 0.000188828\n",
      "step 30190, training accuracy 0.969697, cost 9.8203, change in cost 0.000189781\n",
      "step 30200, training accuracy 0.969697, cost 9.82011, change in cost 0.000190735\n",
      "step 30210, training accuracy 0.969697, cost 9.81992, change in cost 0.00018692\n",
      "step 30220, training accuracy 0.969697, cost 9.81973, change in cost 0.000190735\n",
      "step 30230, training accuracy 0.969697, cost 9.81955, change in cost 0.00018692\n",
      "step 30240, training accuracy 0.969697, cost 9.81936, change in cost 0.000189781\n",
      "step 30250, training accuracy 0.969697, cost 9.81917, change in cost 0.000188828\n",
      "step 30260, training accuracy 0.969697, cost 9.81898, change in cost 0.000190735\n",
      "step 30270, training accuracy 0.969697, cost 9.81879, change in cost 0.000185966\n",
      "step 30280, training accuracy 0.969697, cost 9.8186, change in cost 0.000189781\n",
      "step 30290, training accuracy 0.969697, cost 9.81841, change in cost 0.000189781\n",
      "step 30300, training accuracy 0.969697, cost 9.81822, change in cost 0.000187874\n",
      "step 30310, training accuracy 0.969697, cost 9.81803, change in cost 0.000188828\n",
      "step 30320, training accuracy 0.969697, cost 9.81784, change in cost 0.000189781\n",
      "step 30330, training accuracy 0.969697, cost 9.81766, change in cost 0.00018692\n",
      "step 30340, training accuracy 0.969697, cost 9.81747, change in cost 0.000188828\n",
      "step 30350, training accuracy 0.969697, cost 9.81728, change in cost 0.00018692\n",
      "step 30360, training accuracy 0.969697, cost 9.81709, change in cost 0.000188828\n",
      "step 30370, training accuracy 0.969697, cost 9.81691, change in cost 0.00018692\n",
      "step 30380, training accuracy 0.969697, cost 9.81672, change in cost 0.00018692\n",
      "step 30390, training accuracy 0.969697, cost 9.81653, change in cost 0.00018692\n",
      "step 30400, training accuracy 0.969697, cost 9.81634, change in cost 0.000187874\n",
      "step 30410, training accuracy 0.969697, cost 9.81616, change in cost 0.00018692\n",
      "step 30420, training accuracy 0.969697, cost 9.81597, change in cost 0.000188828\n",
      "step 30430, training accuracy 0.969697, cost 9.81578, change in cost 0.000185966\n",
      "step 30440, training accuracy 0.969697, cost 9.8156, change in cost 0.000185966\n",
      "step 30450, training accuracy 0.969697, cost 9.81541, change in cost 0.000187874\n",
      "step 30460, training accuracy 0.969697, cost 9.81522, change in cost 0.00018692\n",
      "step 30470, training accuracy 0.969697, cost 9.81504, change in cost 0.000185966\n",
      "step 30480, training accuracy 0.969697, cost 9.81485, change in cost 0.000187874\n",
      "step 30490, training accuracy 0.969697, cost 9.81466, change in cost 0.000185013\n",
      "step 30500, training accuracy 0.969697, cost 9.81448, change in cost 0.00018692\n",
      "step 30510, training accuracy 0.969697, cost 9.81429, change in cost 0.000185966\n",
      "step 30520, training accuracy 0.969697, cost 9.8141, change in cost 0.00018692\n",
      "step 30530, training accuracy 0.969697, cost 9.81392, change in cost 0.000185966\n",
      "step 30540, training accuracy 0.969697, cost 9.81373, change in cost 0.00018692\n",
      "step 30550, training accuracy 0.969697, cost 9.81354, change in cost 0.000185966\n",
      "step 30560, training accuracy 0.969697, cost 9.81336, change in cost 0.000187874\n",
      "step 30570, training accuracy 0.969697, cost 9.81317, change in cost 0.000184059\n",
      "step 30580, training accuracy 0.969697, cost 9.81299, change in cost 0.000185966\n",
      "step 30590, training accuracy 0.969697, cost 9.8128, change in cost 0.000184059\n",
      "step 30600, training accuracy 0.969697, cost 9.81262, change in cost 0.000185966\n",
      "step 30610, training accuracy 0.969697, cost 9.81243, change in cost 0.000184059\n",
      "step 30620, training accuracy 0.969697, cost 9.81225, change in cost 0.000185966\n",
      "step 30630, training accuracy 0.969697, cost 9.81206, change in cost 0.000185966\n",
      "step 30640, training accuracy 0.969697, cost 9.81188, change in cost 0.000185013\n",
      "step 30650, training accuracy 0.969697, cost 9.81169, change in cost 0.000184059\n",
      "step 30660, training accuracy 0.969697, cost 9.81151, change in cost 0.000185966\n",
      "step 30670, training accuracy 0.969697, cost 9.81132, change in cost 0.000184059\n",
      "step 30680, training accuracy 0.969697, cost 9.81114, change in cost 0.000185966\n",
      "step 30690, training accuracy 0.969697, cost 9.81095, change in cost 0.000184059\n",
      "step 30700, training accuracy 0.969697, cost 9.81077, change in cost 0.000185013\n",
      "step 30710, training accuracy 0.969697, cost 9.81058, change in cost 0.000183105\n",
      "step 30720, training accuracy 0.969697, cost 9.8104, change in cost 0.00018692\n",
      "step 30730, training accuracy 0.969697, cost 9.81021, change in cost 0.000183105\n",
      "step 30740, training accuracy 0.969697, cost 9.81003, change in cost 0.000185013\n",
      "step 30750, training accuracy 0.969697, cost 9.80984, change in cost 0.000184059\n",
      "step 30760, training accuracy 0.969697, cost 9.80966, change in cost 0.000184059\n",
      "step 30770, training accuracy 0.969697, cost 9.80948, change in cost 0.000184059\n",
      "step 30780, training accuracy 0.969697, cost 9.80929, change in cost 0.000183105\n",
      "step 30790, training accuracy 0.969697, cost 9.80911, change in cost 0.000184059\n",
      "step 30800, training accuracy 0.969697, cost 9.80893, change in cost 0.000183105\n",
      "step 30810, training accuracy 0.969697, cost 9.80874, change in cost 0.000184059\n",
      "step 30820, training accuracy 0.969697, cost 9.80856, change in cost 0.000183105\n",
      "step 30830, training accuracy 0.969697, cost 9.80838, change in cost 0.000182152\n",
      "step 30840, training accuracy 0.969697, cost 9.80819, change in cost 0.000183105\n",
      "step 30850, training accuracy 0.969697, cost 9.80801, change in cost 0.000183105\n",
      "step 30860, training accuracy 0.969697, cost 9.80783, change in cost 0.000183105\n",
      "step 30870, training accuracy 0.969697, cost 9.80764, change in cost 0.000183105\n",
      "step 30880, training accuracy 0.969697, cost 9.80746, change in cost 0.000182152\n",
      "step 30890, training accuracy 0.969697, cost 9.80728, change in cost 0.000184059\n",
      "step 30900, training accuracy 0.969697, cost 9.8071, change in cost 0.000182152\n",
      "step 30910, training accuracy 0.969697, cost 9.80691, change in cost 0.000182152\n",
      "step 30920, training accuracy 0.969697, cost 9.80673, change in cost 0.000182152\n",
      "step 30930, training accuracy 0.969697, cost 9.80655, change in cost 0.000183105\n",
      "step 30940, training accuracy 0.969697, cost 9.80637, change in cost 0.000182152\n",
      "step 30950, training accuracy 0.969697, cost 9.80618, change in cost 0.000183105\n",
      "step 30960, training accuracy 0.969697, cost 9.806, change in cost 0.000181198\n",
      "step 30970, training accuracy 0.969697, cost 9.80582, change in cost 0.000182152\n",
      "step 30980, training accuracy 0.969697, cost 9.80564, change in cost 0.000181198\n",
      "step 30990, training accuracy 0.969697, cost 9.80546, change in cost 0.000183105\n",
      "step 31000, training accuracy 0.969697, cost 9.80527, change in cost 0.000181198\n",
      "step 31010, training accuracy 0.969697, cost 9.80509, change in cost 0.000182152\n",
      "step 31020, training accuracy 0.969697, cost 9.80491, change in cost 0.000183105\n",
      "step 31030, training accuracy 0.969697, cost 9.80473, change in cost 0.000181198\n",
      "step 31040, training accuracy 0.969697, cost 9.80455, change in cost 0.000181198\n",
      "step 31050, training accuracy 0.969697, cost 9.80437, change in cost 0.000180244\n",
      "step 31060, training accuracy 0.969697, cost 9.80418, change in cost 0.000181198\n",
      "step 31070, training accuracy 0.969697, cost 9.804, change in cost 0.000180244\n",
      "step 31080, training accuracy 0.969697, cost 9.80382, change in cost 0.000181198\n",
      "step 31090, training accuracy 0.969697, cost 9.80364, change in cost 0.000182152\n",
      "step 31100, training accuracy 0.969697, cost 9.80346, change in cost 0.000179291\n",
      "step 31110, training accuracy 0.969697, cost 9.80328, change in cost 0.000181198\n",
      "step 31120, training accuracy 0.969697, cost 9.8031, change in cost 0.000181198\n",
      "step 31130, training accuracy 0.969697, cost 9.80292, change in cost 0.000180244\n",
      "step 31140, training accuracy 0.969697, cost 9.80274, change in cost 0.000179291\n",
      "step 31150, training accuracy 0.969697, cost 9.80256, change in cost 0.000181198\n",
      "step 31160, training accuracy 0.969697, cost 9.80238, change in cost 0.000180244\n",
      "step 31170, training accuracy 0.969697, cost 9.8022, change in cost 0.000180244\n",
      "step 31180, training accuracy 0.969697, cost 9.80202, change in cost 0.000180244\n",
      "step 31190, training accuracy 0.969697, cost 9.80184, change in cost 0.000178337\n",
      "step 31200, training accuracy 0.969697, cost 9.80166, change in cost 0.000181198\n",
      "step 31210, training accuracy 0.969697, cost 9.80148, change in cost 0.000178337\n",
      "step 31220, training accuracy 0.969697, cost 9.8013, change in cost 0.000181198\n",
      "step 31230, training accuracy 0.969697, cost 9.80112, change in cost 0.000179291\n",
      "step 31240, training accuracy 0.969697, cost 9.80094, change in cost 0.000179291\n",
      "step 31250, training accuracy 0.969697, cost 9.80076, change in cost 0.000181198\n",
      "step 31260, training accuracy 0.969697, cost 9.80058, change in cost 0.000179291\n",
      "step 31270, training accuracy 0.969697, cost 9.8004, change in cost 0.000180244\n",
      "step 31280, training accuracy 0.969697, cost 9.80022, change in cost 0.000177383\n",
      "step 31290, training accuracy 0.969697, cost 9.80004, change in cost 0.000179291\n",
      "step 31300, training accuracy 0.969697, cost 9.79986, change in cost 0.000178337\n",
      "step 31310, training accuracy 0.969697, cost 9.79969, change in cost 0.000179291\n",
      "step 31320, training accuracy 0.969697, cost 9.79951, change in cost 0.000179291\n",
      "step 31330, training accuracy 0.969697, cost 9.79933, change in cost 0.000177383\n",
      "step 31340, training accuracy 0.969697, cost 9.79915, change in cost 0.000180244\n",
      "step 31350, training accuracy 0.969697, cost 9.79897, change in cost 0.000177383\n",
      "step 31360, training accuracy 0.969697, cost 9.79879, change in cost 0.000179291\n",
      "step 31370, training accuracy 0.969697, cost 9.79861, change in cost 0.000178337\n",
      "step 31380, training accuracy 0.969697, cost 9.79843, change in cost 0.000179291\n",
      "step 31390, training accuracy 0.969697, cost 9.79826, change in cost 0.000177383\n",
      "step 31400, training accuracy 0.969697, cost 9.79808, change in cost 0.000177383\n",
      "step 31410, training accuracy 0.969697, cost 9.7979, change in cost 0.000180244\n",
      "step 31420, training accuracy 0.969697, cost 9.79772, change in cost 0.000177383\n",
      "step 31430, training accuracy 0.969697, cost 9.79755, change in cost 0.00017643\n",
      "step 31440, training accuracy 0.969697, cost 9.79737, change in cost 0.000179291\n",
      "step 31450, training accuracy 0.969697, cost 9.79719, change in cost 0.000177383\n",
      "step 31460, training accuracy 0.969697, cost 9.79701, change in cost 0.000177383\n",
      "step 31470, training accuracy 0.969697, cost 9.79684, change in cost 0.000175476\n",
      "step 31480, training accuracy 0.969697, cost 9.79666, change in cost 0.000180244\n",
      "step 31490, training accuracy 0.969697, cost 9.79648, change in cost 0.000175476\n",
      "step 31500, training accuracy 0.969697, cost 9.7963, change in cost 0.000177383\n",
      "step 31510, training accuracy 0.969697, cost 9.79613, change in cost 0.00017643\n",
      "step 31520, training accuracy 0.969697, cost 9.79595, change in cost 0.000177383\n",
      "step 31530, training accuracy 0.969697, cost 9.79577, change in cost 0.000175476\n",
      "step 31540, training accuracy 0.969697, cost 9.7956, change in cost 0.000177383\n",
      "step 31550, training accuracy 0.969697, cost 9.79542, change in cost 0.000178337\n",
      "step 31560, training accuracy 0.969697, cost 9.79524, change in cost 0.000174522\n",
      "step 31570, training accuracy 0.969697, cost 9.79507, change in cost 0.00017643\n",
      "step 31580, training accuracy 0.969697, cost 9.79489, change in cost 0.00017643\n",
      "step 31590, training accuracy 0.969697, cost 9.79471, change in cost 0.000177383\n",
      "step 31600, training accuracy 0.969697, cost 9.79454, change in cost 0.000175476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31610, training accuracy 0.969697, cost 9.79436, change in cost 0.000175476\n",
      "step 31620, training accuracy 0.969697, cost 9.79419, change in cost 0.00017643\n",
      "step 31630, training accuracy 0.969697, cost 9.79401, change in cost 0.000177383\n",
      "step 31640, training accuracy 0.969697, cost 9.79383, change in cost 0.000175476\n",
      "step 31650, training accuracy 0.969697, cost 9.79366, change in cost 0.000174522\n",
      "step 31660, training accuracy 0.969697, cost 9.79348, change in cost 0.00017643\n",
      "step 31670, training accuracy 0.969697, cost 9.79331, change in cost 0.00017643\n",
      "step 31680, training accuracy 0.969697, cost 9.79313, change in cost 0.000175476\n",
      "step 31690, training accuracy 0.969697, cost 9.79295, change in cost 0.000175476\n",
      "step 31700, training accuracy 0.969697, cost 9.79278, change in cost 0.00017643\n",
      "step 31710, training accuracy 0.969697, cost 9.7926, change in cost 0.000175476\n",
      "step 31720, training accuracy 0.969697, cost 9.79243, change in cost 0.000174522\n",
      "step 31730, training accuracy 0.969697, cost 9.79225, change in cost 0.000177383\n",
      "step 31740, training accuracy 0.969697, cost 9.79208, change in cost 0.000172615\n",
      "step 31750, training accuracy 0.969697, cost 9.7919, change in cost 0.000175476\n",
      "step 31760, training accuracy 0.969697, cost 9.79173, change in cost 0.000175476\n",
      "step 31770, training accuracy 0.969697, cost 9.79155, change in cost 0.000175476\n",
      "step 31780, training accuracy 0.969697, cost 9.79138, change in cost 0.000175476\n",
      "step 31790, training accuracy 0.969697, cost 9.7912, change in cost 0.000171661\n",
      "step 31800, training accuracy 0.969697, cost 9.79103, change in cost 0.000175476\n",
      "step 31810, training accuracy 0.969697, cost 9.79085, change in cost 0.000174522\n",
      "step 31820, training accuracy 0.969697, cost 9.79068, change in cost 0.000174522\n",
      "step 31830, training accuracy 0.969697, cost 9.79051, change in cost 0.000173569\n",
      "step 31840, training accuracy 0.969697, cost 9.79033, change in cost 0.000173569\n",
      "step 31850, training accuracy 0.969697, cost 9.79016, change in cost 0.000174522\n",
      "step 31860, training accuracy 0.969697, cost 9.78998, change in cost 0.000173569\n",
      "step 31870, training accuracy 0.969697, cost 9.78981, change in cost 0.000174522\n",
      "step 31880, training accuracy 0.969697, cost 9.78964, change in cost 0.000174522\n",
      "step 31890, training accuracy 0.969697, cost 9.78946, change in cost 0.000172615\n",
      "step 31900, training accuracy 0.969697, cost 9.78929, change in cost 0.000174522\n",
      "step 31910, training accuracy 0.969697, cost 9.78912, change in cost 0.000171661\n",
      "step 31920, training accuracy 0.969697, cost 9.78894, change in cost 0.000175476\n",
      "step 31930, training accuracy 0.969697, cost 9.78877, change in cost 0.000173569\n",
      "step 31940, training accuracy 0.969697, cost 9.7886, change in cost 0.000172615\n",
      "step 31950, training accuracy 0.969697, cost 9.78842, change in cost 0.000174522\n",
      "step 31960, training accuracy 0.969697, cost 9.78825, change in cost 0.000173569\n",
      "step 31970, training accuracy 0.969697, cost 9.78807, change in cost 0.000172615\n",
      "step 31980, training accuracy 0.969697, cost 9.7879, change in cost 0.000172615\n",
      "step 31990, training accuracy 0.969697, cost 9.78773, change in cost 0.000172615\n",
      "step 32000, training accuracy 0.969697, cost 9.78756, change in cost 0.000171661\n",
      "step 32010, training accuracy 0.969697, cost 9.78738, change in cost 0.000173569\n",
      "step 32020, training accuracy 0.969697, cost 9.78721, change in cost 0.000171661\n",
      "step 32030, training accuracy 0.969697, cost 9.78704, change in cost 0.000174522\n",
      "step 32040, training accuracy 0.969697, cost 9.78687, change in cost 0.000172615\n",
      "step 32050, training accuracy 0.969697, cost 9.78669, change in cost 0.000170708\n",
      "step 32060, training accuracy 0.969697, cost 9.78652, change in cost 0.000171661\n",
      "step 32070, training accuracy 0.969697, cost 9.78635, change in cost 0.000173569\n",
      "step 32080, training accuracy 0.969697, cost 9.78618, change in cost 0.000172615\n",
      "step 32090, training accuracy 0.969697, cost 9.78601, change in cost 0.000171661\n",
      "step 32100, training accuracy 0.969697, cost 9.78583, change in cost 0.000172615\n",
      "step 32110, training accuracy 0.969697, cost 9.78566, change in cost 0.000172615\n",
      "step 32120, training accuracy 0.969697, cost 9.78549, change in cost 0.000170708\n",
      "step 32130, training accuracy 0.969697, cost 9.78532, change in cost 0.000171661\n",
      "step 32140, training accuracy 0.969697, cost 9.78514, change in cost 0.000172615\n",
      "step 32150, training accuracy 0.969697, cost 9.78497, change in cost 0.000171661\n",
      "step 32160, training accuracy 0.969697, cost 9.7848, change in cost 0.000172615\n",
      "step 32170, training accuracy 0.969697, cost 9.78463, change in cost 0.0001688\n",
      "step 32180, training accuracy 0.969697, cost 9.78446, change in cost 0.000173569\n",
      "step 32190, training accuracy 0.969697, cost 9.78429, change in cost 0.000170708\n",
      "step 32200, training accuracy 0.969697, cost 9.78412, change in cost 0.000169754\n",
      "step 32210, training accuracy 0.969697, cost 9.78395, change in cost 0.000171661\n",
      "step 32220, training accuracy 0.969697, cost 9.78378, change in cost 0.000170708\n",
      "step 32230, training accuracy 0.969697, cost 9.78361, change in cost 0.000169754\n",
      "step 32240, training accuracy 0.969697, cost 9.78343, change in cost 0.000172615\n",
      "step 32250, training accuracy 0.969697, cost 9.78326, change in cost 0.000169754\n",
      "step 32260, training accuracy 0.969697, cost 9.78309, change in cost 0.000169754\n",
      "step 32270, training accuracy 0.969697, cost 9.78292, change in cost 0.000169754\n",
      "step 32280, training accuracy 0.969697, cost 9.78275, change in cost 0.000169754\n",
      "step 32290, training accuracy 0.969697, cost 9.78258, change in cost 0.000172615\n",
      "step 32300, training accuracy 0.969697, cost 9.78241, change in cost 0.000169754\n",
      "step 32310, training accuracy 0.969697, cost 9.78224, change in cost 0.000169754\n",
      "step 32320, training accuracy 0.969697, cost 9.78207, change in cost 0.000169754\n",
      "step 32330, training accuracy 0.969697, cost 9.7819, change in cost 0.000169754\n",
      "step 32340, training accuracy 0.969697, cost 9.78173, change in cost 0.000170708\n",
      "step 32350, training accuracy 0.969697, cost 9.78156, change in cost 0.0001688\n",
      "step 32360, training accuracy 0.969697, cost 9.78139, change in cost 0.000169754\n",
      "step 32370, training accuracy 0.969697, cost 9.78122, change in cost 0.000171661\n",
      "step 32380, training accuracy 0.969697, cost 9.78105, change in cost 0.0001688\n",
      "step 32390, training accuracy 0.969697, cost 9.78088, change in cost 0.0001688\n",
      "step 32400, training accuracy 0.969697, cost 9.78071, change in cost 0.000169754\n",
      "step 32410, training accuracy 0.969697, cost 9.78054, change in cost 0.000170708\n",
      "step 32420, training accuracy 0.969697, cost 9.78037, change in cost 0.0001688\n",
      "step 32430, training accuracy 0.969697, cost 9.78021, change in cost 0.0001688\n",
      "step 32440, training accuracy 0.969697, cost 9.78004, change in cost 0.000169754\n",
      "step 32450, training accuracy 0.969697, cost 9.77987, change in cost 0.0001688\n",
      "step 32460, training accuracy 0.969697, cost 9.7797, change in cost 0.000169754\n",
      "step 32470, training accuracy 0.969697, cost 9.77953, change in cost 0.0001688\n",
      "step 32480, training accuracy 0.969697, cost 9.77936, change in cost 0.000166893\n",
      "step 32490, training accuracy 0.969697, cost 9.77919, change in cost 0.0001688\n",
      "step 32500, training accuracy 0.969697, cost 9.77902, change in cost 0.000169754\n",
      "step 32510, training accuracy 0.969697, cost 9.77886, change in cost 0.000165939\n",
      "step 32520, training accuracy 0.969697, cost 9.77869, change in cost 0.000169754\n",
      "step 32530, training accuracy 0.969697, cost 9.77852, change in cost 0.000167847\n",
      "step 32540, training accuracy 0.969697, cost 9.77835, change in cost 0.0001688\n",
      "step 32550, training accuracy 0.969697, cost 9.77818, change in cost 0.000167847\n",
      "step 32560, training accuracy 0.969697, cost 9.77801, change in cost 0.0001688\n",
      "step 32570, training accuracy 0.969697, cost 9.77785, change in cost 0.000166893\n",
      "step 32580, training accuracy 0.969697, cost 9.77768, change in cost 0.0001688\n",
      "step 32590, training accuracy 0.969697, cost 9.77751, change in cost 0.000167847\n",
      "step 32600, training accuracy 0.969697, cost 9.77734, change in cost 0.000167847\n",
      "step 32610, training accuracy 0.969697, cost 9.77718, change in cost 0.000166893\n",
      "step 32620, training accuracy 0.969697, cost 9.77701, change in cost 0.000166893\n",
      "step 32630, training accuracy 0.969697, cost 9.77684, change in cost 0.0001688\n",
      "step 32640, training accuracy 0.969697, cost 9.77667, change in cost 0.0001688\n",
      "step 32650, training accuracy 0.969697, cost 9.77651, change in cost 0.000165939\n",
      "step 32660, training accuracy 0.969697, cost 9.77634, change in cost 0.000166893\n",
      "step 32670, training accuracy 0.969697, cost 9.77617, change in cost 0.000167847\n",
      "step 32680, training accuracy 0.969697, cost 9.776, change in cost 0.000167847\n",
      "step 32690, training accuracy 0.969697, cost 9.77584, change in cost 0.000165939\n",
      "step 32700, training accuracy 0.969697, cost 9.77567, change in cost 0.000165939\n",
      "step 32710, training accuracy 0.969697, cost 9.7755, change in cost 0.000167847\n",
      "step 32720, training accuracy 0.969697, cost 9.77534, change in cost 0.000166893\n",
      "step 32730, training accuracy 0.969697, cost 9.77517, change in cost 0.000166893\n",
      "step 32740, training accuracy 0.969697, cost 9.775, change in cost 0.000164986\n",
      "step 32750, training accuracy 0.969697, cost 9.77484, change in cost 0.000165939\n",
      "step 32760, training accuracy 0.969697, cost 9.77467, change in cost 0.0001688\n",
      "step 32770, training accuracy 0.969697, cost 9.7745, change in cost 0.000164986\n",
      "step 32780, training accuracy 0.969697, cost 9.77434, change in cost 0.000166893\n",
      "step 32790, training accuracy 0.969697, cost 9.77417, change in cost 0.000165939\n",
      "step 32800, training accuracy 0.969697, cost 9.774, change in cost 0.000166893\n",
      "step 32810, training accuracy 0.969697, cost 9.77384, change in cost 0.000164032\n",
      "step 32820, training accuracy 0.969697, cost 9.77367, change in cost 0.000165939\n",
      "step 32830, training accuracy 0.969697, cost 9.77351, change in cost 0.000165939\n",
      "step 32840, training accuracy 0.969697, cost 9.77334, change in cost 0.000167847\n",
      "step 32850, training accuracy 0.969697, cost 9.77318, change in cost 0.000164986\n",
      "step 32860, training accuracy 0.969697, cost 9.77301, change in cost 0.000164986\n",
      "step 32870, training accuracy 0.969697, cost 9.77285, change in cost 0.000164986\n",
      "step 32880, training accuracy 0.969697, cost 9.77268, change in cost 0.000166893\n",
      "step 32890, training accuracy 0.969697, cost 9.77251, change in cost 0.000164986\n",
      "step 32900, training accuracy 0.969697, cost 9.77235, change in cost 0.000165939\n",
      "step 32910, training accuracy 0.969697, cost 9.77218, change in cost 0.000164986\n",
      "step 32920, training accuracy 0.969697, cost 9.77202, change in cost 0.000166893\n",
      "step 32930, training accuracy 0.969697, cost 9.77185, change in cost 0.000164032\n",
      "step 32940, training accuracy 0.969697, cost 9.77169, change in cost 0.000164032\n",
      "step 32950, training accuracy 0.969697, cost 9.77152, change in cost 0.000164986\n",
      "step 32960, training accuracy 0.969697, cost 9.77136, change in cost 0.000165939\n",
      "step 32970, training accuracy 0.969697, cost 9.77119, change in cost 0.000164032\n",
      "step 32980, training accuracy 0.969697, cost 9.77103, change in cost 0.000164032\n",
      "step 32990, training accuracy 0.969697, cost 9.77087, change in cost 0.000164032\n",
      "step 33000, training accuracy 0.969697, cost 9.7707, change in cost 0.000164986\n",
      "step 33010, training accuracy 0.969697, cost 9.77054, change in cost 0.000164032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 33020, training accuracy 0.969697, cost 9.77037, change in cost 0.000164032\n",
      "step 33030, training accuracy 0.969697, cost 9.77021, change in cost 0.000164032\n",
      "step 33040, training accuracy 0.969697, cost 9.77004, change in cost 0.000164986\n",
      "step 33050, training accuracy 0.969697, cost 9.76988, change in cost 0.000165939\n",
      "step 33060, training accuracy 0.969697, cost 9.76972, change in cost 0.000161171\n",
      "step 33070, training accuracy 0.969697, cost 9.76955, change in cost 0.000164032\n",
      "step 33080, training accuracy 0.969697, cost 9.76939, change in cost 0.000163078\n",
      "step 33090, training accuracy 0.969697, cost 9.76922, change in cost 0.000165939\n",
      "step 33100, training accuracy 0.969697, cost 9.76906, change in cost 0.000163078\n",
      "step 33110, training accuracy 0.969697, cost 9.7689, change in cost 0.000163078\n",
      "step 33120, training accuracy 0.969697, cost 9.76873, change in cost 0.000163078\n",
      "step 33130, training accuracy 0.969697, cost 9.76857, change in cost 0.000164986\n",
      "step 33140, training accuracy 0.969697, cost 9.7684, change in cost 0.000164032\n",
      "step 33150, training accuracy 0.969697, cost 9.76824, change in cost 0.000162125\n",
      "step 33160, training accuracy 0.969697, cost 9.76808, change in cost 0.000163078\n",
      "step 33170, training accuracy 0.969697, cost 9.76792, change in cost 0.000164032\n",
      "step 33180, training accuracy 0.969697, cost 9.76775, change in cost 0.000164032\n",
      "step 33190, training accuracy 0.969697, cost 9.76759, change in cost 0.000161171\n",
      "step 33200, training accuracy 0.969697, cost 9.76743, change in cost 0.000164032\n",
      "step 33210, training accuracy 0.969697, cost 9.76727, change in cost 0.000161171\n",
      "step 33220, training accuracy 0.969697, cost 9.7671, change in cost 0.000164032\n",
      "step 33230, training accuracy 0.969697, cost 9.76694, change in cost 0.000161171\n",
      "step 33240, training accuracy 0.969697, cost 9.76678, change in cost 0.000162125\n",
      "step 33250, training accuracy 0.969697, cost 9.76661, change in cost 0.000163078\n",
      "step 33260, training accuracy 0.969697, cost 9.76645, change in cost 0.000163078\n",
      "step 33270, training accuracy 0.969697, cost 9.76629, change in cost 0.000163078\n",
      "step 33280, training accuracy 0.969697, cost 9.76613, change in cost 0.000162125\n",
      "step 33290, training accuracy 0.969697, cost 9.76596, change in cost 0.000162125\n",
      "step 33300, training accuracy 0.969697, cost 9.7658, change in cost 0.000163078\n",
      "step 33310, training accuracy 0.969697, cost 9.76564, change in cost 0.000160217\n",
      "step 33320, training accuracy 0.969697, cost 9.76548, change in cost 0.000161171\n",
      "step 33330, training accuracy 0.969697, cost 9.76532, change in cost 0.000162125\n",
      "step 33340, training accuracy 0.969697, cost 9.76515, change in cost 0.000163078\n",
      "step 33350, training accuracy 0.969697, cost 9.76499, change in cost 0.000161171\n",
      "step 33360, training accuracy 0.969697, cost 9.76483, change in cost 0.000162125\n",
      "step 33370, training accuracy 0.969697, cost 9.76467, change in cost 0.000161171\n",
      "step 33380, training accuracy 0.969697, cost 9.76451, change in cost 0.000161171\n",
      "step 33390, training accuracy 0.969697, cost 9.76435, change in cost 0.000163078\n",
      "step 33400, training accuracy 0.969697, cost 9.76419, change in cost 0.000160217\n",
      "step 33410, training accuracy 0.969697, cost 9.76402, change in cost 0.000162125\n",
      "step 33420, training accuracy 0.969697, cost 9.76386, change in cost 0.000161171\n",
      "step 33430, training accuracy 0.969697, cost 9.7637, change in cost 0.000160217\n",
      "step 33440, training accuracy 0.969697, cost 9.76354, change in cost 0.000162125\n",
      "step 33450, training accuracy 0.969697, cost 9.76338, change in cost 0.000159264\n",
      "step 33460, training accuracy 0.969697, cost 9.76322, change in cost 0.000162125\n",
      "step 33470, training accuracy 0.969697, cost 9.76306, change in cost 0.000159264\n",
      "step 33480, training accuracy 0.969697, cost 9.7629, change in cost 0.000159264\n",
      "step 33490, training accuracy 0.969697, cost 9.76274, change in cost 0.000162125\n",
      "step 33500, training accuracy 0.969697, cost 9.76258, change in cost 0.000161171\n",
      "step 33510, training accuracy 0.969697, cost 9.76242, change in cost 0.00015831\n",
      "step 33520, training accuracy 0.969697, cost 9.76226, change in cost 0.000160217\n",
      "step 33530, training accuracy 0.969697, cost 9.7621, change in cost 0.000160217\n",
      "step 33540, training accuracy 0.969697, cost 9.76194, change in cost 0.000162125\n",
      "step 33550, training accuracy 0.969697, cost 9.76178, change in cost 0.000159264\n",
      "step 33560, training accuracy 0.969697, cost 9.76162, change in cost 0.000159264\n",
      "step 33570, training accuracy 0.969697, cost 9.76146, change in cost 0.000161171\n",
      "step 33580, training accuracy 0.969697, cost 9.7613, change in cost 0.000160217\n",
      "step 33590, training accuracy 0.969697, cost 9.76114, change in cost 0.000159264\n",
      "step 33600, training accuracy 0.969697, cost 9.76098, change in cost 0.00015831\n",
      "step 33610, training accuracy 0.969697, cost 9.76082, change in cost 0.000161171\n",
      "step 33620, training accuracy 0.969697, cost 9.76066, change in cost 0.000160217\n",
      "step 33630, training accuracy 0.969697, cost 9.7605, change in cost 0.000160217\n",
      "step 33640, training accuracy 0.969697, cost 9.76034, change in cost 0.00015831\n",
      "step 33650, training accuracy 0.969697, cost 9.76018, change in cost 0.000160217\n",
      "step 33660, training accuracy 0.969697, cost 9.76002, change in cost 0.000160217\n",
      "step 33670, training accuracy 0.969697, cost 9.75986, change in cost 0.000159264\n",
      "step 33680, training accuracy 0.969697, cost 9.7597, change in cost 0.000160217\n",
      "step 33690, training accuracy 0.969697, cost 9.75954, change in cost 0.00015831\n",
      "step 33700, training accuracy 0.969697, cost 9.75938, change in cost 0.000159264\n",
      "step 33710, training accuracy 0.969697, cost 9.75922, change in cost 0.000157356\n",
      "step 33720, training accuracy 0.969697, cost 9.75906, change in cost 0.000159264\n",
      "step 33730, training accuracy 0.969697, cost 9.75891, change in cost 0.000159264\n",
      "step 33740, training accuracy 0.969697, cost 9.75875, change in cost 0.00015831\n",
      "step 33750, training accuracy 0.969697, cost 9.75859, change in cost 0.000159264\n",
      "step 33760, training accuracy 0.969697, cost 9.75843, change in cost 0.00015831\n",
      "step 33770, training accuracy 0.969697, cost 9.75827, change in cost 0.000157356\n",
      "step 33780, training accuracy 0.969697, cost 9.75811, change in cost 0.00015831\n",
      "step 33790, training accuracy 0.969697, cost 9.75796, change in cost 0.00015831\n",
      "step 33800, training accuracy 0.969697, cost 9.7578, change in cost 0.00015831\n",
      "step 33810, training accuracy 0.969697, cost 9.75764, change in cost 0.00015831\n",
      "step 33820, training accuracy 0.969697, cost 9.75748, change in cost 0.000157356\n",
      "step 33830, training accuracy 0.969697, cost 9.75732, change in cost 0.00015831\n",
      "step 33840, training accuracy 0.969697, cost 9.75717, change in cost 0.000157356\n",
      "step 33850, training accuracy 0.969697, cost 9.75701, change in cost 0.000159264\n",
      "step 33860, training accuracy 0.969697, cost 9.75685, change in cost 0.000156403\n",
      "step 33870, training accuracy 0.969697, cost 9.75669, change in cost 0.000157356\n",
      "step 33880, training accuracy 0.969697, cost 9.75653, change in cost 0.00015831\n",
      "step 33890, training accuracy 0.969697, cost 9.75638, change in cost 0.000159264\n",
      "step 33900, training accuracy 0.969697, cost 9.75622, change in cost 0.000156403\n",
      "step 33910, training accuracy 0.969697, cost 9.75606, change in cost 0.000157356\n",
      "step 33920, training accuracy 0.969697, cost 9.7559, change in cost 0.000157356\n",
      "step 33930, training accuracy 0.969697, cost 9.75575, change in cost 0.000157356\n",
      "step 33940, training accuracy 0.969697, cost 9.75559, change in cost 0.000156403\n",
      "step 33950, training accuracy 0.969697, cost 9.75543, change in cost 0.00015831\n",
      "step 33960, training accuracy 0.969697, cost 9.75528, change in cost 0.000155449\n",
      "step 33970, training accuracy 0.969697, cost 9.75512, change in cost 0.000157356\n",
      "step 33980, training accuracy 0.969697, cost 9.75496, change in cost 0.000157356\n",
      "step 33990, training accuracy 0.969697, cost 9.75481, change in cost 0.000156403\n",
      "step 34000, training accuracy 0.969697, cost 9.75465, change in cost 0.000156403\n",
      "step 34010, training accuracy 0.969697, cost 9.75449, change in cost 0.000156403\n",
      "step 34020, training accuracy 0.969697, cost 9.75434, change in cost 0.000157356\n",
      "step 34030, training accuracy 0.969697, cost 9.75418, change in cost 0.000156403\n",
      "step 34040, training accuracy 0.969697, cost 9.75402, change in cost 0.000156403\n",
      "step 34050, training accuracy 0.969697, cost 9.75387, change in cost 0.000156403\n",
      "step 34060, training accuracy 0.969697, cost 9.75371, change in cost 0.000155449\n",
      "step 34070, training accuracy 0.969697, cost 9.75355, change in cost 0.000156403\n",
      "step 34080, training accuracy 0.969697, cost 9.7534, change in cost 0.000157356\n",
      "step 34090, training accuracy 0.969697, cost 9.75324, change in cost 0.000155449\n",
      "step 34100, training accuracy 0.969697, cost 9.75309, change in cost 0.000155449\n",
      "step 34110, training accuracy 0.969697, cost 9.75293, change in cost 0.00015831\n",
      "step 34120, training accuracy 0.969697, cost 9.75277, change in cost 0.000154495\n",
      "step 34130, training accuracy 0.969697, cost 9.75262, change in cost 0.000156403\n",
      "step 34140, training accuracy 0.969697, cost 9.75246, change in cost 0.000156403\n",
      "step 34150, training accuracy 0.969697, cost 9.75231, change in cost 0.000154495\n",
      "step 34160, training accuracy 0.969697, cost 9.75215, change in cost 0.000155449\n",
      "step 34170, training accuracy 0.969697, cost 9.75199, change in cost 0.000156403\n",
      "step 34180, training accuracy 0.969697, cost 9.75184, change in cost 0.000154495\n",
      "step 34190, training accuracy 0.969697, cost 9.75168, change in cost 0.000156403\n",
      "step 34200, training accuracy 0.969697, cost 9.75153, change in cost 0.000155449\n",
      "step 34210, training accuracy 0.969697, cost 9.75137, change in cost 0.000153542\n",
      "step 34220, training accuracy 0.969697, cost 9.75122, change in cost 0.000155449\n",
      "step 34230, training accuracy 0.969697, cost 9.75106, change in cost 0.000154495\n",
      "step 34240, training accuracy 0.969697, cost 9.75091, change in cost 0.000152588\n",
      "step 34250, training accuracy 0.969697, cost 9.75075, change in cost 0.000157356\n",
      "step 34260, training accuracy 0.969697, cost 9.7506, change in cost 0.000153542\n",
      "step 34270, training accuracy 0.969697, cost 9.75045, change in cost 0.000154495\n",
      "step 34280, training accuracy 0.969697, cost 9.75029, change in cost 0.000153542\n",
      "step 34290, training accuracy 0.969697, cost 9.75014, change in cost 0.000155449\n",
      "step 34300, training accuracy 0.969697, cost 9.74998, change in cost 0.000154495\n",
      "step 34310, training accuracy 0.969697, cost 9.74983, change in cost 0.000153542\n",
      "step 34320, training accuracy 0.969697, cost 9.74967, change in cost 0.000155449\n",
      "step 34330, training accuracy 0.969697, cost 9.74952, change in cost 0.000153542\n",
      "step 34340, training accuracy 0.969697, cost 9.74937, change in cost 0.000154495\n",
      "step 34350, training accuracy 0.969697, cost 9.74921, change in cost 0.000153542\n",
      "step 34360, training accuracy 0.969697, cost 9.74906, change in cost 0.000155449\n",
      "step 34370, training accuracy 0.969697, cost 9.7489, change in cost 0.000152588\n",
      "step 34380, training accuracy 0.969697, cost 9.74875, change in cost 0.000155449\n",
      "step 34390, training accuracy 0.969697, cost 9.7486, change in cost 0.000152588\n",
      "step 34400, training accuracy 0.969697, cost 9.74844, change in cost 0.000154495\n",
      "step 34410, training accuracy 0.969697, cost 9.74829, change in cost 0.000153542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 34420, training accuracy 0.969697, cost 9.74813, change in cost 0.000154495\n",
      "step 34430, training accuracy 0.969697, cost 9.74798, change in cost 0.000153542\n",
      "step 34440, training accuracy 0.969697, cost 9.74783, change in cost 0.000154495\n",
      "step 34450, training accuracy 0.969697, cost 9.74767, change in cost 0.000152588\n",
      "step 34460, training accuracy 0.969697, cost 9.74752, change in cost 0.000152588\n",
      "step 34470, training accuracy 0.969697, cost 9.74737, change in cost 0.000154495\n",
      "step 34480, training accuracy 0.969697, cost 9.74721, change in cost 0.000152588\n",
      "step 34490, training accuracy 0.969697, cost 9.74706, change in cost 0.000153542\n",
      "step 34500, training accuracy 0.969697, cost 9.74691, change in cost 0.000153542\n",
      "step 34510, training accuracy 0.969697, cost 9.74675, change in cost 0.000152588\n",
      "step 34520, training accuracy 0.969697, cost 9.7466, change in cost 0.000151634\n",
      "step 34530, training accuracy 0.969697, cost 9.74645, change in cost 0.000153542\n",
      "step 34540, training accuracy 0.969697, cost 9.74629, change in cost 0.000153542\n",
      "step 34550, training accuracy 0.969697, cost 9.74614, change in cost 0.000152588\n",
      "step 34560, training accuracy 0.969697, cost 9.74599, change in cost 0.000152588\n",
      "step 34570, training accuracy 0.969697, cost 9.74584, change in cost 0.000152588\n",
      "step 34580, training accuracy 0.969697, cost 9.74569, change in cost 0.000151634\n",
      "step 34590, training accuracy 0.969697, cost 9.74553, change in cost 0.000152588\n",
      "step 34600, training accuracy 0.969697, cost 9.74538, change in cost 0.000152588\n",
      "step 34610, training accuracy 0.969697, cost 9.74523, change in cost 0.000151634\n",
      "step 34620, training accuracy 0.969697, cost 9.74508, change in cost 0.000152588\n",
      "step 34630, training accuracy 0.969697, cost 9.74493, change in cost 0.000150681\n",
      "step 34640, training accuracy 0.969697, cost 9.74477, change in cost 0.000152588\n",
      "step 34650, training accuracy 0.969697, cost 9.74462, change in cost 0.000151634\n",
      "step 34660, training accuracy 0.969697, cost 9.74447, change in cost 0.000152588\n",
      "step 34670, training accuracy 0.969697, cost 9.74432, change in cost 0.000152588\n",
      "step 34680, training accuracy 0.969697, cost 9.74416, change in cost 0.000151634\n",
      "step 34690, training accuracy 0.969697, cost 9.74401, change in cost 0.000150681\n",
      "step 34700, training accuracy 0.969697, cost 9.74386, change in cost 0.000151634\n",
      "step 34710, training accuracy 0.969697, cost 9.74371, change in cost 0.000151634\n",
      "step 34720, training accuracy 0.969697, cost 9.74356, change in cost 0.000150681\n",
      "step 34730, training accuracy 0.969697, cost 9.74341, change in cost 0.000150681\n",
      "step 34740, training accuracy 0.969697, cost 9.74326, change in cost 0.000152588\n",
      "step 34750, training accuracy 0.969697, cost 9.74311, change in cost 0.000149727\n",
      "step 34760, training accuracy 0.969697, cost 9.74296, change in cost 0.000151634\n",
      "step 34770, training accuracy 0.969697, cost 9.7428, change in cost 0.000150681\n",
      "step 34780, training accuracy 0.969697, cost 9.74265, change in cost 0.000149727\n",
      "step 34790, training accuracy 0.969697, cost 9.7425, change in cost 0.000150681\n",
      "step 34800, training accuracy 0.969697, cost 9.74235, change in cost 0.000152588\n",
      "step 34810, training accuracy 0.969697, cost 9.7422, change in cost 0.000149727\n",
      "step 34820, training accuracy 0.969697, cost 9.74205, change in cost 0.000150681\n",
      "step 34830, training accuracy 0.969697, cost 9.7419, change in cost 0.000150681\n",
      "step 34840, training accuracy 0.969697, cost 9.74175, change in cost 0.000151634\n",
      "step 34850, training accuracy 0.969697, cost 9.7416, change in cost 0.000150681\n",
      "step 34860, training accuracy 0.969697, cost 9.74145, change in cost 0.000150681\n",
      "step 34870, training accuracy 0.969697, cost 9.7413, change in cost 0.000149727\n",
      "step 34880, training accuracy 0.969697, cost 9.74115, change in cost 0.000149727\n",
      "step 34890, training accuracy 0.969697, cost 9.741, change in cost 0.000151634\n",
      "step 34900, training accuracy 0.969697, cost 9.74085, change in cost 0.000150681\n",
      "step 34910, training accuracy 0.969697, cost 9.74069, change in cost 0.000150681\n",
      "step 34920, training accuracy 0.969697, cost 9.74054, change in cost 0.000151634\n",
      "step 34930, training accuracy 0.969697, cost 9.7404, change in cost 0.000146866\n",
      "step 34940, training accuracy 0.969697, cost 9.74024, change in cost 0.000151634\n",
      "step 34950, training accuracy 0.969697, cost 9.74009, change in cost 0.000150681\n",
      "step 34960, training accuracy 0.969697, cost 9.73995, change in cost 0.00014782\n",
      "step 34970, training accuracy 0.969697, cost 9.7398, change in cost 0.000150681\n",
      "step 34980, training accuracy 0.969697, cost 9.73965, change in cost 0.000148773\n",
      "step 34990, training accuracy 0.969697, cost 9.7395, change in cost 0.000151634\n",
      "step 35000, training accuracy 0.969697, cost 9.73935, change in cost 0.000146866\n",
      "step 35010, training accuracy 0.969697, cost 9.7392, change in cost 0.000149727\n",
      "step 35020, training accuracy 0.969697, cost 9.73905, change in cost 0.000149727\n",
      "step 35030, training accuracy 0.969697, cost 9.7389, change in cost 0.000149727\n",
      "step 35040, training accuracy 0.969697, cost 9.73875, change in cost 0.000146866\n",
      "step 35050, training accuracy 0.969697, cost 9.7386, change in cost 0.000148773\n",
      "step 35060, training accuracy 0.969697, cost 9.73845, change in cost 0.000149727\n",
      "step 35070, training accuracy 0.969697, cost 9.73831, change in cost 0.000148773\n",
      "step 35080, training accuracy 0.969697, cost 9.73816, change in cost 0.000148773\n",
      "step 35090, training accuracy 0.969697, cost 9.73801, change in cost 0.000148773\n",
      "step 35100, training accuracy 0.969697, cost 9.73786, change in cost 0.000148773\n",
      "step 35110, training accuracy 0.969697, cost 9.73771, change in cost 0.000148773\n",
      "step 35120, training accuracy 0.969697, cost 9.73756, change in cost 0.00014782\n",
      "step 35130, training accuracy 0.969697, cost 9.73741, change in cost 0.000149727\n",
      "step 35140, training accuracy 0.969697, cost 9.73726, change in cost 0.00014782\n",
      "step 35150, training accuracy 0.969697, cost 9.73712, change in cost 0.000148773\n",
      "step 35160, training accuracy 0.969697, cost 9.73697, change in cost 0.000148773\n",
      "step 35170, training accuracy 0.969697, cost 9.73682, change in cost 0.00014782\n",
      "step 35180, training accuracy 0.969697, cost 9.73667, change in cost 0.000148773\n",
      "step 35190, training accuracy 0.969697, cost 9.73652, change in cost 0.000148773\n",
      "step 35200, training accuracy 0.969697, cost 9.73637, change in cost 0.000148773\n",
      "step 35210, training accuracy 0.969697, cost 9.73623, change in cost 0.000146866\n",
      "step 35220, training accuracy 0.969697, cost 9.73608, change in cost 0.000146866\n",
      "step 35230, training accuracy 0.969697, cost 9.73593, change in cost 0.000149727\n",
      "step 35240, training accuracy 0.969697, cost 9.73578, change in cost 0.000146866\n",
      "step 35250, training accuracy 0.969697, cost 9.73563, change in cost 0.00014782\n",
      "step 35260, training accuracy 0.969697, cost 9.73549, change in cost 0.00014782\n",
      "step 35270, training accuracy 0.969697, cost 9.73534, change in cost 0.000146866\n",
      "step 35280, training accuracy 0.969697, cost 9.73519, change in cost 0.00014782\n",
      "step 35290, training accuracy 0.969697, cost 9.73504, change in cost 0.00014782\n",
      "step 35300, training accuracy 0.969697, cost 9.7349, change in cost 0.000146866\n",
      "step 35310, training accuracy 0.969697, cost 9.73475, change in cost 0.000145912\n",
      "step 35320, training accuracy 0.969697, cost 9.7346, change in cost 0.00014782\n",
      "step 35330, training accuracy 0.969697, cost 9.73446, change in cost 0.000146866\n",
      "step 35340, training accuracy 0.969697, cost 9.73431, change in cost 0.000146866\n",
      "step 35350, training accuracy 0.969697, cost 9.73416, change in cost 0.000148773\n",
      "step 35360, training accuracy 0.969697, cost 9.73402, change in cost 0.000144958\n",
      "step 35370, training accuracy 0.969697, cost 9.73387, change in cost 0.000146866\n",
      "step 35380, training accuracy 0.969697, cost 9.73372, change in cost 0.00014782\n",
      "step 35390, training accuracy 0.969697, cost 9.73357, change in cost 0.000146866\n",
      "step 35400, training accuracy 0.969697, cost 9.73343, change in cost 0.000145912\n",
      "step 35410, training accuracy 0.969697, cost 9.73328, change in cost 0.00014782\n",
      "step 35420, training accuracy 0.969697, cost 9.73313, change in cost 0.000146866\n",
      "step 35430, training accuracy 0.969697, cost 9.73299, change in cost 0.000145912\n",
      "step 35440, training accuracy 0.969697, cost 9.73284, change in cost 0.000146866\n",
      "step 35450, training accuracy 0.969697, cost 9.73269, change in cost 0.00014782\n",
      "step 35460, training accuracy 0.969697, cost 9.73255, change in cost 0.000146866\n",
      "step 35470, training accuracy 0.969697, cost 9.7324, change in cost 0.000144958\n",
      "step 35480, training accuracy 0.969697, cost 9.73226, change in cost 0.000145912\n",
      "step 35490, training accuracy 0.969697, cost 9.73211, change in cost 0.000145912\n",
      "step 35500, training accuracy 0.969697, cost 9.73197, change in cost 0.000144958\n",
      "step 35510, training accuracy 0.969697, cost 9.73182, change in cost 0.000145912\n",
      "step 35520, training accuracy 0.969697, cost 9.73167, change in cost 0.000145912\n",
      "step 35530, training accuracy 0.969697, cost 9.73153, change in cost 0.000144958\n",
      "step 35540, training accuracy 0.969697, cost 9.73138, change in cost 0.000145912\n",
      "step 35550, training accuracy 0.969697, cost 9.73124, change in cost 0.000146866\n",
      "step 35560, training accuracy 0.969697, cost 9.73109, change in cost 0.000144005\n",
      "step 35570, training accuracy 0.969697, cost 9.73094, change in cost 0.000146866\n",
      "step 35580, training accuracy 0.969697, cost 9.7308, change in cost 0.000144005\n",
      "step 35590, training accuracy 0.969697, cost 9.73066, change in cost 0.000144005\n",
      "step 35600, training accuracy 0.969697, cost 9.73051, change in cost 0.000146866\n",
      "step 35610, training accuracy 0.969697, cost 9.73037, change in cost 0.000143051\n",
      "step 35620, training accuracy 0.969697, cost 9.73022, change in cost 0.00014782\n",
      "step 35630, training accuracy 0.969697, cost 9.73007, change in cost 0.000144958\n",
      "step 35640, training accuracy 0.969697, cost 9.72993, change in cost 0.000144005\n",
      "step 35650, training accuracy 0.969697, cost 9.72978, change in cost 0.000145912\n",
      "step 35660, training accuracy 0.969697, cost 9.72964, change in cost 0.000143051\n",
      "step 35670, training accuracy 0.969697, cost 9.7295, change in cost 0.000145912\n",
      "step 35680, training accuracy 0.969697, cost 9.72935, change in cost 0.000144958\n",
      "step 35690, training accuracy 0.969697, cost 9.7292, change in cost 0.000145912\n",
      "step 35700, training accuracy 0.969697, cost 9.72906, change in cost 0.000144005\n",
      "step 35710, training accuracy 0.969697, cost 9.72892, change in cost 0.000144958\n",
      "step 35720, training accuracy 0.969697, cost 9.72877, change in cost 0.000145912\n",
      "step 35730, training accuracy 0.969697, cost 9.72863, change in cost 0.000144005\n",
      "step 35740, training accuracy 0.969697, cost 9.72848, change in cost 0.000144005\n",
      "step 35750, training accuracy 0.969697, cost 9.72834, change in cost 0.000144005\n",
      "step 35760, training accuracy 0.969697, cost 9.72819, change in cost 0.000144005\n",
      "step 35770, training accuracy 0.969697, cost 9.72805, change in cost 0.000144005\n",
      "step 35780, training accuracy 0.969697, cost 9.72791, change in cost 0.000144005\n",
      "step 35790, training accuracy 0.969697, cost 9.72776, change in cost 0.000144005\n",
      "step 35800, training accuracy 0.969697, cost 9.72762, change in cost 0.000143051\n",
      "step 35810, training accuracy 0.969697, cost 9.72747, change in cost 0.000144005\n",
      "step 35820, training accuracy 0.969697, cost 9.72733, change in cost 0.000144005\n",
      "step 35830, training accuracy 0.969697, cost 9.72719, change in cost 0.000144958\n",
      "step 35840, training accuracy 0.969697, cost 9.72704, change in cost 0.000143051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 35850, training accuracy 0.969697, cost 9.7269, change in cost 0.000142097\n",
      "step 35860, training accuracy 0.969697, cost 9.72676, change in cost 0.000144005\n",
      "step 35870, training accuracy 0.969697, cost 9.72661, change in cost 0.000144005\n",
      "step 35880, training accuracy 0.969697, cost 9.72647, change in cost 0.000144005\n",
      "step 35890, training accuracy 0.969697, cost 9.72633, change in cost 0.000143051\n",
      "step 35900, training accuracy 0.969697, cost 9.72618, change in cost 0.000143051\n",
      "step 35910, training accuracy 0.969697, cost 9.72604, change in cost 0.000143051\n",
      "step 35920, training accuracy 0.969697, cost 9.72589, change in cost 0.000144958\n",
      "step 35930, training accuracy 0.969697, cost 9.72575, change in cost 0.000144005\n",
      "step 35940, training accuracy 0.969697, cost 9.72561, change in cost 0.000141144\n",
      "step 35950, training accuracy 0.969697, cost 9.72547, change in cost 0.000143051\n",
      "step 35960, training accuracy 0.969697, cost 9.72532, change in cost 0.000143051\n",
      "step 35970, training accuracy 0.969697, cost 9.72518, change in cost 0.000141144\n",
      "step 35980, training accuracy 0.969697, cost 9.72504, change in cost 0.000144005\n",
      "step 35990, training accuracy 0.969697, cost 9.72489, change in cost 0.000143051\n",
      "step 36000, training accuracy 0.969697, cost 9.72475, change in cost 0.000143051\n",
      "step 36010, training accuracy 0.969697, cost 9.72461, change in cost 0.000143051\n",
      "step 36020, training accuracy 0.969697, cost 9.72447, change in cost 0.000143051\n",
      "step 36030, training accuracy 0.969697, cost 9.72432, change in cost 0.000142097\n",
      "step 36040, training accuracy 0.969697, cost 9.72418, change in cost 0.000142097\n",
      "step 36050, training accuracy 0.969697, cost 9.72404, change in cost 0.00014019\n",
      "step 36060, training accuracy 0.969697, cost 9.7239, change in cost 0.000143051\n",
      "step 36070, training accuracy 0.969697, cost 9.72376, change in cost 0.000141144\n",
      "step 36080, training accuracy 0.969697, cost 9.72361, change in cost 0.000144005\n",
      "step 36090, training accuracy 0.969697, cost 9.72347, change in cost 0.000141144\n",
      "step 36100, training accuracy 0.969697, cost 9.72333, change in cost 0.000141144\n",
      "step 36110, training accuracy 0.969697, cost 9.72319, change in cost 0.000143051\n",
      "step 36120, training accuracy 0.969697, cost 9.72305, change in cost 0.000141144\n",
      "step 36130, training accuracy 0.969697, cost 9.72291, change in cost 0.000141144\n",
      "step 36140, training accuracy 0.969697, cost 9.72276, change in cost 0.000142097\n",
      "step 36150, training accuracy 0.969697, cost 9.72262, change in cost 0.000141144\n",
      "step 36160, training accuracy 0.969697, cost 9.72248, change in cost 0.000143051\n",
      "step 36170, training accuracy 0.969697, cost 9.72234, change in cost 0.00014019\n",
      "step 36180, training accuracy 0.969697, cost 9.7222, change in cost 0.000142097\n",
      "step 36190, training accuracy 0.969697, cost 9.72205, change in cost 0.000142097\n",
      "step 36200, training accuracy 0.969697, cost 9.72191, change in cost 0.00014019\n",
      "step 36210, training accuracy 0.969697, cost 9.72177, change in cost 0.000142097\n",
      "step 36220, training accuracy 0.969697, cost 9.72163, change in cost 0.000141144\n",
      "step 36230, training accuracy 0.969697, cost 9.72149, change in cost 0.00014019\n",
      "step 36240, training accuracy 0.969697, cost 9.72135, change in cost 0.000142097\n",
      "step 36250, training accuracy 0.969697, cost 9.72121, change in cost 0.000142097\n",
      "step 36260, training accuracy 0.969697, cost 9.72107, change in cost 0.00014019\n",
      "step 36270, training accuracy 0.969697, cost 9.72093, change in cost 0.000141144\n",
      "step 36280, training accuracy 0.969697, cost 9.72079, change in cost 0.00014019\n",
      "step 36290, training accuracy 0.969697, cost 9.72064, change in cost 0.000141144\n",
      "step 36300, training accuracy 0.969697, cost 9.7205, change in cost 0.000141144\n",
      "step 36310, training accuracy 0.969697, cost 9.72036, change in cost 0.00014019\n",
      "step 36320, training accuracy 0.969697, cost 9.72022, change in cost 0.00014019\n",
      "step 36330, training accuracy 0.969697, cost 9.72008, change in cost 0.00014019\n",
      "step 36340, training accuracy 0.969697, cost 9.71994, change in cost 0.000141144\n",
      "step 36350, training accuracy 0.969697, cost 9.7198, change in cost 0.000138283\n",
      "step 36360, training accuracy 0.969697, cost 9.71966, change in cost 0.000141144\n",
      "step 36370, training accuracy 0.969697, cost 9.71952, change in cost 0.00014019\n",
      "step 36380, training accuracy 0.969697, cost 9.71938, change in cost 0.00014019\n",
      "step 36390, training accuracy 0.969697, cost 9.71924, change in cost 0.000139236\n",
      "step 36400, training accuracy 0.969697, cost 9.7191, change in cost 0.00014019\n",
      "step 36410, training accuracy 0.969697, cost 9.71896, change in cost 0.000141144\n",
      "step 36420, training accuracy 0.969697, cost 9.71882, change in cost 0.000139236\n",
      "step 36430, training accuracy 0.969697, cost 9.71868, change in cost 0.000139236\n",
      "step 36440, training accuracy 0.969697, cost 9.71854, change in cost 0.000139236\n",
      "step 36450, training accuracy 0.969697, cost 9.7184, change in cost 0.00014019\n",
      "step 36460, training accuracy 0.969697, cost 9.71826, change in cost 0.000141144\n",
      "step 36470, training accuracy 0.969697, cost 9.71812, change in cost 0.000139236\n",
      "step 36480, training accuracy 0.969697, cost 9.71798, change in cost 0.000139236\n",
      "step 36490, training accuracy 0.969697, cost 9.71784, change in cost 0.000139236\n",
      "step 36500, training accuracy 0.969697, cost 9.7177, change in cost 0.00014019\n",
      "step 36510, training accuracy 0.969697, cost 9.71756, change in cost 0.000139236\n",
      "step 36520, training accuracy 0.969697, cost 9.71743, change in cost 0.000139236\n",
      "step 36530, training accuracy 0.969697, cost 9.71729, change in cost 0.000139236\n",
      "step 36540, training accuracy 0.969697, cost 9.71715, change in cost 0.000139236\n",
      "step 36550, training accuracy 0.969697, cost 9.71701, change in cost 0.000138283\n",
      "step 36560, training accuracy 0.969697, cost 9.71687, change in cost 0.00014019\n",
      "step 36570, training accuracy 0.969697, cost 9.71673, change in cost 0.000138283\n",
      "step 36580, training accuracy 0.969697, cost 9.71659, change in cost 0.000138283\n",
      "step 36590, training accuracy 0.969697, cost 9.71645, change in cost 0.000138283\n",
      "step 36600, training accuracy 0.969697, cost 9.71631, change in cost 0.000139236\n",
      "step 36610, training accuracy 0.969697, cost 9.71618, change in cost 0.000138283\n",
      "step 36620, training accuracy 0.969697, cost 9.71604, change in cost 0.000137329\n",
      "step 36630, training accuracy 0.969697, cost 9.7159, change in cost 0.00014019\n",
      "step 36640, training accuracy 0.969697, cost 9.71576, change in cost 0.000136375\n",
      "step 36650, training accuracy 0.969697, cost 9.71562, change in cost 0.000138283\n",
      "step 36660, training accuracy 0.969697, cost 9.71549, change in cost 0.000138283\n",
      "step 36670, training accuracy 0.969697, cost 9.71535, change in cost 0.00014019\n",
      "step 36680, training accuracy 0.969697, cost 9.71521, change in cost 0.000138283\n",
      "step 36690, training accuracy 0.969697, cost 9.71507, change in cost 0.000137329\n",
      "step 36700, training accuracy 0.969697, cost 9.71493, change in cost 0.000137329\n",
      "step 36710, training accuracy 0.969697, cost 9.71479, change in cost 0.000139236\n",
      "step 36720, training accuracy 0.969697, cost 9.71465, change in cost 0.000139236\n",
      "step 36730, training accuracy 0.969697, cost 9.71452, change in cost 0.000137329\n",
      "step 36740, training accuracy 0.969697, cost 9.71438, change in cost 0.000137329\n",
      "step 36750, training accuracy 0.969697, cost 9.71424, change in cost 0.000139236\n",
      "step 36760, training accuracy 0.969697, cost 9.7141, change in cost 0.000137329\n",
      "step 36770, training accuracy 0.969697, cost 9.71397, change in cost 0.000137329\n",
      "step 36780, training accuracy 0.969697, cost 9.71383, change in cost 0.000139236\n",
      "step 36790, training accuracy 0.969697, cost 9.71369, change in cost 0.000137329\n",
      "step 36800, training accuracy 0.969697, cost 9.71355, change in cost 0.000137329\n",
      "step 36810, training accuracy 0.969697, cost 9.71342, change in cost 0.000136375\n",
      "step 36820, training accuracy 0.969697, cost 9.71328, change in cost 0.000139236\n",
      "step 36830, training accuracy 0.969697, cost 9.71314, change in cost 0.000138283\n",
      "step 36840, training accuracy 0.969697, cost 9.713, change in cost 0.000136375\n",
      "step 36850, training accuracy 0.969697, cost 9.71286, change in cost 0.000136375\n",
      "step 36860, training accuracy 0.969697, cost 9.71273, change in cost 0.000137329\n",
      "step 36870, training accuracy 0.969697, cost 9.71259, change in cost 0.000136375\n",
      "step 36880, training accuracy 0.969697, cost 9.71245, change in cost 0.000138283\n",
      "step 36890, training accuracy 0.969697, cost 9.71232, change in cost 0.000135422\n",
      "step 36900, training accuracy 0.969697, cost 9.71218, change in cost 0.000137329\n",
      "step 36910, training accuracy 0.969697, cost 9.71204, change in cost 0.000136375\n",
      "step 36920, training accuracy 0.969697, cost 9.71191, change in cost 0.000135422\n",
      "step 36930, training accuracy 0.969697, cost 9.71177, change in cost 0.000137329\n",
      "step 36940, training accuracy 0.969697, cost 9.71163, change in cost 0.000136375\n",
      "step 36950, training accuracy 0.969697, cost 9.7115, change in cost 0.000137329\n",
      "step 36960, training accuracy 0.969697, cost 9.71136, change in cost 0.000135422\n",
      "step 36970, training accuracy 0.969697, cost 9.71123, change in cost 0.000136375\n",
      "step 36980, training accuracy 0.969697, cost 9.71109, change in cost 0.000135422\n",
      "step 36990, training accuracy 0.969697, cost 9.71095, change in cost 0.000135422\n",
      "step 37000, training accuracy 0.969697, cost 9.71082, change in cost 0.000138283\n",
      "step 37010, training accuracy 0.969697, cost 9.71068, change in cost 0.000134468\n",
      "step 37020, training accuracy 0.969697, cost 9.71054, change in cost 0.000137329\n",
      "step 37030, training accuracy 0.969697, cost 9.71041, change in cost 0.000135422\n",
      "step 37040, training accuracy 0.969697, cost 9.71027, change in cost 0.000135422\n",
      "step 37050, training accuracy 0.969697, cost 9.71014, change in cost 0.000137329\n",
      "step 37060, training accuracy 0.969697, cost 9.71, change in cost 0.000135422\n",
      "step 37070, training accuracy 0.969697, cost 9.70986, change in cost 0.000136375\n",
      "step 37080, training accuracy 0.969697, cost 9.70973, change in cost 0.000135422\n",
      "step 37090, training accuracy 0.969697, cost 9.70959, change in cost 0.000135422\n",
      "step 37100, training accuracy 0.969697, cost 9.70946, change in cost 0.000135422\n",
      "step 37110, training accuracy 0.969697, cost 9.70932, change in cost 0.000137329\n",
      "step 37120, training accuracy 0.969697, cost 9.70919, change in cost 0.000134468\n",
      "step 37130, training accuracy 0.969697, cost 9.70905, change in cost 0.000135422\n",
      "step 37140, training accuracy 0.969697, cost 9.70892, change in cost 0.000135422\n",
      "step 37150, training accuracy 0.969697, cost 9.70878, change in cost 0.000135422\n",
      "step 37160, training accuracy 0.969697, cost 9.70865, change in cost 0.000134468\n",
      "step 37170, training accuracy 0.969697, cost 9.70851, change in cost 0.000135422\n",
      "step 37180, training accuracy 0.969697, cost 9.70838, change in cost 0.000134468\n",
      "step 37190, training accuracy 0.969697, cost 9.70824, change in cost 0.000134468\n",
      "step 37200, training accuracy 0.969697, cost 9.70811, change in cost 0.000135422\n",
      "step 37210, training accuracy 0.969697, cost 9.70797, change in cost 0.000135422\n",
      "step 37220, training accuracy 0.969697, cost 9.70784, change in cost 0.000134468\n",
      "step 37230, training accuracy 0.969697, cost 9.7077, change in cost 0.000134468\n",
      "step 37240, training accuracy 0.969697, cost 9.70757, change in cost 0.000136375\n",
      "step 37250, training accuracy 0.969697, cost 9.70743, change in cost 0.000133514\n",
      "step 37260, training accuracy 0.969697, cost 9.7073, change in cost 0.000134468\n",
      "step 37270, training accuracy 0.969697, cost 9.70716, change in cost 0.000134468\n",
      "step 37280, training accuracy 0.969697, cost 9.70703, change in cost 0.000135422\n",
      "step 37290, training accuracy 0.969697, cost 9.70689, change in cost 0.000135422\n",
      "step 37300, training accuracy 0.969697, cost 9.70676, change in cost 0.000134468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 37310, training accuracy 0.969697, cost 9.70662, change in cost 0.000134468\n",
      "step 37320, training accuracy 0.969697, cost 9.70649, change in cost 0.000134468\n",
      "step 37330, training accuracy 0.969697, cost 9.70636, change in cost 0.000133514\n",
      "step 37340, training accuracy 0.969697, cost 9.70622, change in cost 0.000135422\n",
      "step 37350, training accuracy 0.969697, cost 9.70609, change in cost 0.000133514\n",
      "step 37360, training accuracy 0.969697, cost 9.70595, change in cost 0.000134468\n",
      "step 37370, training accuracy 0.969697, cost 9.70582, change in cost 0.000133514\n",
      "step 37380, training accuracy 0.969697, cost 9.70568, change in cost 0.000133514\n",
      "step 37390, training accuracy 0.969697, cost 9.70555, change in cost 0.000134468\n",
      "step 37400, training accuracy 0.969697, cost 9.70542, change in cost 0.000133514\n",
      "step 37410, training accuracy 0.969697, cost 9.70528, change in cost 0.000133514\n",
      "step 37420, training accuracy 0.969697, cost 9.70515, change in cost 0.000132561\n",
      "step 37430, training accuracy 0.969697, cost 9.70502, change in cost 0.000133514\n",
      "step 37440, training accuracy 0.969697, cost 9.70488, change in cost 0.000133514\n",
      "step 37450, training accuracy 0.969697, cost 9.70475, change in cost 0.000133514\n",
      "step 37460, training accuracy 0.969697, cost 9.70462, change in cost 0.000133514\n",
      "step 37470, training accuracy 0.969697, cost 9.70448, change in cost 0.000133514\n",
      "step 37480, training accuracy 0.969697, cost 9.70435, change in cost 0.000132561\n",
      "step 37490, training accuracy 0.969697, cost 9.70422, change in cost 0.000132561\n",
      "step 37500, training accuracy 0.969697, cost 9.70409, change in cost 0.000131607\n",
      "step 37510, training accuracy 0.969697, cost 9.70395, change in cost 0.000134468\n",
      "step 37520, training accuracy 0.969697, cost 9.70382, change in cost 0.000132561\n",
      "step 37530, training accuracy 0.969697, cost 9.70369, change in cost 0.000132561\n",
      "step 37540, training accuracy 0.969697, cost 9.70355, change in cost 0.000133514\n",
      "step 37550, training accuracy 0.969697, cost 9.70342, change in cost 0.000133514\n",
      "step 37560, training accuracy 0.969697, cost 9.70329, change in cost 0.000131607\n",
      "step 37570, training accuracy 0.969697, cost 9.70315, change in cost 0.000134468\n",
      "step 37580, training accuracy 0.969697, cost 9.70302, change in cost 0.000131607\n",
      "step 37590, training accuracy 0.969697, cost 9.70289, change in cost 0.000133514\n",
      "step 37600, training accuracy 0.969697, cost 9.70275, change in cost 0.000133514\n",
      "step 37610, training accuracy 0.969697, cost 9.70262, change in cost 0.000131607\n",
      "step 37620, training accuracy 0.969697, cost 9.70249, change in cost 0.000131607\n",
      "step 37630, training accuracy 0.969697, cost 9.70236, change in cost 0.000135422\n",
      "step 37640, training accuracy 0.969697, cost 9.70223, change in cost 0.0001297\n",
      "step 37650, training accuracy 0.969697, cost 9.70209, change in cost 0.000134468\n",
      "step 37660, training accuracy 0.969697, cost 9.70196, change in cost 0.000131607\n",
      "step 37670, training accuracy 0.969697, cost 9.70183, change in cost 0.000131607\n",
      "step 37680, training accuracy 0.969697, cost 9.7017, change in cost 0.000132561\n",
      "step 37690, training accuracy 0.969697, cost 9.70156, change in cost 0.000131607\n",
      "step 37700, training accuracy 0.969697, cost 9.70143, change in cost 0.000133514\n",
      "step 37710, training accuracy 0.969697, cost 9.7013, change in cost 0.000130653\n",
      "step 37720, training accuracy 0.969697, cost 9.70117, change in cost 0.000130653\n",
      "step 37730, training accuracy 0.969697, cost 9.70104, change in cost 0.000133514\n",
      "step 37740, training accuracy 0.969697, cost 9.7009, change in cost 0.000132561\n",
      "step 37750, training accuracy 0.969697, cost 9.70077, change in cost 0.0001297\n",
      "step 37760, training accuracy 0.969697, cost 9.70064, change in cost 0.000131607\n",
      "step 37770, training accuracy 0.969697, cost 9.70051, change in cost 0.000131607\n",
      "step 37780, training accuracy 0.969697, cost 9.70038, change in cost 0.000131607\n",
      "step 37790, training accuracy 0.969697, cost 9.70025, change in cost 0.000131607\n",
      "step 37800, training accuracy 0.969697, cost 9.70012, change in cost 0.000130653\n",
      "step 37810, training accuracy 0.969697, cost 9.69999, change in cost 0.000131607\n",
      "step 37820, training accuracy 0.969697, cost 9.69985, change in cost 0.000131607\n",
      "step 37830, training accuracy 0.969697, cost 9.69972, change in cost 0.000131607\n",
      "step 37840, training accuracy 0.969697, cost 9.69959, change in cost 0.000131607\n",
      "step 37850, training accuracy 0.969697, cost 9.69946, change in cost 0.0001297\n",
      "step 37860, training accuracy 0.969697, cost 9.69933, change in cost 0.000131607\n",
      "step 37870, training accuracy 0.969697, cost 9.6992, change in cost 0.000131607\n",
      "step 37880, training accuracy 0.969697, cost 9.69907, change in cost 0.000130653\n",
      "step 37890, training accuracy 0.969697, cost 9.69894, change in cost 0.0001297\n",
      "step 37900, training accuracy 0.969697, cost 9.69881, change in cost 0.000131607\n",
      "step 37910, training accuracy 0.969697, cost 9.69868, change in cost 0.0001297\n",
      "step 37920, training accuracy 0.969697, cost 9.69854, change in cost 0.000133514\n",
      "step 37930, training accuracy 0.969697, cost 9.69841, change in cost 0.000130653\n",
      "step 37940, training accuracy 0.969697, cost 9.69828, change in cost 0.0001297\n",
      "step 37950, training accuracy 0.969697, cost 9.69815, change in cost 0.000131607\n",
      "step 37960, training accuracy 0.969697, cost 9.69802, change in cost 0.000128746\n",
      "step 37970, training accuracy 0.969697, cost 9.69789, change in cost 0.000130653\n",
      "step 37980, training accuracy 0.969697, cost 9.69776, change in cost 0.000130653\n",
      "step 37990, training accuracy 0.969697, cost 9.69763, change in cost 0.000128746\n",
      "step 38000, training accuracy 0.969697, cost 9.6975, change in cost 0.000131607\n",
      "step 38010, training accuracy 0.969697, cost 9.69737, change in cost 0.000128746\n",
      "step 38020, training accuracy 0.969697, cost 9.69724, change in cost 0.0001297\n",
      "step 38030, training accuracy 0.969697, cost 9.69711, change in cost 0.0001297\n",
      "step 38040, training accuracy 0.969697, cost 9.69698, change in cost 0.000130653\n",
      "step 38050, training accuracy 0.969697, cost 9.69685, change in cost 0.000130653\n",
      "step 38060, training accuracy 0.969697, cost 9.69672, change in cost 0.000128746\n",
      "step 38070, training accuracy 0.969697, cost 9.69659, change in cost 0.0001297\n",
      "step 38080, training accuracy 0.969697, cost 9.69646, change in cost 0.0001297\n",
      "step 38090, training accuracy 0.969697, cost 9.69633, change in cost 0.0001297\n",
      "step 38100, training accuracy 0.969697, cost 9.6962, change in cost 0.000130653\n",
      "step 38110, training accuracy 0.969697, cost 9.69607, change in cost 0.0001297\n",
      "step 38120, training accuracy 0.969697, cost 9.69594, change in cost 0.0001297\n",
      "step 38130, training accuracy 0.969697, cost 9.69581, change in cost 0.0001297\n",
      "step 38140, training accuracy 0.969697, cost 9.69568, change in cost 0.0001297\n",
      "step 38150, training accuracy 0.969697, cost 9.69555, change in cost 0.000128746\n",
      "step 38160, training accuracy 0.969697, cost 9.69543, change in cost 0.0001297\n",
      "step 38170, training accuracy 0.969697, cost 9.6953, change in cost 0.0001297\n",
      "step 38180, training accuracy 0.969697, cost 9.69517, change in cost 0.0001297\n",
      "step 38190, training accuracy 0.969697, cost 9.69504, change in cost 0.000127792\n",
      "step 38200, training accuracy 0.969697, cost 9.69491, change in cost 0.000128746\n",
      "step 38210, training accuracy 0.969697, cost 9.69478, change in cost 0.000130653\n",
      "step 38220, training accuracy 0.969697, cost 9.69465, change in cost 0.000128746\n",
      "step 38230, training accuracy 0.969697, cost 9.69452, change in cost 0.0001297\n",
      "step 38240, training accuracy 0.969697, cost 9.69439, change in cost 0.000127792\n",
      "step 38250, training accuracy 0.969697, cost 9.69426, change in cost 0.0001297\n",
      "step 38260, training accuracy 0.969697, cost 9.69414, change in cost 0.000126839\n",
      "step 38270, training accuracy 0.969697, cost 9.69401, change in cost 0.000127792\n",
      "step 38280, training accuracy 0.969697, cost 9.69388, change in cost 0.0001297\n",
      "step 38290, training accuracy 0.969697, cost 9.69375, change in cost 0.000127792\n",
      "step 38300, training accuracy 0.969697, cost 9.69362, change in cost 0.0001297\n",
      "step 38310, training accuracy 0.969697, cost 9.69349, change in cost 0.000127792\n",
      "step 38320, training accuracy 0.969697, cost 9.69337, change in cost 0.000125885\n",
      "step 38330, training accuracy 0.969697, cost 9.69324, change in cost 0.0001297\n",
      "step 38340, training accuracy 0.969697, cost 9.69311, change in cost 0.000127792\n",
      "step 38350, training accuracy 0.969697, cost 9.69298, change in cost 0.000128746\n",
      "step 38360, training accuracy 0.969697, cost 9.69285, change in cost 0.000127792\n",
      "step 38370, training accuracy 0.969697, cost 9.69272, change in cost 0.000128746\n",
      "step 38380, training accuracy 0.969697, cost 9.6926, change in cost 0.000126839\n",
      "step 38390, training accuracy 0.969697, cost 9.69247, change in cost 0.000127792\n",
      "step 38400, training accuracy 0.969697, cost 9.69234, change in cost 0.000127792\n",
      "step 38410, training accuracy 0.969697, cost 9.69221, change in cost 0.000128746\n",
      "step 38420, training accuracy 0.969697, cost 9.69208, change in cost 0.000128746\n",
      "step 38430, training accuracy 0.969697, cost 9.69196, change in cost 0.000126839\n",
      "step 38440, training accuracy 0.969697, cost 9.69183, change in cost 0.000127792\n",
      "step 38450, training accuracy 0.969697, cost 9.6917, change in cost 0.000125885\n",
      "step 38460, training accuracy 0.969697, cost 9.69158, change in cost 0.000127792\n",
      "step 38470, training accuracy 0.969697, cost 9.69145, change in cost 0.000128746\n",
      "step 38480, training accuracy 0.969697, cost 9.69132, change in cost 0.000127792\n",
      "step 38490, training accuracy 0.969697, cost 9.69119, change in cost 0.000127792\n",
      "step 38500, training accuracy 0.969697, cost 9.69106, change in cost 0.000126839\n",
      "step 38510, training accuracy 0.969697, cost 9.69094, change in cost 0.000127792\n",
      "step 38520, training accuracy 0.969697, cost 9.69081, change in cost 0.000126839\n",
      "step 38530, training accuracy 0.969697, cost 9.69068, change in cost 0.000128746\n",
      "step 38540, training accuracy 0.969697, cost 9.69056, change in cost 0.000124931\n",
      "step 38550, training accuracy 0.969697, cost 9.69043, change in cost 0.000128746\n",
      "step 38560, training accuracy 0.969697, cost 9.6903, change in cost 0.000126839\n",
      "step 38570, training accuracy 0.969697, cost 9.69018, change in cost 0.000125885\n",
      "step 38580, training accuracy 0.969697, cost 9.69005, change in cost 0.000126839\n",
      "step 38590, training accuracy 0.969697, cost 9.68992, change in cost 0.000126839\n",
      "step 38600, training accuracy 0.969697, cost 9.6898, change in cost 0.000125885\n",
      "step 38610, training accuracy 0.969697, cost 9.68967, change in cost 0.000127792\n",
      "step 38620, training accuracy 0.969697, cost 9.68954, change in cost 0.000124931\n",
      "step 38630, training accuracy 0.969697, cost 9.68941, change in cost 0.000127792\n",
      "step 38640, training accuracy 0.969697, cost 9.68929, change in cost 0.000126839\n",
      "step 38650, training accuracy 0.969697, cost 9.68916, change in cost 0.000124931\n",
      "step 38660, training accuracy 0.969697, cost 9.68904, change in cost 0.000127792\n",
      "step 38670, training accuracy 0.969697, cost 9.68891, change in cost 0.000126839\n",
      "step 38680, training accuracy 0.969697, cost 9.68878, change in cost 0.000124931\n",
      "step 38690, training accuracy 0.969697, cost 9.68866, change in cost 0.000127792\n",
      "step 38700, training accuracy 0.969697, cost 9.68853, change in cost 0.000123978\n",
      "step 38710, training accuracy 0.969697, cost 9.6884, change in cost 0.000127792\n",
      "step 38720, training accuracy 0.969697, cost 9.68828, change in cost 0.000126839\n",
      "step 38730, training accuracy 0.969697, cost 9.68815, change in cost 0.000125885\n",
      "step 38740, training accuracy 0.969697, cost 9.68803, change in cost 0.000125885\n",
      "step 38750, training accuracy 0.969697, cost 9.6879, change in cost 0.000125885\n",
      "step 38760, training accuracy 0.969697, cost 9.68777, change in cost 0.000124931\n",
      "step 38770, training accuracy 0.969697, cost 9.68765, change in cost 0.000126839\n",
      "step 38780, training accuracy 0.969697, cost 9.68752, change in cost 0.000125885\n",
      "step 38790, training accuracy 0.969697, cost 9.68739, change in cost 0.000127792\n",
      "step 38800, training accuracy 0.969697, cost 9.68727, change in cost 0.000124931\n",
      "step 38810, training accuracy 0.969697, cost 9.68714, change in cost 0.000125885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 38820, training accuracy 0.969697, cost 9.68702, change in cost 0.000123978\n",
      "step 38830, training accuracy 0.969697, cost 9.68689, change in cost 0.000127792\n",
      "step 38840, training accuracy 0.969697, cost 9.68677, change in cost 0.000123978\n",
      "step 38850, training accuracy 0.969697, cost 9.68664, change in cost 0.000126839\n",
      "step 38860, training accuracy 0.969697, cost 9.68652, change in cost 0.000123978\n",
      "step 38870, training accuracy 0.969697, cost 9.68639, change in cost 0.000125885\n",
      "step 38880, training accuracy 0.969697, cost 9.68627, change in cost 0.000123978\n",
      "step 38890, training accuracy 0.969697, cost 9.68614, change in cost 0.000125885\n",
      "step 38900, training accuracy 0.969697, cost 9.68602, change in cost 0.000123978\n",
      "step 38910, training accuracy 0.969697, cost 9.68589, change in cost 0.000125885\n",
      "step 38920, training accuracy 0.969697, cost 9.68577, change in cost 0.000124931\n",
      "step 38930, training accuracy 0.969697, cost 9.68564, change in cost 0.000124931\n",
      "step 38940, training accuracy 0.969697, cost 9.68552, change in cost 0.000124931\n",
      "step 38950, training accuracy 0.969697, cost 9.68539, change in cost 0.000124931\n",
      "step 38960, training accuracy 0.969697, cost 9.68527, change in cost 0.000123024\n",
      "step 38970, training accuracy 0.969697, cost 9.68514, change in cost 0.000125885\n",
      "step 38980, training accuracy 0.969697, cost 9.68502, change in cost 0.000124931\n",
      "step 38990, training accuracy 0.969697, cost 9.68489, change in cost 0.000123978\n",
      "step 39000, training accuracy 0.969697, cost 9.68477, change in cost 0.000123978\n",
      "step 39010, training accuracy 0.969697, cost 9.68464, change in cost 0.000125885\n",
      "step 39020, training accuracy 0.969697, cost 9.68452, change in cost 0.00012207\n",
      "step 39030, training accuracy 0.969697, cost 9.68439, change in cost 0.000126839\n",
      "step 39040, training accuracy 0.969697, cost 9.68427, change in cost 0.000124931\n",
      "step 39050, training accuracy 0.969697, cost 9.68415, change in cost 0.000123024\n",
      "step 39060, training accuracy 0.969697, cost 9.68402, change in cost 0.000125885\n",
      "step 39070, training accuracy 0.969697, cost 9.6839, change in cost 0.000123978\n",
      "step 39080, training accuracy 0.969697, cost 9.68377, change in cost 0.000123978\n",
      "step 39090, training accuracy 0.969697, cost 9.68365, change in cost 0.000123978\n",
      "step 39100, training accuracy 0.969697, cost 9.68353, change in cost 0.000123978\n",
      "step 39110, training accuracy 0.969697, cost 9.6834, change in cost 0.000123978\n",
      "step 39120, training accuracy 0.969697, cost 9.68328, change in cost 0.000123978\n",
      "step 39130, training accuracy 0.969697, cost 9.68315, change in cost 0.000123978\n",
      "step 39140, training accuracy 0.969697, cost 9.68303, change in cost 0.000123024\n",
      "step 39150, training accuracy 0.969697, cost 9.68291, change in cost 0.000123978\n",
      "step 39160, training accuracy 0.969697, cost 9.68278, change in cost 0.000123024\n",
      "step 39170, training accuracy 0.969697, cost 9.68266, change in cost 0.000123978\n",
      "step 39180, training accuracy 0.969697, cost 9.68254, change in cost 0.000123024\n",
      "step 39190, training accuracy 0.969697, cost 9.68241, change in cost 0.000123024\n",
      "step 39200, training accuracy 0.969697, cost 9.68229, change in cost 0.000123978\n",
      "step 39210, training accuracy 0.969697, cost 9.68217, change in cost 0.000123978\n",
      "step 39220, training accuracy 0.969697, cost 9.68204, change in cost 0.000120163\n",
      "step 39230, training accuracy 0.969697, cost 9.68192, change in cost 0.000125885\n",
      "step 39240, training accuracy 0.969697, cost 9.6818, change in cost 0.000123024\n",
      "step 39250, training accuracy 0.969697, cost 9.68167, change in cost 0.000123024\n",
      "step 39260, training accuracy 0.969697, cost 9.68155, change in cost 0.00012207\n",
      "step 39270, training accuracy 0.969697, cost 9.68143, change in cost 0.000124931\n",
      "step 39280, training accuracy 0.969697, cost 9.6813, change in cost 0.000121117\n",
      "step 39290, training accuracy 0.969697, cost 9.68118, change in cost 0.000124931\n",
      "step 39300, training accuracy 0.969697, cost 9.68106, change in cost 0.000123024\n",
      "step 39310, training accuracy 0.969697, cost 9.68093, change in cost 0.00012207\n",
      "step 39320, training accuracy 0.969697, cost 9.68081, change in cost 0.000123024\n",
      "step 39330, training accuracy 0.969697, cost 9.68069, change in cost 0.00012207\n",
      "step 39340, training accuracy 0.969697, cost 9.68057, change in cost 0.000123978\n",
      "step 39350, training accuracy 0.969697, cost 9.68044, change in cost 0.000123024\n",
      "step 39360, training accuracy 0.969697, cost 9.68032, change in cost 0.00012207\n",
      "step 39370, training accuracy 0.969697, cost 9.6802, change in cost 0.000123024\n",
      "step 39380, training accuracy 0.969697, cost 9.68008, change in cost 0.00012207\n",
      "step 39390, training accuracy 0.969697, cost 9.67995, change in cost 0.000124931\n",
      "step 39400, training accuracy 0.969697, cost 9.67983, change in cost 0.00012207\n",
      "step 39410, training accuracy 0.969697, cost 9.67971, change in cost 0.000121117\n",
      "step 39420, training accuracy 0.969697, cost 9.67959, change in cost 0.00012207\n",
      "step 39430, training accuracy 0.969697, cost 9.67946, change in cost 0.000123978\n",
      "step 39440, training accuracy 0.969697, cost 9.67934, change in cost 0.00012207\n",
      "step 39450, training accuracy 0.969697, cost 9.67922, change in cost 0.000121117\n",
      "step 39460, training accuracy 0.969697, cost 9.6791, change in cost 0.000120163\n",
      "step 39470, training accuracy 0.969697, cost 9.67897, change in cost 0.000123978\n",
      "step 39480, training accuracy 0.969697, cost 9.67885, change in cost 0.000120163\n",
      "step 39490, training accuracy 0.969697, cost 9.67873, change in cost 0.000121117\n",
      "step 39500, training accuracy 0.969697, cost 9.67861, change in cost 0.000123978\n",
      "step 39510, training accuracy 0.969697, cost 9.67849, change in cost 0.000120163\n",
      "step 39520, training accuracy 0.969697, cost 9.67837, change in cost 0.000123024\n",
      "step 39530, training accuracy 0.969697, cost 9.67825, change in cost 0.000120163\n",
      "step 39540, training accuracy 0.969697, cost 9.67812, change in cost 0.00012207\n",
      "step 39550, training accuracy 0.969697, cost 9.678, change in cost 0.00012207\n",
      "step 39560, training accuracy 0.969697, cost 9.67788, change in cost 0.000121117\n",
      "step 39570, training accuracy 0.969697, cost 9.67776, change in cost 0.00012207\n",
      "step 39580, training accuracy 0.969697, cost 9.67764, change in cost 0.000121117\n",
      "step 39590, training accuracy 0.969697, cost 9.67752, change in cost 0.000121117\n",
      "step 39600, training accuracy 0.969697, cost 9.67739, change in cost 0.000121117\n",
      "step 39610, training accuracy 0.969697, cost 9.67727, change in cost 0.000120163\n",
      "step 39620, training accuracy 0.969697, cost 9.67715, change in cost 0.000121117\n",
      "step 39630, training accuracy 0.969697, cost 9.67703, change in cost 0.000123024\n",
      "step 39640, training accuracy 0.969697, cost 9.67691, change in cost 0.000121117\n",
      "step 39650, training accuracy 0.969697, cost 9.67679, change in cost 0.000120163\n",
      "step 39660, training accuracy 0.969697, cost 9.67667, change in cost 0.00012207\n",
      "step 39670, training accuracy 0.969697, cost 9.67655, change in cost 0.000121117\n",
      "step 39680, training accuracy 0.969697, cost 9.67642, change in cost 0.000121117\n",
      "step 39690, training accuracy 0.969697, cost 9.6763, change in cost 0.000120163\n",
      "step 39700, training accuracy 0.969697, cost 9.67618, change in cost 0.000121117\n",
      "step 39710, training accuracy 0.969697, cost 9.67606, change in cost 0.00012207\n",
      "step 39720, training accuracy 0.969697, cost 9.67594, change in cost 0.000118256\n",
      "step 39730, training accuracy 0.969697, cost 9.67582, change in cost 0.00012207\n",
      "step 39740, training accuracy 0.969697, cost 9.6757, change in cost 0.000120163\n",
      "step 39750, training accuracy 0.969697, cost 9.67558, change in cost 0.000119209\n",
      "step 39760, training accuracy 0.969697, cost 9.67546, change in cost 0.00012207\n",
      "step 39770, training accuracy 0.969697, cost 9.67534, change in cost 0.00012207\n",
      "step 39780, training accuracy 0.969697, cost 9.67522, change in cost 0.000118256\n",
      "step 39790, training accuracy 0.969697, cost 9.6751, change in cost 0.000120163\n",
      "step 39800, training accuracy 0.969697, cost 9.67498, change in cost 0.000120163\n",
      "step 39810, training accuracy 0.969697, cost 9.67486, change in cost 0.00012207\n",
      "step 39820, training accuracy 0.969697, cost 9.67474, change in cost 0.000118256\n",
      "step 39830, training accuracy 0.969697, cost 9.67462, change in cost 0.000121117\n",
      "step 39840, training accuracy 0.969697, cost 9.6745, change in cost 0.000120163\n",
      "step 39850, training accuracy 0.969697, cost 9.67438, change in cost 0.000120163\n",
      "step 39860, training accuracy 0.969697, cost 9.67426, change in cost 0.000120163\n",
      "step 39870, training accuracy 0.969697, cost 9.67414, change in cost 0.000119209\n",
      "step 39880, training accuracy 0.969697, cost 9.67402, change in cost 0.000121117\n",
      "step 39890, training accuracy 0.969697, cost 9.6739, change in cost 0.000118256\n",
      "step 39900, training accuracy 0.969697, cost 9.67378, change in cost 0.00012207\n",
      "step 39910, training accuracy 0.969697, cost 9.67366, change in cost 0.000120163\n",
      "step 39920, training accuracy 0.969697, cost 9.67354, change in cost 0.000119209\n",
      "step 39930, training accuracy 0.969697, cost 9.67342, change in cost 0.000118256\n",
      "step 39940, training accuracy 0.969697, cost 9.6733, change in cost 0.000120163\n",
      "step 39950, training accuracy 0.969697, cost 9.67318, change in cost 0.000121117\n",
      "step 39960, training accuracy 0.969697, cost 9.67306, change in cost 0.000119209\n",
      "step 39970, training accuracy 0.969697, cost 9.67294, change in cost 0.000119209\n",
      "step 39980, training accuracy 0.969697, cost 9.67282, change in cost 0.000120163\n",
      "step 39990, training accuracy 0.969697, cost 9.6727, change in cost 0.000120163\n",
      "step 40000, training accuracy 0.969697, cost 9.67258, change in cost 0.000118256\n",
      "step 40010, training accuracy 0.969697, cost 9.67246, change in cost 0.000120163\n",
      "step 40020, training accuracy 0.969697, cost 9.67234, change in cost 0.000118256\n",
      "step 40030, training accuracy 0.969697, cost 9.67222, change in cost 0.000118256\n",
      "step 40040, training accuracy 0.969697, cost 9.6721, change in cost 0.000119209\n",
      "step 40050, training accuracy 0.969697, cost 9.67198, change in cost 0.000120163\n",
      "step 40060, training accuracy 0.969697, cost 9.67187, change in cost 0.000117302\n",
      "step 40070, training accuracy 0.969697, cost 9.67175, change in cost 0.000117302\n",
      "step 40080, training accuracy 0.969697, cost 9.67163, change in cost 0.000120163\n",
      "step 40090, training accuracy 0.969697, cost 9.67151, change in cost 0.000118256\n",
      "step 40100, training accuracy 0.969697, cost 9.67139, change in cost 0.000117302\n",
      "step 40110, training accuracy 0.969697, cost 9.67128, change in cost 0.000119209\n",
      "step 40120, training accuracy 0.969697, cost 9.67116, change in cost 0.000119209\n",
      "step 40130, training accuracy 0.969697, cost 9.67104, change in cost 0.000118256\n",
      "step 40140, training accuracy 0.969697, cost 9.67092, change in cost 0.000116348\n",
      "step 40150, training accuracy 0.969697, cost 9.6708, change in cost 0.000119209\n",
      "step 40160, training accuracy 0.969697, cost 9.67068, change in cost 0.000119209\n",
      "step 40170, training accuracy 0.969697, cost 9.67056, change in cost 0.000118256\n",
      "step 40180, training accuracy 0.969697, cost 9.67045, change in cost 0.000118256\n",
      "step 40190, training accuracy 0.969697, cost 9.67033, change in cost 0.000117302\n",
      "step 40200, training accuracy 0.969697, cost 9.67021, change in cost 0.000118256\n",
      "step 40210, training accuracy 0.969697, cost 9.67009, change in cost 0.000118256\n",
      "step 40220, training accuracy 0.969697, cost 9.66997, change in cost 0.000118256\n",
      "step 40230, training accuracy 0.969697, cost 9.66986, change in cost 0.000118256\n",
      "step 40240, training accuracy 0.969697, cost 9.66974, change in cost 0.000118256\n",
      "step 40250, training accuracy 0.969697, cost 9.66962, change in cost 0.000117302\n",
      "step 40260, training accuracy 0.969697, cost 9.6695, change in cost 0.000120163\n",
      "step 40270, training accuracy 0.969697, cost 9.66938, change in cost 0.000116348\n",
      "step 40280, training accuracy 0.969697, cost 9.66927, change in cost 0.000118256\n",
      "step 40290, training accuracy 0.969697, cost 9.66915, change in cost 0.000118256\n",
      "step 40300, training accuracy 0.969697, cost 9.66903, change in cost 0.000118256\n",
      "step 40310, training accuracy 0.969697, cost 9.66891, change in cost 0.000117302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40320, training accuracy 0.969697, cost 9.66879, change in cost 0.000117302\n",
      "step 40330, training accuracy 0.969697, cost 9.66868, change in cost 0.000117302\n",
      "step 40340, training accuracy 0.969697, cost 9.66856, change in cost 0.000117302\n",
      "step 40350, training accuracy 0.969697, cost 9.66844, change in cost 0.000119209\n",
      "step 40360, training accuracy 0.969697, cost 9.66833, change in cost 0.000115395\n",
      "step 40370, training accuracy 0.969697, cost 9.66821, change in cost 0.000117302\n",
      "step 40380, training accuracy 0.969697, cost 9.66809, change in cost 0.000116348\n",
      "step 40390, training accuracy 0.969697, cost 9.66797, change in cost 0.000118256\n",
      "step 40400, training accuracy 0.969697, cost 9.66785, change in cost 0.000119209\n",
      "step 40410, training accuracy 0.969697, cost 9.66774, change in cost 0.000114441\n",
      "step 40420, training accuracy 0.969697, cost 9.66762, change in cost 0.000118256\n",
      "step 40430, training accuracy 0.969697, cost 9.6675, change in cost 0.000117302\n",
      "step 40440, training accuracy 0.969697, cost 9.66739, change in cost 0.000115395\n",
      "step 40450, training accuracy 0.969697, cost 9.66727, change in cost 0.000118256\n",
      "step 40460, training accuracy 0.969697, cost 9.66715, change in cost 0.000117302\n",
      "step 40470, training accuracy 0.969697, cost 9.66704, change in cost 0.000117302\n",
      "step 40480, training accuracy 0.969697, cost 9.66692, change in cost 0.000115395\n",
      "step 40490, training accuracy 0.969697, cost 9.6668, change in cost 0.000116348\n",
      "step 40500, training accuracy 0.969697, cost 9.66669, change in cost 0.000118256\n",
      "step 40510, training accuracy 0.969697, cost 9.66657, change in cost 0.000115395\n",
      "step 40520, training accuracy 0.969697, cost 9.66645, change in cost 0.000117302\n",
      "step 40530, training accuracy 0.969697, cost 9.66634, change in cost 0.000116348\n",
      "step 40540, training accuracy 0.969697, cost 9.66622, change in cost 0.000117302\n",
      "step 40550, training accuracy 0.969697, cost 9.6661, change in cost 0.000116348\n",
      "step 40560, training accuracy 0.969697, cost 9.66599, change in cost 0.000116348\n",
      "step 40570, training accuracy 0.969697, cost 9.66587, change in cost 0.000116348\n",
      "step 40580, training accuracy 0.969697, cost 9.66575, change in cost 0.000116348\n",
      "step 40590, training accuracy 0.969697, cost 9.66564, change in cost 0.000117302\n",
      "step 40600, training accuracy 0.969697, cost 9.66552, change in cost 0.000115395\n",
      "step 40610, training accuracy 0.969697, cost 9.6654, change in cost 0.000117302\n",
      "step 40620, training accuracy 0.969697, cost 9.66529, change in cost 0.000116348\n",
      "step 40630, training accuracy 0.969697, cost 9.66517, change in cost 0.000114441\n",
      "step 40640, training accuracy 0.969697, cost 9.66506, change in cost 0.000116348\n",
      "step 40650, training accuracy 0.969697, cost 9.66494, change in cost 0.000114441\n",
      "step 40660, training accuracy 0.969697, cost 9.66483, change in cost 0.000116348\n",
      "step 40670, training accuracy 0.969697, cost 9.66471, change in cost 0.000114441\n",
      "step 40680, training accuracy 0.969697, cost 9.6646, change in cost 0.000116348\n",
      "step 40690, training accuracy 0.969697, cost 9.66448, change in cost 0.000117302\n",
      "step 40700, training accuracy 0.969697, cost 9.66436, change in cost 0.000113487\n",
      "step 40710, training accuracy 0.969697, cost 9.66425, change in cost 0.000117302\n",
      "step 40720, training accuracy 0.969697, cost 9.66413, change in cost 0.000114441\n",
      "step 40730, training accuracy 0.969697, cost 9.66402, change in cost 0.000115395\n",
      "step 40740, training accuracy 0.969697, cost 9.6639, change in cost 0.000115395\n",
      "step 40750, training accuracy 0.969697, cost 9.66379, change in cost 0.000115395\n",
      "step 40760, training accuracy 0.969697, cost 9.66367, change in cost 0.000115395\n",
      "step 40770, training accuracy 0.969697, cost 9.66356, change in cost 0.000115395\n",
      "step 40780, training accuracy 0.969697, cost 9.66344, change in cost 0.000114441\n",
      "step 40790, training accuracy 0.969697, cost 9.66333, change in cost 0.000116348\n",
      "step 40800, training accuracy 0.969697, cost 9.66321, change in cost 0.000114441\n",
      "step 40810, training accuracy 0.969697, cost 9.6631, change in cost 0.000114441\n",
      "step 40820, training accuracy 0.969697, cost 9.66298, change in cost 0.000115395\n",
      "step 40830, training accuracy 0.969697, cost 9.66286, change in cost 0.000117302\n",
      "step 40840, training accuracy 0.969697, cost 9.66275, change in cost 0.000113487\n",
      "step 40850, training accuracy 0.969697, cost 9.66263, change in cost 0.000116348\n",
      "step 40860, training accuracy 0.969697, cost 9.66252, change in cost 0.000113487\n",
      "step 40870, training accuracy 0.969697, cost 9.66241, change in cost 0.000115395\n",
      "step 40880, training accuracy 0.969697, cost 9.66229, change in cost 0.000115395\n",
      "step 40890, training accuracy 0.969697, cost 9.66218, change in cost 0.000113487\n",
      "step 40900, training accuracy 0.969697, cost 9.66206, change in cost 0.000114441\n",
      "step 40910, training accuracy 0.969697, cost 9.66194, change in cost 0.000117302\n",
      "step 40920, training accuracy 0.969697, cost 9.66183, change in cost 0.000114441\n",
      "step 40930, training accuracy 0.969697, cost 9.66172, change in cost 0.000114441\n",
      "step 40940, training accuracy 0.969697, cost 9.6616, change in cost 0.000114441\n",
      "step 40950, training accuracy 0.969697, cost 9.66149, change in cost 0.000114441\n",
      "step 40960, training accuracy 0.969697, cost 9.66137, change in cost 0.000114441\n",
      "step 40970, training accuracy 0.969697, cost 9.66126, change in cost 0.000114441\n",
      "step 40980, training accuracy 0.969697, cost 9.66114, change in cost 0.000114441\n",
      "step 40990, training accuracy 0.969697, cost 9.66103, change in cost 0.000114441\n",
      "step 41000, training accuracy 0.969697, cost 9.66091, change in cost 0.000114441\n",
      "step 41010, training accuracy 0.969697, cost 9.6608, change in cost 0.000113487\n",
      "step 41020, training accuracy 0.969697, cost 9.66069, change in cost 0.000113487\n",
      "step 41030, training accuracy 0.969697, cost 9.66057, change in cost 0.000114441\n",
      "step 41040, training accuracy 0.969697, cost 9.66046, change in cost 0.000114441\n",
      "step 41050, training accuracy 0.969697, cost 9.66034, change in cost 0.000114441\n",
      "step 41060, training accuracy 0.969697, cost 9.66023, change in cost 0.000114441\n",
      "step 41070, training accuracy 0.969697, cost 9.66012, change in cost 0.000113487\n",
      "step 41080, training accuracy 0.969697, cost 9.66, change in cost 0.000114441\n",
      "step 41090, training accuracy 0.969697, cost 9.65989, change in cost 0.000112534\n",
      "step 41100, training accuracy 0.969697, cost 9.65978, change in cost 0.000113487\n",
      "step 41110, training accuracy 0.969697, cost 9.65966, change in cost 0.000112534\n",
      "step 41120, training accuracy 0.969697, cost 9.65955, change in cost 0.000115395\n",
      "step 41130, training accuracy 0.969697, cost 9.65943, change in cost 0.000114441\n",
      "step 41140, training accuracy 0.969697, cost 9.65932, change in cost 0.000112534\n",
      "step 41150, training accuracy 0.969697, cost 9.65921, change in cost 0.000113487\n",
      "step 41160, training accuracy 0.969697, cost 9.65909, change in cost 0.000113487\n",
      "step 41170, training accuracy 0.969697, cost 9.65898, change in cost 0.000113487\n",
      "step 41180, training accuracy 0.969697, cost 9.65887, change in cost 0.000113487\n",
      "step 41190, training accuracy 0.969697, cost 9.65875, change in cost 0.000113487\n",
      "step 41200, training accuracy 0.969697, cost 9.65864, change in cost 0.000113487\n",
      "step 41210, training accuracy 0.969697, cost 9.65853, change in cost 0.000112534\n",
      "step 41220, training accuracy 0.969697, cost 9.65841, change in cost 0.000113487\n",
      "step 41230, training accuracy 0.969697, cost 9.6583, change in cost 0.000112534\n",
      "step 41240, training accuracy 0.969697, cost 9.65819, change in cost 0.000113487\n",
      "step 41250, training accuracy 0.969697, cost 9.65807, change in cost 0.000113487\n",
      "step 41260, training accuracy 0.969697, cost 9.65796, change in cost 0.000112534\n",
      "step 41270, training accuracy 0.969697, cost 9.65785, change in cost 0.000112534\n",
      "step 41280, training accuracy 0.969697, cost 9.65774, change in cost 0.00011158\n",
      "step 41290, training accuracy 0.969697, cost 9.65762, change in cost 0.000113487\n",
      "step 41300, training accuracy 0.969697, cost 9.65751, change in cost 0.000112534\n",
      "step 41310, training accuracy 0.969697, cost 9.6574, change in cost 0.000113487\n",
      "step 41320, training accuracy 0.969697, cost 9.65729, change in cost 0.000110626\n",
      "step 41330, training accuracy 0.969697, cost 9.65717, change in cost 0.000114441\n",
      "step 41340, training accuracy 0.969697, cost 9.65706, change in cost 0.000110626\n",
      "step 41350, training accuracy 0.969697, cost 9.65695, change in cost 0.000112534\n",
      "step 41360, training accuracy 0.969697, cost 9.65684, change in cost 0.000112534\n",
      "step 41370, training accuracy 0.969697, cost 9.65672, change in cost 0.000113487\n",
      "step 41380, training accuracy 0.969697, cost 9.65661, change in cost 0.00011158\n",
      "step 41390, training accuracy 0.969697, cost 9.6565, change in cost 0.00011158\n",
      "step 41400, training accuracy 0.969697, cost 9.65639, change in cost 0.000112534\n",
      "step 41410, training accuracy 0.969697, cost 9.65628, change in cost 0.000112534\n",
      "step 41420, training accuracy 0.969697, cost 9.65616, change in cost 0.00011158\n",
      "step 41430, training accuracy 0.969697, cost 9.65605, change in cost 0.000112534\n",
      "step 41440, training accuracy 0.969697, cost 9.65594, change in cost 0.000112534\n",
      "step 41450, training accuracy 0.969697, cost 9.65583, change in cost 0.000112534\n",
      "step 41460, training accuracy 0.969697, cost 9.65571, change in cost 0.00011158\n",
      "step 41470, training accuracy 0.969697, cost 9.6556, change in cost 0.00011158\n",
      "step 41480, training accuracy 0.969697, cost 9.65549, change in cost 0.000112534\n",
      "step 41490, training accuracy 0.969697, cost 9.65538, change in cost 0.000112534\n",
      "step 41500, training accuracy 0.969697, cost 9.65527, change in cost 0.000110626\n",
      "step 41510, training accuracy 0.969697, cost 9.65516, change in cost 0.000112534\n",
      "step 41520, training accuracy 0.969697, cost 9.65504, change in cost 0.00011158\n",
      "step 41530, training accuracy 0.969697, cost 9.65493, change in cost 0.00011158\n",
      "step 41540, training accuracy 0.969697, cost 9.65482, change in cost 0.00011158\n",
      "step 41550, training accuracy 0.969697, cost 9.65471, change in cost 0.00011158\n",
      "step 41560, training accuracy 0.969697, cost 9.6546, change in cost 0.00011158\n",
      "step 41570, training accuracy 0.969697, cost 9.65449, change in cost 0.000110626\n",
      "step 41580, training accuracy 0.969697, cost 9.65438, change in cost 0.00011158\n",
      "step 41590, training accuracy 0.969697, cost 9.65426, change in cost 0.000112534\n",
      "step 41600, training accuracy 0.969697, cost 9.65415, change in cost 0.000110626\n",
      "step 41610, training accuracy 0.969697, cost 9.65404, change in cost 0.000112534\n",
      "step 41620, training accuracy 0.969697, cost 9.65393, change in cost 0.000110626\n",
      "step 41630, training accuracy 0.969697, cost 9.65382, change in cost 0.000109673\n",
      "step 41640, training accuracy 0.969697, cost 9.65371, change in cost 0.00011158\n",
      "step 41650, training accuracy 0.969697, cost 9.65359, change in cost 0.000112534\n",
      "step 41660, training accuracy 0.969697, cost 9.65348, change in cost 0.00011158\n",
      "step 41670, training accuracy 0.969697, cost 9.65337, change in cost 0.000109673\n",
      "step 41680, training accuracy 0.969697, cost 9.65326, change in cost 0.000110626\n",
      "step 41690, training accuracy 0.969697, cost 9.65315, change in cost 0.00011158\n",
      "step 41700, training accuracy 0.969697, cost 9.65304, change in cost 0.000109673\n",
      "step 41710, training accuracy 0.969697, cost 9.65293, change in cost 0.00011158\n",
      "step 41720, training accuracy 0.969697, cost 9.65282, change in cost 0.000110626\n",
      "step 41730, training accuracy 0.969697, cost 9.65271, change in cost 0.000110626\n",
      "step 41740, training accuracy 0.969697, cost 9.6526, change in cost 0.00011158\n",
      "step 41750, training accuracy 0.969697, cost 9.65249, change in cost 0.000110626\n",
      "step 41760, training accuracy 0.969697, cost 9.65238, change in cost 0.000108719\n",
      "step 41770, training accuracy 0.969697, cost 9.65227, change in cost 0.000112534\n",
      "step 41780, training accuracy 0.969697, cost 9.65216, change in cost 0.000108719\n",
      "step 41790, training accuracy 0.969697, cost 9.65205, change in cost 0.00011158\n",
      "step 41800, training accuracy 0.969697, cost 9.65194, change in cost 0.000109673\n",
      "step 41810, training accuracy 0.969697, cost 9.65182, change in cost 0.000110626\n",
      "step 41820, training accuracy 0.969697, cost 9.65171, change in cost 0.00011158\n",
      "step 41830, training accuracy 0.969697, cost 9.6516, change in cost 0.000110626\n",
      "step 41840, training accuracy 0.969697, cost 9.65149, change in cost 0.000108719\n",
      "step 41850, training accuracy 0.969697, cost 9.65138, change in cost 0.00011158\n",
      "step 41860, training accuracy 0.969697, cost 9.65127, change in cost 0.000109673\n",
      "step 41870, training accuracy 0.969697, cost 9.65116, change in cost 0.000110626\n",
      "step 41880, training accuracy 0.969697, cost 9.65105, change in cost 0.000108719\n",
      "step 41890, training accuracy 0.969697, cost 9.65094, change in cost 0.000109673\n",
      "step 41900, training accuracy 0.969697, cost 9.65083, change in cost 0.000110626\n",
      "step 41910, training accuracy 0.969697, cost 9.65072, change in cost 0.000109673\n",
      "step 41920, training accuracy 0.969697, cost 9.65061, change in cost 0.000110626\n",
      "step 41930, training accuracy 0.969697, cost 9.6505, change in cost 0.000109673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 41940, training accuracy 0.969697, cost 9.65039, change in cost 0.000108719\n",
      "step 41950, training accuracy 0.969697, cost 9.65029, change in cost 0.000108719\n",
      "step 41960, training accuracy 0.969697, cost 9.65018, change in cost 0.000110626\n",
      "step 41970, training accuracy 0.969697, cost 9.65006, change in cost 0.000110626\n",
      "step 41980, training accuracy 0.969697, cost 9.64996, change in cost 0.000108719\n",
      "step 41990, training accuracy 0.969697, cost 9.64985, change in cost 0.000109673\n",
      "step 42000, training accuracy 0.969697, cost 9.64974, change in cost 0.000109673\n",
      "step 42010, training accuracy 0.969697, cost 9.64963, change in cost 0.000109673\n",
      "step 42020, training accuracy 0.969697, cost 9.64952, change in cost 0.000108719\n",
      "step 42030, training accuracy 0.969697, cost 9.64941, change in cost 0.000108719\n",
      "step 42040, training accuracy 0.969697, cost 9.6493, change in cost 0.000109673\n",
      "step 42050, training accuracy 0.969697, cost 9.64919, change in cost 0.000109673\n",
      "step 42060, training accuracy 0.969697, cost 9.64908, change in cost 0.000110626\n",
      "step 42070, training accuracy 0.969697, cost 9.64897, change in cost 0.000107765\n",
      "step 42080, training accuracy 0.969697, cost 9.64886, change in cost 0.000109673\n",
      "step 42090, training accuracy 0.969697, cost 9.64875, change in cost 0.000109673\n",
      "step 42100, training accuracy 0.969697, cost 9.64864, change in cost 0.000107765\n",
      "step 42110, training accuracy 0.969697, cost 9.64853, change in cost 0.000109673\n",
      "step 42120, training accuracy 0.969697, cost 9.64843, change in cost 0.000107765\n",
      "step 42130, training accuracy 0.969697, cost 9.64832, change in cost 0.000109673\n",
      "step 42140, training accuracy 0.969697, cost 9.64821, change in cost 0.000109673\n",
      "step 42150, training accuracy 0.969697, cost 9.6481, change in cost 0.000108719\n",
      "step 42160, training accuracy 0.969697, cost 9.64799, change in cost 0.000108719\n",
      "step 42170, training accuracy 0.969697, cost 9.64788, change in cost 0.000109673\n",
      "step 42180, training accuracy 0.969697, cost 9.64777, change in cost 0.000108719\n",
      "step 42190, training accuracy 0.969697, cost 9.64766, change in cost 0.000107765\n",
      "step 42200, training accuracy 0.969697, cost 9.64755, change in cost 0.000109673\n",
      "step 42210, training accuracy 0.969697, cost 9.64745, change in cost 0.000106812\n",
      "step 42220, training accuracy 0.969697, cost 9.64734, change in cost 0.000110626\n",
      "step 42230, training accuracy 0.969697, cost 9.64723, change in cost 0.000108719\n",
      "step 42240, training accuracy 0.969697, cost 9.64712, change in cost 0.000107765\n",
      "step 42250, training accuracy 0.969697, cost 9.64701, change in cost 0.000107765\n",
      "step 42260, training accuracy 0.969697, cost 9.6469, change in cost 0.000107765\n",
      "step 42270, training accuracy 0.969697, cost 9.6468, change in cost 0.000107765\n",
      "step 42280, training accuracy 0.969697, cost 9.64669, change in cost 0.000108719\n",
      "step 42290, training accuracy 0.969697, cost 9.64658, change in cost 0.000107765\n",
      "step 42300, training accuracy 0.969697, cost 9.64647, change in cost 0.000108719\n",
      "step 42310, training accuracy 0.969697, cost 9.64636, change in cost 0.000108719\n",
      "step 42320, training accuracy 0.969697, cost 9.64626, change in cost 0.000107765\n",
      "step 42330, training accuracy 0.969697, cost 9.64615, change in cost 0.000107765\n",
      "step 42340, training accuracy 0.969697, cost 9.64604, change in cost 0.000107765\n",
      "step 42350, training accuracy 0.969697, cost 9.64593, change in cost 0.000107765\n",
      "step 42360, training accuracy 0.969697, cost 9.64582, change in cost 0.000107765\n",
      "step 42370, training accuracy 0.969697, cost 9.64572, change in cost 0.000107765\n",
      "step 42380, training accuracy 0.969697, cost 9.64561, change in cost 0.000109673\n",
      "step 42390, training accuracy 0.969697, cost 9.6455, change in cost 0.000106812\n",
      "step 42400, training accuracy 0.969697, cost 9.64539, change in cost 0.000106812\n",
      "step 42410, training accuracy 0.969697, cost 9.64528, change in cost 0.000108719\n",
      "step 42420, training accuracy 0.969697, cost 9.64518, change in cost 0.000105858\n",
      "step 42430, training accuracy 0.969697, cost 9.64507, change in cost 0.000107765\n",
      "step 42440, training accuracy 0.969697, cost 9.64496, change in cost 0.000106812\n",
      "step 42450, training accuracy 0.969697, cost 9.64486, change in cost 0.000108719\n",
      "step 42460, training accuracy 0.969697, cost 9.64475, change in cost 0.000107765\n",
      "step 42470, training accuracy 0.969697, cost 9.64464, change in cost 0.000106812\n",
      "step 42480, training accuracy 0.969697, cost 9.64453, change in cost 0.000106812\n",
      "step 42490, training accuracy 0.969697, cost 9.64443, change in cost 0.000108719\n",
      "step 42500, training accuracy 0.969697, cost 9.64432, change in cost 0.000105858\n",
      "step 42510, training accuracy 0.969697, cost 9.64421, change in cost 0.000106812\n",
      "step 42520, training accuracy 0.969697, cost 9.6441, change in cost 0.000107765\n",
      "step 42530, training accuracy 0.969697, cost 9.644, change in cost 0.000106812\n",
      "step 42540, training accuracy 0.969697, cost 9.64389, change in cost 0.000107765\n",
      "step 42550, training accuracy 0.969697, cost 9.64378, change in cost 0.000106812\n",
      "step 42560, training accuracy 0.969697, cost 9.64368, change in cost 0.000105858\n",
      "step 42570, training accuracy 0.969697, cost 9.64357, change in cost 0.000105858\n",
      "step 42580, training accuracy 0.969697, cost 9.64347, change in cost 0.000106812\n",
      "step 42590, training accuracy 0.969697, cost 9.64336, change in cost 0.000106812\n",
      "step 42600, training accuracy 0.969697, cost 9.64325, change in cost 0.000104904\n",
      "step 42610, training accuracy 0.969697, cost 9.64314, change in cost 0.000108719\n",
      "step 42620, training accuracy 0.969697, cost 9.64304, change in cost 0.000104904\n",
      "step 42630, training accuracy 0.969697, cost 9.64293, change in cost 0.000107765\n",
      "step 42640, training accuracy 0.969697, cost 9.64283, change in cost 0.000106812\n",
      "step 42650, training accuracy 0.969697, cost 9.64272, change in cost 0.000105858\n",
      "step 42660, training accuracy 0.969697, cost 9.64261, change in cost 0.000105858\n",
      "step 42670, training accuracy 0.969697, cost 9.6425, change in cost 0.000108719\n",
      "step 42680, training accuracy 0.969697, cost 9.6424, change in cost 0.000104904\n",
      "step 42690, training accuracy 0.969697, cost 9.64229, change in cost 0.000107765\n",
      "step 42700, training accuracy 0.969697, cost 9.64219, change in cost 0.000103951\n",
      "step 42710, training accuracy 0.969697, cost 9.64208, change in cost 0.000107765\n",
      "step 42720, training accuracy 0.969697, cost 9.64197, change in cost 0.000106812\n",
      "step 42730, training accuracy 0.969697, cost 9.64187, change in cost 0.000104904\n",
      "step 42740, training accuracy 0.969697, cost 9.64176, change in cost 0.000106812\n",
      "step 42750, training accuracy 0.969697, cost 9.64165, change in cost 0.000106812\n",
      "step 42760, training accuracy 0.969697, cost 9.64155, change in cost 0.000104904\n",
      "step 42770, training accuracy 0.969697, cost 9.64144, change in cost 0.000105858\n",
      "step 42780, training accuracy 0.969697, cost 9.64134, change in cost 0.000106812\n",
      "step 42790, training accuracy 0.969697, cost 9.64123, change in cost 0.000106812\n",
      "step 42800, training accuracy 0.969697, cost 9.64113, change in cost 0.000104904\n",
      "step 42810, training accuracy 0.969697, cost 9.64102, change in cost 0.000105858\n",
      "step 42820, training accuracy 0.969697, cost 9.64091, change in cost 0.000106812\n",
      "step 42830, training accuracy 0.969697, cost 9.64081, change in cost 0.000105858\n",
      "step 42840, training accuracy 0.969697, cost 9.6407, change in cost 0.000106812\n",
      "step 42850, training accuracy 0.969697, cost 9.6406, change in cost 0.000103951\n",
      "step 42860, training accuracy 0.969697, cost 9.64049, change in cost 0.000104904\n",
      "step 42870, training accuracy 0.969697, cost 9.64038, change in cost 0.000106812\n",
      "step 42880, training accuracy 0.969697, cost 9.64028, change in cost 0.000104904\n",
      "step 42890, training accuracy 0.969697, cost 9.64017, change in cost 0.000104904\n",
      "step 42900, training accuracy 0.969697, cost 9.64007, change in cost 0.000106812\n",
      "step 42910, training accuracy 0.969697, cost 9.63996, change in cost 0.000104904\n",
      "step 42920, training accuracy 0.969697, cost 9.63986, change in cost 0.000104904\n",
      "step 42930, training accuracy 0.969697, cost 9.63975, change in cost 0.000104904\n",
      "step 42940, training accuracy 0.969697, cost 9.63965, change in cost 0.000105858\n",
      "step 42950, training accuracy 0.969697, cost 9.63954, change in cost 0.000105858\n",
      "step 42960, training accuracy 0.969697, cost 9.63944, change in cost 0.000104904\n",
      "step 42970, training accuracy 0.969697, cost 9.63933, change in cost 0.000104904\n",
      "step 42980, training accuracy 0.969697, cost 9.63923, change in cost 0.000105858\n",
      "step 42990, training accuracy 0.969697, cost 9.63912, change in cost 0.000104904\n",
      "step 43000, training accuracy 0.969697, cost 9.63902, change in cost 0.000104904\n",
      "step 43010, training accuracy 0.969697, cost 9.63891, change in cost 0.000104904\n",
      "step 43020, training accuracy 0.969697, cost 9.63881, change in cost 0.000104904\n",
      "step 43030, training accuracy 0.969697, cost 9.6387, change in cost 0.000104904\n",
      "step 43040, training accuracy 0.969697, cost 9.6386, change in cost 0.000104904\n",
      "step 43050, training accuracy 0.969697, cost 9.63849, change in cost 0.000104904\n",
      "step 43060, training accuracy 0.969697, cost 9.63839, change in cost 0.000104904\n",
      "step 43070, training accuracy 0.969697, cost 9.63828, change in cost 0.000103951\n",
      "step 43080, training accuracy 0.969697, cost 9.63818, change in cost 0.000104904\n",
      "step 43090, training accuracy 0.969697, cost 9.63807, change in cost 0.000103951\n",
      "step 43100, training accuracy 0.969697, cost 9.63797, change in cost 0.000104904\n",
      "step 43110, training accuracy 0.969697, cost 9.63786, change in cost 0.000105858\n",
      "step 43120, training accuracy 0.969697, cost 9.63776, change in cost 0.000103951\n",
      "step 43130, training accuracy 0.969697, cost 9.63766, change in cost 0.000103951\n",
      "step 43140, training accuracy 0.969697, cost 9.63755, change in cost 0.000105858\n",
      "step 43150, training accuracy 0.969697, cost 9.63744, change in cost 0.000104904\n",
      "step 43160, training accuracy 0.969697, cost 9.63734, change in cost 0.000103951\n",
      "step 43170, training accuracy 0.969697, cost 9.63724, change in cost 0.000102997\n",
      "step 43180, training accuracy 0.969697, cost 9.63713, change in cost 0.000105858\n",
      "step 43190, training accuracy 0.969697, cost 9.63703, change in cost 0.000102997\n",
      "step 43200, training accuracy 0.969697, cost 9.63692, change in cost 0.000104904\n",
      "step 43210, training accuracy 0.969697, cost 9.63682, change in cost 0.000101089\n",
      "step 43220, training accuracy 0.969697, cost 9.63672, change in cost 0.000105858\n",
      "step 43230, training accuracy 0.969697, cost 9.63661, change in cost 0.000104904\n",
      "step 43240, training accuracy 0.969697, cost 9.63651, change in cost 0.000103951\n",
      "step 43250, training accuracy 0.969697, cost 9.63641, change in cost 0.000102043\n",
      "step 43260, training accuracy 0.969697, cost 9.6363, change in cost 0.000102997\n",
      "step 43270, training accuracy 0.969697, cost 9.6362, change in cost 0.000104904\n",
      "step 43280, training accuracy 0.969697, cost 9.6361, change in cost 0.000102997\n",
      "step 43290, training accuracy 0.969697, cost 9.63599, change in cost 0.000103951\n",
      "step 43300, training accuracy 0.969697, cost 9.63589, change in cost 0.000103951\n",
      "step 43310, training accuracy 0.969697, cost 9.63578, change in cost 0.000103951\n",
      "step 43320, training accuracy 0.969697, cost 9.63568, change in cost 0.000103951\n",
      "step 43330, training accuracy 0.969697, cost 9.63558, change in cost 0.000102997\n",
      "step 43340, training accuracy 0.969697, cost 9.63547, change in cost 0.000103951\n",
      "step 43350, training accuracy 0.969697, cost 9.63537, change in cost 0.000103951\n",
      "step 43360, training accuracy 0.969697, cost 9.63527, change in cost 0.000102043\n",
      "step 43370, training accuracy 0.969697, cost 9.63516, change in cost 0.000102997\n",
      "step 43380, training accuracy 0.969697, cost 9.63506, change in cost 0.000102997\n",
      "step 43390, training accuracy 0.969697, cost 9.63495, change in cost 0.000105858\n",
      "step 43400, training accuracy 0.969697, cost 9.63485, change in cost 0.000102997\n",
      "step 43410, training accuracy 0.969697, cost 9.63475, change in cost 0.000102043\n",
      "step 43420, training accuracy 0.969697, cost 9.63465, change in cost 0.000103951\n",
      "step 43430, training accuracy 0.969697, cost 9.63454, change in cost 0.000104904\n",
      "step 43440, training accuracy 0.969697, cost 9.63444, change in cost 0.000102997\n",
      "step 43450, training accuracy 0.969697, cost 9.63434, change in cost 0.000102043\n",
      "step 43460, training accuracy 0.969697, cost 9.63423, change in cost 0.000103951\n",
      "step 43470, training accuracy 0.969697, cost 9.63413, change in cost 0.000102997\n",
      "step 43480, training accuracy 0.969697, cost 9.63403, change in cost 0.000102997\n",
      "step 43490, training accuracy 0.969697, cost 9.63392, change in cost 0.000102997\n",
      "step 43500, training accuracy 0.969697, cost 9.63382, change in cost 0.000103951\n",
      "step 43510, training accuracy 0.969697, cost 9.63372, change in cost 0.000101089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 43520, training accuracy 0.969697, cost 9.63361, change in cost 0.000103951\n",
      "step 43530, training accuracy 0.969697, cost 9.63351, change in cost 0.000100136\n",
      "step 43540, training accuracy 0.969697, cost 9.63341, change in cost 0.000104904\n",
      "step 43550, training accuracy 0.969697, cost 9.63331, change in cost 0.000102997\n",
      "step 43560, training accuracy 0.969697, cost 9.6332, change in cost 0.000101089\n",
      "step 43570, training accuracy 0.969697, cost 9.6331, change in cost 0.000102997\n",
      "step 43580, training accuracy 0.969697, cost 9.633, change in cost 0.000102043\n",
      "step 43590, training accuracy 0.969697, cost 9.6329, change in cost 0.000102997\n",
      "step 43600, training accuracy 0.969697, cost 9.63279, change in cost 0.000102997\n",
      "step 43610, training accuracy 0.969697, cost 9.63269, change in cost 0.000101089\n",
      "step 43620, training accuracy 0.969697, cost 9.63259, change in cost 0.000103951\n",
      "step 43630, training accuracy 0.969697, cost 9.63249, change in cost 0.000102043\n",
      "step 43640, training accuracy 0.969697, cost 9.63239, change in cost 0.000101089\n",
      "step 43650, training accuracy 0.969697, cost 9.63228, change in cost 0.000103951\n",
      "step 43660, training accuracy 0.969697, cost 9.63218, change in cost 0.000100136\n",
      "step 43670, training accuracy 0.969697, cost 9.63208, change in cost 0.000103951\n",
      "step 43680, training accuracy 0.969697, cost 9.63198, change in cost 0.000101089\n",
      "step 43690, training accuracy 0.969697, cost 9.63188, change in cost 0.000101089\n",
      "step 43700, training accuracy 0.969697, cost 9.63177, change in cost 0.000102043\n",
      "step 43710, training accuracy 0.969697, cost 9.63167, change in cost 0.000102997\n",
      "step 43720, training accuracy 0.969697, cost 9.63157, change in cost 0.000102043\n",
      "step 43730, training accuracy 0.969697, cost 9.63147, change in cost 0.000101089\n",
      "step 43740, training accuracy 0.969697, cost 9.63136, change in cost 0.000102997\n",
      "step 43750, training accuracy 0.969697, cost 9.63126, change in cost 0.000102043\n",
      "step 43760, training accuracy 0.969697, cost 9.63116, change in cost 0.000101089\n",
      "step 43770, training accuracy 0.969697, cost 9.63106, change in cost 0.000101089\n",
      "step 43780, training accuracy 0.969697, cost 9.63096, change in cost 0.000102997\n",
      "step 43790, training accuracy 0.969697, cost 9.63086, change in cost 0.000101089\n",
      "step 43800, training accuracy 0.969697, cost 9.63075, change in cost 0.000101089\n",
      "step 43810, training accuracy 0.969697, cost 9.63065, change in cost 0.000101089\n",
      "step 43820, training accuracy 0.969697, cost 9.63055, change in cost 0.000102043\n",
      "step 43830, training accuracy 0.969697, cost 9.63045, change in cost 0.000102043\n",
      "step 43840, training accuracy 0.969697, cost 9.63035, change in cost 0.000101089\n",
      "step 43850, training accuracy 0.969697, cost 9.63025, change in cost 0.000100136\n",
      "step 43860, training accuracy 0.969697, cost 9.63015, change in cost 0.000102043\n",
      "step 43870, training accuracy 0.969697, cost 9.63004, change in cost 0.000102043\n",
      "step 43880, training accuracy 0.969697, cost 9.62994, change in cost 9.91821e-05\n",
      "change in cost 9.91821e-05; convergence.\n",
      "final accuracy on test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize reporting variables\n",
    "cost = 0\n",
    "diff = 1\n",
    "epoch_values = []\n",
    "accuracy_values = []\n",
    "cost_values = []\n",
    "\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in cost %g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "        # Report occasional stats\n",
    "        if i % 10 == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            # Generate accuracy stats on test data\n",
    "            train_accuracy, newCost = sess.run([accuracy_OP, cost_OP], feed_dict={X: trainX, yGold: trainY})\n",
    "            # Add accuracy to live graphing variable\n",
    "            accuracy_values.append(train_accuracy)\n",
    "            # Add cost to live graphing variable\n",
    "            cost_values.append(newCost)\n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "\n",
    "            #generate print statements\n",
    "            print(\"step %d, training accuracy %g, cost %g, change in cost %g\"%(i, train_accuracy, newCost, diff))\n",
    "\n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP, \n",
    "                                                     feed_dict={X: testX, \n",
    "                                                                yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico do custo/erro ao longo do tempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/guilherme/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGyNJREFUeJzt3XuQXOV95vHvr+9z1dyl0WV0MwiEuEgIIuMYY5zlYhOwKceB2BsWu4JNarcSb7kSO87am6qtlIO9u7HjsBReA3YS49tCQrwxmDUGTAICIW4SIJAEuo2kGc391tPT0+/+0WdGo6FbMxp65sw5/Xyqps7pt8/M+fVb8JxX73m725xziIhI8EX8LkBEREpDgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIiGhQBcRCQkFuohISCjQRURCIraQJ2tqanJr1qxZyFOKiATe888/f8I51zzTcQsa6GvWrGHHjh0LeUoRkcAzswOzOU5TLiIiIaFAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iERCAC/ZevHefOx/f6XYaIyKIWiEB/fE8n33lyv99liIgsaoEI9GjEGM/py6xFRE5HgS4iEhLBCXSnQBcROZ1ABHrEjFzO7ypERBa3QAR6NIJG6CIiMwhGoJvm0EVEZhKMQI/ky8wp1EVEigpIoOe3WQW6iEhRgQj0SMQAyGkeXUSkqEAEetTyga55dBGR4oIR6N4IXStdRESKC0SgR7wRum6KiogUF4hAj0Xzga6boiIixQUi0DVCFxGZWSACXXPoIiIzC0aga5WLiMiMZgx0M7vHzDrMbNeUtovM7Bkze9HMdpjZpfNa5MQ6dH1Al4hIUbMZod8HXDOt7Q7gL5xzFwFf8R7Pm5imXEREZjRjoDvnngS6pzcDtd7+EqC9xHWdYmKEPq4huohIUbE5/t4fA4+Y2TfIXxQuK11J73RyDn0+zyIiEmxzvSl6O/B559wq4PPAd4sdaGa3efPsOzo7O+d0sokP59JNURGR4uYa6LcAD3j7PwGK3hR1zt3tnNvqnNva3Nw8p5NNrkPXHLqISFFzDfR24APe/pXAm6Upp7DJdegaoYuIFDXjHLqZ3Q9cATSZ2WHgq8AfAN80sxiQBm6bzyInAl1v/RcRKW7GQHfO3VzkqYtLXEtRUX0euojIjPROURGRkAhEoJ98p6gCXUSkmEAEekxz6CIiMwpEoMe9hehjemeRiEhRCnQRkZAIRKAnYvkyM+OachERKSYYgT4xQs9qhC4iUkwgAj0ey98UzWjKRUSkqGAEuubQRURmFIhAn5xD15SLiEhRwQj0yRG6boqKiBQTiECfmHLRCF1EpLhABHo0YkRMc+giIqcTiECH/Dy6Al1EpLjABHo8GtGyRRGR0whMoCeiEc2hi4icRmACPR7VlIuIyOkEJtDzc+hatigiUkxgAj0eNc2hi4icRoACXXPoIiKnE5hAT8WjpMfG/S5DRGTRCkygVyYU6CIipxOYQK+IRxnOKNBFRIoJTKCnElFGNEIXESkqMIFeEY+S1ghdRKSowAR6pUboIiKnFZhAr4gr0EVETicwgZ5ftpgjl9O7RUVECglMoFckogCksxqli4gUEphAr/QCfUQ3RkVECgpMoKfiXqBrHl1EpKDABHqFF+h6t6iISGGBC/SRjD6gS0SkkBkD3czuMbMOM9s1rf0/mdkeM9ttZnfMX4l5E3Pow5nsfJ9KRCSQZjNCvw+4ZmqDmX0QuAG4wDl3HvCN0pd2qqpkDIDBUQW6iEghMwa6c+5JoHta8+3A15xzo94xHfNQ2ylqUvlAH0gr0EVECpnrHPrZwPvNbLuZPWFml5SyqEJqUnEABtJj830qEZFAir2L36sHtgGXAD82s3XOuXe8jdPMbgNuA2hra5trnZMj9H6N0EVECprrCP0w8IDLexbIAU2FDnTO3e2c2+qc29rc3DzXOknGIsSjpikXEZEi5hro/whcCWBmZwMJ4ESpiirEzKhJxTXlIiJSxIxTLmZ2P3AF0GRmh4GvAvcA93hLGTPALYWmW0qtJhXTCF1EpIgZA905d3ORpz5V4lpmlA90jdBFRAoJzDtFAWqScY3QRUSKCFaga8pFRKSoQAV6taZcRESKClSg16Y05SIiUkygAr2uMs7AaJaxcX3ioojIdIEK9IaqBAC9w5p2ERGZLlCBXl+ZD/Se4YzPlYiILD6BCvSJEXr3kAJdRGQ6BbqISEgo0EVEQiJQgV5Xmf9M9B4FuojIOwQq0JOxKNXJGN26KSoi8g6BCnSA+qq4RugiIgUELtAbKhN0ax26iMg7BC7Q66sSdA2O+l2GiMiiE7hAX1qTonNAgS4iMl3gAr2lNsmJwVGy+jwXEZFTBDDQU+QcdOnGqIjIKQIX6EtrkgAc70/7XImIyOISvECvTQHQ0a95dBGRqQIb6McHNEIXEZkqcIHeVJ3ADI5rhC4icorABXosGqGxKkmH5tBFRE4RuEAHWFqb1E1REZFpAhnoy2pTHO1ToIuITBXIQF9ZX8GRnhGcc36XIiKyaAQy0Fc1VDIwmqV/JOt3KSIii0YgA31lfQUAh3qGfa5ERGTxCGigVwJwWIEuIjIpkIG+ajLQR3yuRERk8QhkoNdWxKhJxhToIiJTBDLQzYwV9RUc6taUi4jIhEAGOuRXumiELiJyUmADva2hkgPdQ+RyWosuIgKzCHQzu8fMOsxsV4HnvmBmzsya5qe84tY3V5Mey9Hep1G6iAjMboR+H3DN9EYzWwX8O+BgiWualfXNVQDs6xzy4/QiIovOjIHunHsS6C7w1P8E/gTwZc5jfUs1APs6Bv04vYjIojOnOXQzux444px7aRbH3mZmO8xsR2dn51xOV1BjVYIlFXH2n1Cgi4jAHALdzCqBLwNfmc3xzrm7nXNbnXNbm5ubz/R0p6uDdc1V7OvQlIuICMxthL4eWAu8ZGZvAyuBnWa2rJSFzaqQ5mr2dWqELiICcwh059wrzrkW59wa59wa4DCwxTl3rOTVzWB9czUdA6P0jYwt9KlFRBad2SxbvB94GthgZofN7DPzX9bsnLOsBoA9xwZ8rkRExH+xmQ5wzt08w/NrSlbNGdq4vBaAV9v7uHRtg19liIgsCoF9pyhAS02SxqoEu9v7/S5FRMR3gQ50M2Pj8lpePapAFxEJdKBDftrlzeODZLI5v0sREfFV8AO9tZbMeE7LF0Wk7AU+0M9bvgSAV470+VyJiIi/Ah/o65qqqE3FeOFgj9+liIj4KvCBHokYm9vq2Xmg1+9SRER8FfhAB7h4dT1vdAzoHaMiUtZCE+jOwYuHNEoXkfIVikC/cFUdEYOdBzSPLiLlKxSBXp2Mcc6yWp59q9D3cIiIlIdQBDrAZesbef5ADyOZcb9LERHxRWgC/TfPaiIznuO5tzVKF5HyFJpAv3RtA/Go8a97T/hdioiIL0IT6JWJGFva6nlKgS4iZSo0gQ7w/rOa2N3eT+fAqN+liIgsuFAF+gfPaQHgsdeP+1yJiMjCC1Wgb2ytZWV9BY/sVqCLSPkJVaCbGVdtXMZTe08wOJr1uxwRkQUVqkAHuOq8pWSyOZ7Y0+l3KSIiCyp0gX7JmgYaqxL831fa/S5FRGRBhS7QoxHjty9czv97rUOfvigiZSV0gQ5w45YVZLI5/uWVo36XIiKyYEIZ6OevWMJ7Wqp5YOdhv0sREVkwoQx0M+PGLSt47u0e9uvLo0WkTIQy0AE+fvFK4lHj+08f8LsUEZEFEdpAb6lJ8ZHzW/np84e1Jl1EykJoAx3glsvWMDia1Vy6iJSFUAf65rZ6LlxVxz1PvUV2POd3OSIi8yrUgQ5w+wfW83bXMP/8st5oJCLhFvpAv2rjUs5ZVsPfPLaX8ZzzuxwRkXkT+kCPRIw/+tBZ7O8c4p9f0ihdRMIr9IEOcPV5y9jYWsvXH9lDekxfIi0i4TRjoJvZPWbWYWa7prR93cxeN7OXzexBM6ub3zLfnUjE+PPrzuVI7wjffeotv8sREZkXsxmh3wdcM63tUWCTc+4C4A3gSyWuq+QuW9/E1ect5W9/tZfj/Wm/yxERKbkZA9059yTQPa3tF865iXfrPAOsnIfaSu7PPnwu2Zzjvz602+9SRERKrhRz6J8Gfl6CvzPvVjdW8fnfOpuf7zqmT2IUkdB5V4FuZl8GssA/nOaY28xsh5nt6Oz0/1uE/uD9azl/xRL+yz/uonso43c5IiIlM+dAN7NbgOuATzrnii7wds7d7Zzb6pzb2tzcPNfTlUwsGuHrv3MBA+ksX/jJS+S0Nl1EQmJOgW5m1wB/ClzvnBsubUnz75xltfz5defy2OsdfOfX+/0uR0SkJGazbPF+4Glgg5kdNrPPAN8GaoBHzexFM7trnussuX+/bTUfOb+VOx7Zw/b9XX6XIyLyrtlpZktKbuvWrW7Hjh0Ldr6Z9KfH+Oi3/5Xu4QwP/uH7WNtU5XdJIiLvYGbPO+e2znRcWbxTtJjaVJx7b72EiBm33vusbpKKSKCVdaBDfinjd37/Ytr70tx633MMpMf8LklEZE7KPtABLl7dwLdv3szuI338h3uf0zcciUggKdA9V523jL+5eTMvHurl1nufpV8jdREJGAX6FNee38q3btrMCwd7+cRdT+szX0QkUBTo03zkglbuvfUSDnUPc+Od/8abxwf8LklEZFYU6AW8/6xmfvTZ9zKazfGxO/+Nh3cd87skEZEZKdCL2LRiCQ/9x/exvqWaz/398/zVw6/ri6ZFZFFToJ/G8roKfvzZbfzeb7Txvx7fx+/e/QwHuob8LktEpCAF+gySsSh/+bHz+eZNF/Hm8QGu/eav+cH2gyzkO2xFRGZDgT5LN1y0gkc+fzlb2ur5swdf4fe+s529HbphKiKLhwL9DLQuqeD7n76U//bRTexu7+Pab/6aOx5+neGM3ogkIv5ToJ+hSMT41LbVPPaFK7j+whXc+fg+rvzGE/zw2YO6aSoivlKgz1FTdZL//okL+cnn3ktrXYovPvAKV//1kzy865jm10XEFwr0d+mSNQ08cPtl3PWpiwH43N8/z4e/9RQPvdTOuL4NSUQWUFl/HnqpZcdzPPjCEe56Yh/7OodY3VjJZy9fz41bVpCKR/0uT0QCarafh65Anwe5nOMXrx7jzsf38fLhPuoq4/zu1lV8attqVjVU+l2eiASMAn0RcM7x9P4u/u7pA/zi1ePknOODG1r45G+0cfnZzcSjmvESkZnNNtBjC1FMuTIzLlvfxGXrmzjaN8L92w/yg2cP8djrO2iqTnD9hSu4ccsKzltei5n5Xa6IBJxG6Assk83xxBudPLDzML98rYPMeI4NS2u4YfNyrt3Uqu81FZF30JRLAPQOZ/jZy0d5YOdhdh7sBeCcZTVcs2kZ125q5eyl1Rq5i4gCPWiO9I7wyK5jPLzrGM8d6MY5WNdUxQfPaeGKDc1curaBZEwrZUTKkQI9wDoG0vxi93Ee2X2M7W91k8nmqIhHed97GvnAhhauOLtZq2VEyogCPSSGM1me2d/Fr17v5PE3OjjUPQLA6sZKtq1t5L3rG9m2rpFlS1I+Vyoi80WBHkLOOfafGOKJPZ08vb+L7fu76E/nPxhsbVMV29Y1sG1dIxevrmdFXYXm30VCQoFeBsZzjteO9vPM/i6e2d/N9re6GPACvqUmyea2Ora01bO5rZ7zVyyhIqE5eJEgUqCXoYmAf+FgDzsP9vLCwR7e7hoGIBYxzm2tZXNbHZtWLGHT8iWctbRab24SCQAFugDQNTjKi4d62Xmwh50Henn5cC9DmXEAEtEIG5bVsGlFLRuXL2HT8lrOba3V586ILDIKdCkol3O83TXErvZ+dh/pY1d7H7vb++kdHgMgYrCuuZoNy2o4u6WGDcuqOXtpDasbq4hGNCcv4ge99V8KikSMdc3VrGuu5voLlwP5m61HekfYdaSfV9v7ePVoP68c7uNfXjnKxPU+EYvwHi/oz1pazYalNaxvrmZlfQUxTduILAoKdMHMWFlfycr6Sq7ZtGyyfTiTZW/HIG8cH+SN4wPsOTbA9v1dPPjCkclj4lGjraGStU3VrGuuYm1TFeuaqljbXEVzdVIrbUQWkAJdiqpMxLhgZR0XrKw7pb0/PcabxwfY1znEWyeGeMvbPvlmJ5nsya/hq07GWNuUD/nVjZWsqq9kVUMlqxoqaF1SoSkckRJToMsZq03FuXh1AxevbjilfTznaO8dyYe897P/xBA7D/bws5fbmfoFTvGosbyugraG/L8M2rygb2vIB39dZVyje5EzNGOgm9k9wHVAh3Nuk9fWAPwIWAO8DXzCOdczf2VKEEQj5o3AK7n87OZTnhsbz3G0N82hnmEOdg9zqNvb9ozwyO5jdA9lTjm+Ih6ltS7F8iUVLK9L0VpgW5XUeERkqhlXuZjZ5cAg8P0pgX4H0O2c+5qZfRGod8796Uwn0yoXKWZwNMuhKUF/tC/N0b4RjvSmOdo7QufgKNP/U61NxVheV8Hyugpal6RYVptiaW2K5tokLTVJltamaKhMENHUjgRcyVa5OOeeNLM105pvAK7w9r8HPA7MGOgixVQnY5zbml8HX0gmm+N4f3oy6Nt707T3jkzuv3Cwhx5v6eVUsYjRXJMP+OaaFEtrk7TUpGipTZ6y31iV1Jy+BN5c/8261Dl3FMA5d9TMWkpYk8g7JGKRyemcYtJj43QOjNIxkKajf5SOgVGO96fpGMjvH+4ZZufBnndM7wCYQX1lgsaqBI3VCRqrkzRV5beN1Qkaq5I0VSdo8NpqUzHN8cuiM++TkGZ2G3AbQFtb23yfTspYKh6dMfQhP9rvHBylYyLs+9N0DmboGhyleyhD12CG14720zWYoW/knaN+yN/UbaxKnhL+dZUJ6ivj1FV524oEdZVx6r3HFfGoLgIyr+Ya6MfNrNUbnbcCHcUOdM7dDdwN+Tn0OZ5PpGQSsQgr6ipYUVcx47GZbI6e4QwnBkfpGszQNZTfnvAuAF1D+e2+jkF6hzOTH6tQ7Lx1FXHqK/NBX1c5se9dACrj3n7++dpUnNqKmC4EMmtzDfSHgFuAr3nbfypZRSKLSCIWYal3s3U2RrPj9A2P0TM8Ru9w5tTtSIbeoTF6hjP0Do/x1okhdg730jucYWy8+FgnFjFqK+LUpmLeNh/0+e3p2nVBKDezWbZ4P/kboE1mdhj4Kvkg/7GZfQY4CPzOfBYpEhTJWJSW2igts7wAQP6jF4Yz45NB3zOcoX8kS396jL6RMfpHxuhPj0229Y+Mcaw/PdmeHsud9u9PXBBqUjGqkzGqkjFqvG211za1vTrlPTfxk4pRnYhRlYzqYx4Wudmscrm5yFMfKnEtImXJzKjyAnVl/Zn//mh2nIF01gv4bMELwMTjodEsA6NZjvWnGRz1HqezjGZPf1GYkIpHqE7GqU5G88GfiE1eKCqTMaoSUSoSMSoT0cn9/DZKpddemYhSlYzl2+K6SJSS3pkhEnDJWJRkdZSm6uSc/8bYeG4y3IcyWQbTWQZHs6eE/tDoOIOjYwyOjk+2D6aztPeevDgMZ8YZGSt+H6GQRCziXQC8kJ/8iRXcr0jkp5EqEhFSsSipRDS/jUeo8PYntqlEhEQ0UjZTTgp0ESEejVDn3aB9t3I5x8jYOMOZcYYzWW97cn8kM85QJsuI1z51f+rxHQPp/P7oyfZs7szXVZgxJeQjBS8Ap7TFo6Ti+eOTsSnHTLmIJOP551LxSP6COrGNRUjG/LuAKNBFpKQikZNTSDD3fzUUksnmGPH+FZAeO7lNj+W87URbbvK50QJt6SmPe4fHTvkbE8fM4doxaSLYk/HoZOj/5cfO59K1DTP/8rugQBeRwEjEIiRiEZYQn9fzOOfIjOdIj+UKXhBGs7nJ7ej0x9kco9lxRsembnNUL8BnDynQRUSmMTNvCiUKFfN78Sgl3V4WEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiITHjl0SX9GRmncCBM/iVJuDEPJUTZOqXwtQvhalfCgtSv6x2zjXPdNCCBvqZMrMds/mm63KjfilM/VKY+qWwMPaLplxEREJCgS4iEhKLPdDv9ruARUr9Upj6pTD1S2Gh65dFPYcuIiKzt9hH6CIiMkuLMtDN7Boz22Nme83si37XM9/M7B4z6zCzXVPaGszsUTN709vWe+1mZt/y+uZlM9sy5Xdu8Y5/08xu8eO1lJKZrTKzX5nZa2a228z+yGsv674xs5SZPWtmL3n98hde+1oz2+69xh+ZWcJrT3qP93rPr5nyt77kte8xs6v9eUWlZWZRM3vBzH7mPS6ffnHOLaofIArsA9YBCeAlYKPfdc3za74c2ALsmtJ2B/BFb/+LwF95+x8Gfg4YsA3Y7rU3APu9bb23X+/3a3uX/dIKbPH2a4A3gI3l3jfe66v29uPAdu/1/hi4yWu/C7jd2/9D4C5v/ybgR97+Ru//rySw1vv/Lur36ytB//xn4AfAz7zHZdMvi3GEfimw1zm33zmXAX4I3OBzTfPKOfck0D2t+Qbge97+94CPTmn/vst7Bqgzs1bgauBR51y3c64HeBS4Zv6rnz/OuaPOuZ3e/gDwGrCCMu8b7/UNeg/j3o8DrgR+6rVP75eJ/vop8CHLf4vxDcAPnXOjzrm3gL3k//8LLDNbCXwE+N/eY6OM+mUxBvoK4NCUx4e9tnKz1Dl3FPLBBrR47cX6J9T95v1zeDP50WjZ9403rfAi0EH+ArUP6HXOZb1Dpr7GydfvPd8HNBLCfgH+GvgTIOc9bqSM+mUxBroVaNNSnJOK9U9o+83MqoH/A/yxc67/dIcWaAtl3zjnxp1zFwEryY8ezy10mLcti34xs+uADufc81ObCxwa2n5ZjIF+GFg15fFKoN2nWvx03JsuwNt2eO3F+ieU/WZmcfJh/g/OuQe8ZvWNxznXCzxOfg69zswmvvh96mucfP3e80vIT/GFrV/eB1xvZm+Tn6q9kvyIvWz6ZTEG+nPAWd6d6QT5mxUP+VyTHx4CJlZj3AL805T23/dWdGwD+rxph0eAq8ys3lv1cZXXFljefOZ3gdecc/9jylNl3Tdm1mxmdd5+BfBb5O8v/Ar4uHfY9H6Z6K+PA4+5/N2/h4CbvNUea4GzgGcX5lWUnnPuS865lc65NeRz4zHn3Ccpp37x+65soR/yqxXeID8v+GW/61mA13s/cBQYIz86+Az5ubxfAm962wbvWAP+1uubV4CtU/7Op8nfwNkL3Or36ypBv/wm+X/qvgy86P18uNz7BrgAeMHrl13AV7z2deSDZy/wEyDptae8x3u959dN+Vtf9vprD3Ct36+thH10BSdXuZRNv+idoiIiIbEYp1xERGQOFOgiIiGhQBcRCQkFuohISCjQRURCQoEuIhISCnQRkZBQoIuIhMT/BwYeBuIURrS4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([np.mean(cost_values[i-50:i]) for i in range(len(cost_values))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
